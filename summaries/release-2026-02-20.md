# Weekly Release Report for vllm-project/vllm (2026-02-20)

This week merged 173 PRs from 88 contributors. Key areas: features 3, fixes 54, performance 9.

## Executive Summary

本次版本更新的核心是渲染器架构重构与核心采样性能优化。主要工作包括将 InputPreprocessor 完全移入 Renderer，并引入了基于 Triton 的新 Top-k/Top-p 采样器内核以提升效率。同时，新增了对 COLQwen3 模型的支持。

在修复方面，集中解决了多模型（如 Qwen3.5、DeepSeek V3、Nemotron 系列）的权重加载、量化配置及特定硬件（如 Hopper）下的内核问题，并对多专家模型（MoE）路由、LoRA 集成和 Mamba 状态管理等进行了多项关键修复。性能优化包括针对 CPU 的模型运行器优化、选择性 CPU 权重卸载以及启用了新的编译优化选项。本次更新未包含显著的破坏性变更，建议用户在升级后重点测试涉及修复的模型与功能。

## Highlights

* [Renderer] Move InputPreprocessor into Renderer (2/2) ([#34560](https://github.com/vllm-project/vllm/pull/34560)) by @DarkLight1337
* [Kernel] Triton-based Top-k and Top-p sampler kernels ([#33538](https://github.com/vllm-project/vllm/pull/33538)) by @cakeng
* [Docs] Clean up speculators docs ([#34065](https://github.com/vllm-project/vllm/pull/34065)) by @kylesayrs
* [Renderer] Move InputPreprocessor into Renderer (1/2) ([#34510](https://github.com/vllm-project/vllm/pull/34510)) by @DarkLight1337
* [new model] add COLQwen3 code & Inference ([#34398](https://github.com/vllm-project/vllm/pull/34398)) by @craftsangjae

## Features & Enhancements

* Add explicit validation error for tool calls. ([#34438](https://github.com/vllm-project/vllm/pull/34438)) by @juliendenize
* Add new sections to CODEOWNERS ([#34309](https://github.com/vllm-project/vllm/pull/34309)) by @DarkLight1337
* Add unit tests for fp8 output fusion of triton_attn ([#34228](https://github.com/vllm-project/vllm/pull/34228)) by @bringlein

## Bug Fixes

* [Bugfix] Fix Qwen3.5 Cutlass fp8 kernel on hopper - clamp block scales ([#34914](https://github.com/vllm-project/vllm/pull/34914)) by @ywang96
* [Bugfix] Fix edge case in UUID data parsing ([#34884](https://github.com/vllm-project/vllm/pull/34884)) by @DarkLight1337
* [Bug] Fix DeepSeek V3 weight loading caused by incorrect prefix ([#34876](https://github.com/vllm-project/vllm/pull/34876)) by @wzhao18
* [Bugfix] Add Quant Config to Llava Next Projector ([#34847](https://github.com/vllm-project/vllm/pull/34847)) by @alex-jw-brooks
* [BUG] Fixing Weight Sync unit test ([#34841](https://github.com/vllm-project/vllm/pull/34841)) by @hao-aaron
* fix(docs): fix typos in comments and docstrings ([#34836](https://github.com/vllm-project/vllm/pull/34836)) by @machov
* [Bugfix] Fix lora tests ([#34834](https://github.com/vllm-project/vllm/pull/34834)) by @DarkLight1337
* [Bugfix] Fix Basic Models Test ([#34818](https://github.com/vllm-project/vllm/pull/34818)) by @MatthewBonanni
* Revert "[NemotronH] Do not force router to run in fp32 (#34582)" ([#34808](https://github.com/vllm-project/vllm/pull/34808)) by @roikoren755
* Fix empty tool_call_id in Anthropic messages API tool result conversion ([#34745](https://github.com/vllm-project/vllm/pull/34745)) by @sfeng33
* [Bugfix] Fix NVFP4 TRTLLM MoE non-gated support; add gsm8k for Nemotron-3-Nano FP8+NVFP4 ([#34725](https://github.com/vllm-project/vllm/pull/34725)) by @mgoin
* [Bugfix] Fix prefix creation for Qwen3.5 ([#34723](https://github.com/vllm-project/vllm/pull/34723)) by @mgoin
* [Bugfix] Qwen3.5 kv-scale weight remapping ([#34719](https://github.com/vllm-project/vllm/pull/34719)) by @Linda-Stadter
* [BugFix] Fix sp tests ([#34716](https://github.com/vllm-project/vllm/pull/34716)) by @zou3519
* [Bugfix] Redo Qwen3.5/Qwen3-Next GDN projector fusion ([#34697](https://github.com/vllm-project/vllm/pull/34697)) by @Isotr0py
* [Bugfix] fix activation in cpu_fused_moe_torch call ([#34696](https://github.com/vllm-project/vllm/pull/34696)) by @michalowski-arm
* Fix docs build warning ([#34686](https://github.com/vllm-project/vllm/pull/34686)) by @hmellor
* [Bugfix][MoE Kernel] Fix incorrect routing selection for models without expert groups (e.g., MiniMax-M2.1) ([#34673](https://github.com/vllm-project/vllm/pull/34673)) by @wwl2755
* [Bugfix] Fix benchmark_fused_collective crash on CustomOp init ([#34665](https://github.com/vllm-project/vllm/pull/34665)) by @mayank-ketkar-sf
* [CI][AMD][BugFix] Skip tests in test_unquantized_backend_selection that should not run on ROCm ([#34655](https://github.com/vllm-project/vllm/pull/34655)) by @rasmith
* [BugFix] [Build] fix string literals comparison in indexer_k_quant_and_cache calling site ([#34653](https://github.com/vllm-project/vllm/pull/34653)) by @hongxiayang
* [Bugfix][CI] Fix flaky `entrypoints/openai/test_response_api_with_harmony.py::test_function_calling[openai/gpt-oss-20b]` ([#34624](https://github.com/vllm-project/vllm/pull/34624)) by @NickLucche
* (bugfix): Fixed encode in LLM entrypoint for IOProcessr plugin prompts ([#34618](https://github.com/vllm-project/vllm/pull/34618)) by @christian-pinto
* Fix call to moe_mk in modelopt MoE modules (required for LoRA) ([#34575](https://github.com/vllm-project/vllm/pull/34575)) by @danisereb
* [Bugfix] Fix Qwen3.5 config loading ([#34554](https://github.com/vllm-project/vllm/pull/34554)) by @ywang96
* [BugFix] Fix Python 3.13 FlashMLA import error ([#34548](https://github.com/vllm-project/vllm/pull/34548)) by @LucasWilkinson
* [Bugfix] Fix ROCm UVA CPU weight offloading broken by #32993 ([#34543](https://github.com/vllm-project/vllm/pull/34543)) by @AndreasKaratzas
* [bug] Make sure get_modality_with_max_tokens is deterministic ([#34533](https://github.com/vllm-project/vllm/pull/34533)) by @842974287
* Revert "[Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides" ([#34530](https://github.com/vllm-project/vllm/pull/34530)) by @mgoin
* fix: use `__annotations__` instead of `get_type_hints()` for dynamic `kwargs` detection ([#34527](https://github.com/vllm-project/vllm/pull/34527)) by @perone
* [bugfix] Fix critical bug when reporting for all paths where handler.create_error_response is used ([#34516](https://github.com/vllm-project/vllm/pull/34516)) by @kizill
* [Bugfix] Exclude `language_model_only` key from MM AOT compile hash but include in model one ([#34508](https://github.com/vllm-project/vllm/pull/34508)) by @ywang96
* [Bugfix] Fix fused MoE int32 overflow in stride*offset without perf regression ([#34507](https://github.com/vllm-project/vllm/pull/34507)) by @haosdent
* [Bugfix] Add quant_config in ViT of Kimi-K2.5 ([#34501](https://github.com/vllm-project/vllm/pull/34501)) by @LoganJane
* [Bugfix] Handle num_expert_group=None in flashinfer block-scale FP8 MoE ([#34494](https://github.com/vllm-project/vllm/pull/34494)) by @haosdent
* [Bugfix] Fix mamba state dtype setting for Qwen3-Next and Qwen3.5 ([#34489](https://github.com/vllm-project/vllm/pull/34489)) by @ywang96
* [Bugfix] Fix encoder cache underestimation for GLM-4V/GLM-OCR single image ([#34483](https://github.com/vllm-project/vllm/pull/34483)) by @haosdent
* [BUGFIX] Fix accuracy regression for NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 with TP>1 ([#34476](https://github.com/vllm-project/vllm/pull/34476)) by @vadiklyutiy
* [Bugfix][MTP][Sparse MLA] Allow sparse MLA with MTP to run with FULL cudagraphs ([#34457](https://github.com/vllm-project/vllm/pull/34457)) by @MatthewBonanni
* [Bugfix]: Fix structured output in multi-turn gpt-oss ([#34454](https://github.com/vllm-project/vllm/pull/34454)) by @bbrowning
* [Bugfix] Add method to swap quant_method on FusedMoE to fix LoRA issues ([#34453](https://github.com/vllm-project/vllm/pull/34453)) by @bnellnm
* Fix num_logprobs parameter description in sampler.py ([#34451](https://github.com/vllm-project/vllm/pull/34451)) by @zhuohan123
* [BugFix] Add block_size validation for mamba cache align mode ([#34445](https://github.com/vllm-project/vllm/pull/34445)) by @peakcrosser7
* [BugFix] Fix and optimize max_num_blocks_per_req calculation for MambaSpec ([#34440](https://github.com/vllm-project/vllm/pull/34440)) by @peakcrosser7
* [Bugfix] Delete unused redundant code in Kimi-K2.5 ([#34427](https://github.com/vllm-project/vllm/pull/34427)) by @LoganJane
* [Bug Fix] Fix MambaManager.cache_blocks() crash on null blocks in align mode ([#34418](https://github.com/vllm-project/vllm/pull/34418)) by @haosdent
* [Bugfix] Standardize getting number of image patches/tokens ([#34358](https://github.com/vllm-project/vllm/pull/34358)) by @DarkLight1337
* Fixed whisper CPU test that does not spawn properly. ([#34324](https://github.com/vllm-project/vllm/pull/34324)) by @almayne
* [Bugfix] Fix Dynamo unexpected keyword argument  ([#34320](https://github.com/vllm-project/vllm/pull/34320)) by @samutamm
* [Bugfix] fix the import path in moe test utils.py ([#34245](https://github.com/vllm-project/vllm/pull/34245)) by @michalowski-arm
* [CI][AMD][BugFix] Use torch.testing.assert_close instead of assert torch.allclose in test_rocm_skinny_gemms.py ([#34181](https://github.com/vllm-project/vllm/pull/34181)) by @rasmith
* [Bugfix] Treat generation_config max_tokens as default not ceiling ([#34063](https://github.com/vllm-project/vllm/pull/34063)) by @almogtavor
* [Bugfix] Fix Random Dataset Prefix Length Inaccuracy ([#33907](https://github.com/vllm-project/vllm/pull/33907)) by @frankwang28
* [Bugfix] Fix quant RMS norm fusion for quantization with TMA-aligned scales ([#33255](https://github.com/vllm-project/vllm/pull/33255)) by @ElizaWszola

## Performance

* [Model Runner V2] Minor CPU optimizations ([#34856](https://github.com/vllm-project/vllm/pull/34856)) by @njhill
* [Model Bash] DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5% E2E Speedup) ([#34758](https://github.com/vllm-project/vllm/pull/34758)) by @robertgshaw2-redhat
* [torch.compile] Turn on silu+fp4 quant fusion by default for O1+ ([#34718](https://github.com/vllm-project/vllm/pull/34718)) by @ProExpertProg
* [Feature][Perf] Support Selective CPU Weight Offloading ([#34535](https://github.com/vllm-project/vllm/pull/34535)) by @wzhao18
* [Renderer] Move InputPreprocessor into Renderer (1/2) ([#34510](https://github.com/vllm-project/vllm/pull/34510)) by @DarkLight1337
* [Perf] fused_moe: add int4_w4a16 benchmark support and tuning config ([#34130](https://github.com/vllm-project/vllm/pull/34130)) by @mgehre-amd
* [Kernel] Triton-based Top-k and Top-p sampler kernels ([#33538](https://github.com/vllm-project/vllm/pull/33538)) by @cakeng
* [Feature] Pipeline Parallel Async send/recv, 2.9% E2E throughput improvement ([#33368](https://github.com/vllm-project/vllm/pull/33368)) by @yewentao256
* [MM Encoder] Add Triton ViT attention backend ([#32183](https://github.com/vllm-project/vllm/pull/32183)) by @Isotr0py

## Model Support

* [Model Runner V2] Use FP32 for Gumbel Noise ([#34854](https://github.com/vllm-project/vllm/pull/34854)) by @WoosukKwon
* [Model Runner V2] Minor simplification for DCP ([#34786](https://github.com/vllm-project/vllm/pull/34786)) by @WoosukKwon
* [Model Runner V2] Avoid prepare prefill kernel launch overhead ([#34780](https://github.com/vllm-project/vllm/pull/34780)) by @njhill
* [Model Runner V2] A bit more PP simplification ([#34766](https://github.com/vllm-project/vllm/pull/34766)) by @njhill
* [Model Runner V2] Further simplification for PP ([#34724](https://github.com/vllm-project/vllm/pull/34724)) by @WoosukKwon
* Revert "[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj" ([#34683](https://github.com/vllm-project/vllm/pull/34683)) by @ZJY0516
* [Model Runner V2] Minor simplification for BadWordsState ([#34669](https://github.com/vllm-project/vllm/pull/34669)) by @WoosukKwon
* [Model Runner V2] Fix unintended CPU-GPU sync in make_dummy ([#34667](https://github.com/vllm-project/vllm/pull/34667)) by @WoosukKwon
* [Model Runner V2] Minor cleanup for PP ([#34666](https://github.com/vllm-project/vllm/pull/34666)) by @WoosukKwon
* [Model Runner V2] Minor refactoring for penalties ([#34662](https://github.com/vllm-project/vllm/pull/34662)) by @WoosukKwon
* Revert "[Misc] fix qwen3.5 config" ([#34610](https://github.com/vllm-project/vllm/pull/34610)) by @ywang96
* [Misc] fix qwen3.5 config ([#34604](https://github.com/vllm-project/vllm/pull/34604)) by @JJJYmmm
* [CI/Build] Enable tests for recent day-0 new models ([#34585](https://github.com/vllm-project/vllm/pull/34585)) by @Isotr0py
* [Doc] Add Mistral-7b-v0.3 model to the batch invariance validated model ([#34584](https://github.com/vllm-project/vllm/pull/34584)) by @banparth
* [Doc] Update Encoder-Decoder models support doc with Florence-2 ([#34581](https://github.com/vllm-project/vllm/pull/34581)) by @Isotr0py
* [Model Runner V2] Minor cleanup for Sampler ([#34563](https://github.com/vllm-project/vllm/pull/34563)) by @WoosukKwon
* [Misc] Port Qwen3.5 Configs ([#34512](https://github.com/vllm-project/vllm/pull/34512)) by @ywang96
* [Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj ([#34492](https://github.com/vllm-project/vllm/pull/34492)) by @Isotr0py
* [CI/Build] Fix CUDA re-initialization error in distributed model tests ([#34491](https://github.com/vllm-project/vllm/pull/34491)) by @DarkLight1337
* [Llama4,Quantization] Simplify and generalize logic for Q/K permutations in quantized self-attn layers  ([#34471](https://github.com/vllm-project/vllm/pull/34471)) by @eldarkurtic
* [New Model] support new model ovis2.6 ([#34426](https://github.com/vllm-project/vllm/pull/34426)) by @myselvess
* [Misc] Update tests and examples for Prithvi/Terratorch models ([#34416](https://github.com/vllm-project/vllm/pull/34416)) by @christian-pinto
* [CI] Add GPT-OSS Eval job for H100 ([#34359](https://github.com/vllm-project/vllm/pull/34359)) by @mgoin
* [CI] Heavy refactoring of Voxtral multimodal audio model tests ([#34294](https://github.com/vllm-project/vllm/pull/34294)) by @AndreasKaratzas
* [Feature] Decode Context Parallel support for GPU model runner v2 ([#34179](https://github.com/vllm-project/vllm/pull/34179)) by @yewentao256
* [Hybrid] Enable mamba prefix cache "align" mode with async scheduling  ([#33997](https://github.com/vllm-project/vllm/pull/33997)) by @tdoublep
* [Core] Pipeline Parallel support for Model Runner V2 ([#33960](https://github.com/vllm-project/vllm/pull/33960)) by @ZhanqiuHu
* [Model Runner V2] support bad_words sampling param ([#33433](https://github.com/vllm-project/vllm/pull/33433)) by @izhuhaoran
* [Model Runner V2] support piecewise & mixed cudagraph ([#32771](https://github.com/vllm-project/vllm/pull/32771)) by @izhuhaoran

## Hardware & Backend

* Change targets for AMD build in the "CI" pipeline ([#34918](https://github.com/vllm-project/vllm/pull/34918)) by @Alexei-V-Ivanov-AMD
* [ci] Use the right tag for CPU arm64 image ([#34915](https://github.com/vllm-project/vllm/pull/34915)) by @khluu
* [UX] More descriptive reasons in is_supported_config for MoE ([#34908](https://github.com/vllm-project/vllm/pull/34908)) by @mgoin
* [Refactor] Implement output type check in LLM ([#34794](https://github.com/vllm-project/vllm/pull/34794)) by @DarkLight1337
* [ROCm][CI] Fix plugins test group; updating terratorch and dependencies ([#34589](https://github.com/vllm-project/vllm/pull/34589)) by @AndreasKaratzas
* [MoE Refactor] Convert mxfp4 marlin into modular kernel format  ([#34588](https://github.com/vllm-project/vllm/pull/34588)) by @zyongye
* [ROCm][CI] Guard sparse MLA backend imports for ROCm compatibility in tests ([#34538](https://github.com/vllm-project/vllm/pull/34538)) by @AndreasKaratzas
* [Kernels] Fix Helion GPU utils to use platform-agnostic device name API ([#34537](https://github.com/vllm-project/vllm/pull/34537)) by @AndreasKaratzas
* [Misc] vLLM's --enforce-eager should turn off compile and cudagraphs only ([#34523](https://github.com/vllm-project/vllm/pull/34523)) by @zou3519
* [GDN] Use CPU tensors to build GDN metadata ([#34498](https://github.com/vllm-project/vllm/pull/34498)) by @WoosukKwon
* Use paged_attention_v1 for sliding window decode in rocm_aiter_fa ([#34378](https://github.com/vllm-project/vllm/pull/34378)) by @iseeyuan
* [CI] Enable mypy coverage for individual excluded files ([#34292](https://github.com/vllm-project/vllm/pull/34292)) by @Lucaskabela
* [ROCm][CI] Fix serving tokens test failures ([#34047](https://github.com/vllm-project/vllm/pull/34047)) by @AndreasKaratzas
* [Kernel] [Helion] [5/N] Add Helion Autotuning infrastructure ([#34025](https://github.com/vllm-project/vllm/pull/34025)) by @gmagogsfm
* Bump `lm-eval` version for Transformers v5 compatibility ([#33994](https://github.com/vllm-project/vllm/pull/33994)) by @hmellor
* [Frontend] Enable generic structured_outputs for responses API ([#33709](https://github.com/vllm-project/vllm/pull/33709)) by @alecsolder
* [Kernel] [Helion] [4/N] Add silu_mul_fp8 Helion kernel  ([#33373](https://github.com/vllm-project/vllm/pull/33373)) by @gmagogsfm
* [Feature] Support CPU Offloading without Pytorch Pinned Memory that leads to doubled allocation ([#32993](https://github.com/vllm-project/vllm/pull/32993)) by @wzhao18

## Refactoring & Core

* [Refactor] Call renderer for online IO processor request ([#34490](https://github.com/vllm-project/vllm/pull/34490)) by @DarkLight1337
* [Refactor] Pass full VllmConfig to Renderer ([#34485](https://github.com/vllm-project/vllm/pull/34485)) by @DarkLight1337
* [Frontend] Fix reasoning_tokens for text-based parsers in Responses API ([#33513](https://github.com/vllm-project/vllm/pull/33513)) by @anencore94

## Build, CI & Testing

* [CI/Build] Try to make beam search test less flaky ([#34885](https://github.com/vllm-project/vllm/pull/34885)) by @DarkLight1337
* [CI] temporarily disable multi-node tests ([#34825](https://github.com/vllm-project/vllm/pull/34825)) by @robertgshaw2-redhat
* [CI] Fix flaky test_parsable_context ([#34717](https://github.com/vllm-project/vllm/pull/34717)) by @sfeng33
* [CI] Fix bake config artifact path for AMI rebuild pipeline ([#34656](https://github.com/vllm-project/vllm/pull/34656)) by @amrmahdi
* Targeting the MI355 agent pool with all existing tests ([#34629](https://github.com/vllm-project/vllm/pull/34629)) by @Alexei-V-Ivanov-AMD
* [CI][Nixl] Add CrossLayer KV layout tests ([#34615](https://github.com/vllm-project/vllm/pull/34615)) by @NickLucche
* [CI][Frontend] Return 422 instead of 500 for invalid Anthropic tool_choice ([#34590](https://github.com/vllm-project/vllm/pull/34590)) by @AndreasKaratzas
* [CI] Write bake config to temp directory instead of repo root ([#34569](https://github.com/vllm-project/vllm/pull/34569)) by @amrmahdi
* [CI][Metrics] Stabilize tests with polling and subprocess guards ([#34566](https://github.com/vllm-project/vllm/pull/34566)) by @AndreasKaratzas
* [CI][Entrypoints] Validate detokenize token IDs to prevent int64 overflow causing 500 ([#34468](https://github.com/vllm-project/vllm/pull/34468)) by @AndreasKaratzas
* [CI/Build] Update video URLs for testing ([#34446](https://github.com/vllm-project/vllm/pull/34446)) by @DarkLight1337
* [torch.compile] Disable ar-rms fusion for ds3-fp4 & DP, fix CI test ([#34392](https://github.com/vllm-project/vllm/pull/34392)) by @ProExpertProg
* [Fix] Fix tracing test race condition by adding server readiness check ([#34364](https://github.com/vllm-project/vllm/pull/34364)) by @emricksini-h

## Documentation

* Extend ColBERT support to non-standard BERT backbones ([#34170](https://github.com/vllm-project/vllm/pull/34170)) by @ieBoytsov
* [Core] Move pause and resume functions into engine ([#34125](https://github.com/vllm-project/vllm/pull/34125)) by @hao-aaron

## Miscellaneous

* [Voxtral Realtime] Fix engine crash on empty multimodal embeddings ([#34862](https://github.com/vllm-project/vllm/pull/34862)) by @talnirnx
* [Core] Fix state names in pause_scheduler() ([#34840](https://github.com/vllm-project/vllm/pull/34840)) by @markmc
* [Misc] Add mooncake-transfer-engine to kv_connectors requirements ([#34826](https://github.com/vllm-project/vllm/pull/34826)) by @stmatengss
* [Core] Fix SSRF bypass via backslash-@ URL parsing inconsistency ([#34743](https://github.com/vllm-project/vllm/pull/34743)) by @russellb
* [Quantization] - Added uses_meta_device_weights to quant config ([#34645](https://github.com/vllm-project/vllm/pull/34645)) by @Josephasafg
* [Renderer] Move InputPreprocessor into Renderer (1.5/2) ([#34598](https://github.com/vllm-project/vllm/pull/34598)) by @DarkLight1337
* [Core] Profiler improvements and lazy initialization ([#33198](https://github.com/vllm-project/vllm/pull/33198)) by @jaewonlee-fb
* [Core] Add sleep level 0 mode with enqueue/wait pattern ([#33195](https://github.com/vllm-project/vllm/pull/33195)) by @jaewonlee-fb
* [Scheduler][ASR] Fix CrossAttn blocks per-request for Variable length encoder inputs ([#31058](https://github.com/vllm-project/vllm/pull/31058)) by @ekagra-ranjan

## Breaking Changes

* [ROCm][CI] Removing all blocking labels from MI355 until stable infra ([#34879](https://github.com/vllm-project/vllm/pull/34879)) by @AndreasKaratzas
* [ROCm][Test] Fix beam search determinism failures from batch-size-dependent FP divergence and removed wrong marker ([#34878](https://github.com/vllm-project/vllm/pull/34878)) by @AndreasKaratzas
* Deprecate test-pipeline.yaml ([#34864](https://github.com/vllm-project/vllm/pull/34864)) by @khluu
* [Model Runner V2] Remove unnecessary copies in PW CUDA graph capture ([#34849](https://github.com/vllm-project/vllm/pull/34849)) by @WoosukKwon
* [CI][Bugfix] Fix multinode test script ([#34820](https://github.com/vllm-project/vllm/pull/34820)) by @ilmarkov
* [Renderer] Deprecate code paths for old input processing ([#34775](https://github.com/vllm-project/vllm/pull/34775)) by @DarkLight1337
* [CI] Remove unused precompiled wheel args from image build ([#34767](https://github.com/vllm-project/vllm/pull/34767)) by @amrmahdi
* [ROCm][CI] Removed hard-coded attn backend requirement for Qwen VL ([#34753](https://github.com/vllm-project/vllm/pull/34753)) by @AndreasKaratzas
* [Renderer] Move MM Hash parsing into Renderer ([#34711](https://github.com/vllm-project/vllm/pull/34711)) by @DarkLight1337
* [CI/Build] Remove use of `skip_v1` ([#34699](https://github.com/vllm-project/vllm/pull/34699)) by @DarkLight1337
* [CI] Enable mypy import following for vllm/v1/kv_offload ([#34639](https://github.com/vllm-project/vllm/pull/34639)) by @aneeshkp
* Remove dead bitsandbytes CxB code from 8-bit inference path ([#34633](https://github.com/vllm-project/vllm/pull/34633)) by @TimDettmers
* [CI] Disable precompiled wheel path in CI image builds ([#34606](https://github.com/vllm-project/vllm/pull/34606)) by @amrmahdi
* [NemotronH] Do not force router to run in fp32 ([#34582](https://github.com/vllm-project/vllm/pull/34582)) by @roikoren755
* [Bugfix] Fix ARC touch KeyError for non-ready T1 blocks in kv offload ([#34576](https://github.com/vllm-project/vllm/pull/34576)) by @Vivo50E
* [Renderer] Move InputPreprocessor into Renderer (2/2) ([#34560](https://github.com/vllm-project/vllm/pull/34560)) by @DarkLight1337
* [CI][BugFix] ShellCheck cleanup to remove baseline and preserve runtime behavior ([#34514](https://github.com/vllm-project/vllm/pull/34514)) by @junuxyz
* [Bugfix] Replace c10::optional with std::optional in topk kernel ([#34467](https://github.com/vllm-project/vllm/pull/34467)) by @FloatingVertex
* [Bugfix] Remove assert causing hipErrorStreamCaptureUnsupported ([#34455](https://github.com/vllm-project/vllm/pull/34455)) by @JadenMathias
* [Bugfix] Remove assert that's no longer valid ([#34443](https://github.com/vllm-project/vllm/pull/34443)) by @bnellnm
* [Refactor] Simplify BOS/EOS token handling ([#34435](https://github.com/vllm-project/vllm/pull/34435)) by @DarkLight1337
* [KV Connector] Add temporary, off-by-default `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION` workaround ([#34415](https://github.com/vllm-project/vllm/pull/34415)) by @eicherseiji
* [new model] add COLQwen3 code & Inference ([#34398](https://github.com/vllm-project/vllm/pull/34398)) by @craftsangjae
* [Ray] Propagate third-party env vars to Ray workers via prefix matching ([#34383](https://github.com/vllm-project/vllm/pull/34383)) by @kouroshHakha
* [Model Bash][DeepSeekR1] Remove Shared Expert Clone ([#34344](https://github.com/vllm-project/vllm/pull/34344)) by @robertgshaw2-redhat
* [Refactor] Deprecate `head_first` for `chunk_gated_delta_rule` ([#34263](https://github.com/vllm-project/vllm/pull/34263)) by @yewentao256
* [KVConnector] Clean up redundant code in KV connectors ([#34147](https://github.com/vllm-project/vllm/pull/34147)) by @hickeyma
* [Docs] Clean up speculators docs ([#34065](https://github.com/vllm-project/vllm/pull/34065)) by @kylesayrs
*  [Hybrid] Fix and optimize block-aligned splitting in mamba cache align mode ([#33706](https://github.com/vllm-project/vllm/pull/33706)) by @peakcrosser7
* [Hybrid] Enable spec decoding in mamba cache align mode ([#33705](https://github.com/vllm-project/vllm/pull/33705)) by @peakcrosser7
* [Attention] Refactor `check_and_update_config` ([#33600](https://github.com/vllm-project/vllm/pull/33600)) by @MatthewBonanni
* [CPU][ARM] Add ARM BF16 cross-compilation support and improve documen… ([#33079](https://github.com/vllm-project/vllm/pull/33079)) by @maryamtahhan
* [Bugfix] Fix 'remove_instance_endpoint' method logic in disagg_proxy_demo ([#32922](https://github.com/vllm-project/vllm/pull/32922)) by @ChenqianCao

## Upgrade Notes

- During the migration, the tag got changed by mistake. This is to fix it back to the right tag
- csrc/quantization/w8a8/fp8/amd/quant_utils.cuh:643:18: note: expanded from macro 'DISPATCH_BY_KV_CACHE_DTYPE'
- Note: For Qwen3-Next, it should still be initialized with `--mamba-ssm-cache-dtype float32` which was previously a bug that's not discovered.
- ^^ Note: the score of `0.7051` here is expected. It is the same with and without this PR.
- > **Note:** `nvidia/nemotron-colembed-vl-*` models are architecturally compatible but currently blocked by an upstream tokenizer issue (`extra_special_tokens` is a `list` instead of `dict` in `tokenizer_config.json`, incompatible with the installed transformers version). Support will work automatica
- NOTE: this is a manually stacked PR, each commit is reviewed separately. For this PR, please only review the top commit: Add Helion autotuning infra
- *Note: Claude (Anthropic) was used as a coding assistant during development of this PR.*
- **NOTE: this is a manually stacked PR, each commit is reviewed separately. For this PR, please only review the top commit: Migrate silu_mul_fp8 kernel to new registration system**

## Contributors

@842974287, @Alexei-V-Ivanov-AMD, @AndreasKaratzas, @ChenqianCao, @DarkLight1337, @ElizaWszola, @FloatingVertex, @Isotr0py, @JJJYmmm, @JadenMathias, @Josephasafg, @Linda-Stadter, @LoganJane, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @NickLucche, @ProExpertProg, @TimDettmers, @Vivo50E, @WoosukKwon, @ZJY0516, @ZhanqiuHu, @alecsolder, @alex-jw-brooks, @almayne, @almogtavor, @amrmahdi, @aneeshkp, @anencore94, @banparth, @bbrowning, @bnellnm, @bringlein, @cakeng, @christian-pinto, @craftsangjae, @danisereb, @eicherseiji, @ekagra-ranjan, @eldarkurtic, @emricksini-h, @frankwang28, @gmagogsfm, @hao-aaron, @haosdent, @hickeyma, @hmellor, @hongxiayang, @ieBoytsov, @ilmarkov, @iseeyuan, @izhuhaoran, @jaewonlee-fb, @juliendenize, @junuxyz, @khluu, @kizill, @kouroshHakha, @kylesayrs, @machov, @markmc, @maryamtahhan, @mayank-ketkar-sf, @mgehre-amd, @mgoin, @michalowski-arm, @myselvess, @njhill, @peakcrosser7, @perone, @rasmith, @robertgshaw2-redhat, @roikoren755, @russellb, @samutamm, @sfeng33, @stmatengss, @talnirnx, @tdoublep, @vadiklyutiy, @wwl2755, @wzhao18, @yewentao256, @ywang96, @zhuohan123, @zou3519, @zyongye