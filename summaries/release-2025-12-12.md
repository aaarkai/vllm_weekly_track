# Weekly Release Report for vllm-project/vllm (2025-12-12)

This week merged 231 PRs from 130 contributors. Key areas: features 8, fixes 11, performance 18.

## Executive Summary

本周发布涵盖多项关键更新。功能增强方面，Transformers 后端新增对 Eagle 和 Eagle3 模型的支持，并引入了 latent MoE 支持。性能优化工作包括为 Hopper 架构提供 W4A8 Grouped GEMM 内核支持，以及实现融合的块量化 RMS 归一化以提升效率。

缺陷修复涉及多个方面，重点解决了 Afmoe 模型的 rope_parameters 问题、跨进程共享内存的可见性同步，以及修复了使用 FlashInfer attention 时的配置警告。模型支持与硬件适配方面，增强了对 Qwen2 MoE 等模型的 GGUF 量化支持，并修复了多项模型加载与兼容性问题。

本次版本包含多项破坏性变更，例如弃用 `SupportsMultiModal.merge_by_field_config` 方法，移除已废弃的 `task`、`seed` 等设置字段，并将 `get_rope` 函数参数标准化为使用 `rope_parameters["partial_rotary_factor"]`。升级前请仔细核查相关变更说明。

## Highlights

* [Chore] Deprecate `SupportsMultiModal.merge_by_field_config` ([#30170](https://github.com/vllm-project/vllm/pull/30170)) by @DarkLight1337
* [Cleanup] Refactor profiling env vars into a CLI config ([#29912](https://github.com/vllm-project/vllm/pull/29912)) by @benchislett
* [Kernel]Support W4A8 Grouped GEMM on Hopper ([#29691](https://github.com/vllm-project/vllm/pull/29691)) by @czhu-cohere
* [Performance] Fused blockwise quant RMS norm ([#27883](https://github.com/vllm-project/vllm/pull/27883)) by @ElizaWszola
* Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` ([#30389](https://github.com/vllm-project/vllm/pull/30389)) by @hmellor

## Features & Enhancements

* Add Eagle and Eagle3 support to Transformers modeling backend ([#30340](https://github.com/vllm-project/vllm/pull/30340)) by @hmellor
* Add tip for `mypy` and `markdownlint` to the pre-commit comment ([#30259](https://github.com/vllm-project/vllm/pull/30259)) by @hmellor
* Add latent MoE support ([#30203](https://github.com/vllm-project/vllm/pull/30203)) by @shaharmor98
* feat(metrics): Add prefill KV compute metric excluding cached tokens ([#30189](https://github.com/vllm-project/vllm/pull/30189)) by @ziliangpeng
* Add more docs for regex ([#30106](https://github.com/vllm-project/vllm/pull/30106)) by @xu-song
* Support tokenization_kwargs override ([#29794](https://github.com/vllm-project/vllm/pull/29794)) by @piood
* Add SpecDec support to `selective_state_update` ([#29488](https://github.com/vllm-project/vllm/pull/29488)) by @roikoren755
* Add evaluate_guards option to DynamicShapesConfig ([#27432](https://github.com/vllm-project/vllm/pull/27432)) by @laithsakka

## Bug Fixes

* [Bugfix][Model] Fix Afmoe rope_parameters issue ([#30505](https://github.com/vllm-project/vllm/pull/30505)) by @mgoin
* Fix typo of endpoint name in CLI args docs ([#30473](https://github.com/vllm-project/vllm/pull/30473)) by @kmaehashi
* fix(shm): Add memory barriers for cross-process shared memory visibility ([#30407](https://github.com/vllm-project/vllm/pull/30407)) by @kitaekatt
* [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` ([#30399](https://github.com/vllm-project/vllm/pull/30399)) by @LucasWilkinson
* Fix typos in comments across multiple files ([#30345](https://github.com/vllm-project/vllm/pull/30345)) by @wilsonwu
* fix: enhance human_readable_int function ([#30337](https://github.com/vllm-project/vllm/pull/30337)) by @andyxning
* [Bugfix] Fix fp8 DeepGemm compilation issues ([#30336](https://github.com/vllm-project/vllm/pull/30336)) by @ElizaWszola
* [Bugfix] Fix compressed-tensors models failing to load with transformers backend ([#30287](https://github.com/vllm-project/vllm/pull/30287)) by @mgoin
* [bug] Fix "Current vLLM config is not set." warnings when FlashInfer attention is used ([#30241](https://github.com/vllm-project/vllm/pull/30241)) by @nvpohanh
* [BugFix] Unblock use of LoRA with data parallel mode ([#30220](https://github.com/vllm-project/vllm/pull/30220)) by @njhill
* fix#30092 Kimi-Linear model loading failure with missing indexer_rotary_emb ([#30093](https://github.com/vllm-project/vllm/pull/30093)) by @baonudesifeizhai

## Performance

* [Perf] Optimize deepgemm experts initialization, 3.9% TTFT improvement ([#30494](https://github.com/vllm-project/vllm/pull/30494)) by @yewentao256
* [Bugfix] Fix `task` still being passed in tests/benchmarks ([#30476](https://github.com/vllm-project/vllm/pull/30476)) by @DarkLight1337
* [Misc] Consistent case for `vllm bench serve` results ([#30403](https://github.com/vllm-project/vllm/pull/30403)) by @MatthewBonanni
* [Perf] Optimize `group_topk` kernel, 1.9% Throughput improvement, 2.1% TPOT improvemnt ([#30159](https://github.com/vllm-project/vllm/pull/30159)) by @yewentao256
* [Misc] Rename TensorRT Model Optimizer to Model Optimizer ([#30091](https://github.com/vllm-project/vllm/pull/30091)) by @Edwardf0t1
* [Perf] Enable separate shared_experts stream only for CUDA ([#30085](https://github.com/vllm-project/vllm/pull/30085)) by @alexm-redhat
* [CPU][Perf] Add fast vectorized exp impl from Arm Optimized Routines ([#30068](https://github.com/vllm-project/vllm/pull/30068)) by @Elm8116
* [Structured Output][Reasoning] Improves decoding throughput for models using single-token reasoning endings. ([#30056](https://github.com/vllm-project/vllm/pull/30056)) by @hdlj-h
* [Bugfix]: Fix `TokenizerLike` interface ([#30009](https://github.com/vllm-project/vllm/pull/30009)) by @Rohan138
* [bench] Support common prefix len config (for decode-only bench) ([#29934](https://github.com/vllm-project/vllm/pull/29934)) by @minosfuture
* [Perf] Improve fp8 quant in mla; replace ReduceSum with ReduceScatterSum ([#29795](https://github.com/vllm-project/vllm/pull/29795)) by @IwakuraRein
* [V1][Spec Decode] Optimize Medusa proposer to avoid GPU-CPU sync ([#29723](https://github.com/vllm-project/vllm/pull/29723)) by @dongbo910220
* [perf] Use direct copy (broadcast) instead of cat for k_nope/k_pe in MLA prefill ([#29710](https://github.com/vllm-project/vllm/pull/29710)) by @minosfuture
* [Kernel][MoE] optimize `moe_align_block_size` ([#29642](https://github.com/vllm-project/vllm/pull/29642)) by @jinzhen-lin
* [Perf] Enable environment cache in EngineCore to enable the feature for UniProcExecutor as well ([#29289](https://github.com/vllm-project/vllm/pull/29289)) by @Jialin
* Lora MoE Align Improvements ([#29257](https://github.com/vllm-project/vllm/pull/29257)) by @gnovack
* [docs] Improve wide-EP performance + benchmarking documentation ([#27933](https://github.com/vllm-project/vllm/pull/27933)) by @eicherseiji
* [Performance] Fused blockwise quant RMS norm ([#27883](https://github.com/vllm-project/vllm/pull/27883)) by @ElizaWszola

## Model Support

* [BugFix] Fix minimax m2 model rotary_dim ([#30384](https://github.com/vllm-project/vllm/pull/30384)) by @rogeryoungh
* [Model Runner V2] Fix Triton warning on tl.where ([#30355](https://github.com/vllm-project/vllm/pull/30355)) by @WoosukKwon
* [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() ([#30331](https://github.com/vllm-project/vllm/pull/30331)) by @dtrifiro
* [bugfix][quantization] fix quark qwen3 kv_cache quantization ([#30308](https://github.com/vllm-project/vllm/pull/30308)) by @haoyangli-amd
* [Model][Quantization] Fix / Add GGUF support for Qwen2 MoE models ([#30307](https://github.com/vllm-project/vllm/pull/30307)) by @a4lg
* [Bugfix] Qwen 3 VL Embedding loading ([#30303](https://github.com/vllm-project/vllm/pull/30303)) by @noooop
* Ensure minimum frames for GLM 4.6V compatibility ([#30285](https://github.com/vllm-project/vllm/pull/30285)) by @gh-wf
* Mark qwen2_5_vl as xfail ([#30283](https://github.com/vllm-project/vllm/pull/30283)) by @gmagogsfm
* gptq marlin quantization support for fused moe with lora ([#30254](https://github.com/vllm-project/vllm/pull/30254)) by @Bhanu068
* [Bugfix]: Fix glm46 awq marlin moe wna16 compatibility ([#30210](https://github.com/vllm-project/vllm/pull/30210)) by @baonudesifeizhai
* Revert "[Renderer] Separate out `RendererConfig` from `ModelConfig` (#30145)" ([#30199](https://github.com/vllm-project/vllm/pull/30199)) by @DarkLight1337
* [Model Runner V2] Support num NaNs in logits ([#30187](https://github.com/vllm-project/vllm/pull/30187)) by @WoosukKwon
* [Model] Move `multimodal_cpu_fields` definition to field config ([#30181](https://github.com/vllm-project/vllm/pull/30181)) by @DarkLight1337
* [Model Runner V2] Support min-p sampling ([#30171](https://github.com/vllm-project/vllm/pull/30171)) by @WoosukKwon
* let draft model follow target model's config_format ([#30152](https://github.com/vllm-project/vllm/pull/30152)) by @bangshengtang
* [Renderer] Separate out `RendererConfig` from `ModelConfig` ([#30145](https://github.com/vllm-project/vllm/pull/30145)) by @DarkLight1337
* [Model][Quantization] Restore MoE + GGUF models support (incl. Qwen3 MoE) by allowing Sideload Parameters ([#30116](https://github.com/vllm-project/vllm/pull/30116)) by @a4lg
* [Model] Add support for transformer-based Ultravox v0.7 projector ([#30089](https://github.com/vllm-project/vllm/pull/30089)) by @petersalas
* [Model] Add Holo2 reasoning parser ([#30048](https://github.com/vllm-project/vllm/pull/30048)) by @hdlj-h
* [Frontend][Model] Add 'float16' to possible mamba cache dtype values, override mamba SSM cache dtype value for NemotronH ([#29978](https://github.com/vllm-project/vllm/pull/29978)) by @amitz-nv
* [Bugfix][llama4_eagle] Fix missing 'lm_head' attribute ([#29926](https://github.com/vllm-project/vllm/pull/29926)) by @divakar-amd
* [CI/Build][AMD] Add Llama4 Maverick FP8 to AMD CI ([#28695](https://github.com/vllm-project/vllm/pull/28695)) by @zhewenl
* [Tests] Tool call tests for openai/gpt-oss-20b ([#26237](https://github.com/vllm-project/vllm/pull/26237)) by @debroy-rh

## Hardware & Backend

* [ROCm][CI] Use mi325_4 agent pool for V1 e2e tests ([#30526](https://github.com/vllm-project/vllm/pull/30526)) by @AndreasKaratzas
* [CI/Build][AMD] Skip test_cutlass_w4a8_moe tests on ROCm sine they require cutlass_pack_scale_fp8 ([#30508](https://github.com/vllm-project/vllm/pull/30508)) by @rasmith
* [Docs][CPU backend] Add pre-built Arm CPU Docker images ([#30491](https://github.com/vllm-project/vllm/pull/30491)) by @ioghiban
* [CPU][FIX] Fix build failures on Arm CPUs with torch nightly ([#30481](https://github.com/vllm-project/vllm/pull/30481)) by @fadara01
* [Doc] Add Baidu Kunlun XPU support ([#30455](https://github.com/vllm-project/vllm/pull/30455)) by @xyDong0223
* [ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding ([#30430](https://github.com/vllm-project/vllm/pull/30430)) by @AndreasKaratzas
* [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  ([#30417](https://github.com/vllm-project/vllm/pull/30417)) by @rasmith
* [Docs][CPU Backend] Add nightly and per revision pre-built Arm CPU wheels ([#30402](https://github.com/vllm-project/vllm/pull/30402)) by @ioghiban
* [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output ([#30371](https://github.com/vllm-project/vllm/pull/30371)) by @chaunceyjiang
* [cpu][ci] Add CPU Attention Tests for Neon Backend ([#30347](https://github.com/vllm-project/vllm/pull/30347)) by @fadara01
* [CI] refine more logic when generating and using nightly wheels & indices, add cuda130 build for aarch64, specify correct manylinux version ([#30341](https://github.com/vllm-project/vllm/pull/30341)) by @Harry-Chen
* [Bugfix] Fix cuda graph sizes when running with speculative decoding ([#30330](https://github.com/vllm-project/vllm/pull/30330)) by @PatrykSaffer
* [fix] fix SM check for Flashinfer TRTLLM MOE ([#30314](https://github.com/vllm-project/vllm/pull/30314)) by @jiahanc
* [CI] Fix Flaky test_eagle_max_len Test ([#30306](https://github.com/vllm-project/vllm/pull/30306)) by @micah-wil
* Update AMD test definitions (2025-12-08) ([#30298](https://github.com/vllm-project/vllm/pull/30298)) by @Alexei-V-Ivanov-AMD
* [ROCM][CI] Fix AMD Examples Test Group ([#30276](https://github.com/vllm-project/vllm/pull/30276)) by @Concurrensee
* [CI/Build] Use spawn subprocess for ROCm ([#30272](https://github.com/vllm-project/vllm/pull/30272)) by @rjrock
* [bugfix][quantization] Fix fp8 per_tensor scale shape ([#30257](https://github.com/vllm-project/vllm/pull/30257)) by @haoyangli-amd
* [ROCm] Guard group quant RMS norm fusion patterns ([#30239](https://github.com/vllm-project/vllm/pull/30239)) by @yeqcharlotte
* [TPU] Bump tpu-inference to 0.12.0 ([#30221](https://github.com/vllm-project/vllm/pull/30221)) by @jcyang43
* [BugFix][DeepSeek-V3.2] Fix backend selection logic for Blackwell ([#30195](https://github.com/vllm-project/vllm/pull/30195)) by @LucasWilkinson
* [BugFix] Fix `assert  batch_descriptor.num_tokens == num_tokens_padded` ([#30173](https://github.com/vllm-project/vllm/pull/30173)) by @LucasWilkinson
* [CI/Build][AMD][Quantization] Fix test_int8_kernel.py by updating int8_utils to use hip.libdevice.round ([#30151](https://github.com/vllm-project/vllm/pull/30151)) by @rasmith
* Bump nvshmem to 3.3.24 and fix CUDA 13 installation ([#30149](https://github.com/vllm-project/vllm/pull/30149)) by @dmitry-tokarev-nv
* [CPU][CI] Enable fused MoE tests in Arm CI ([#30132](https://github.com/vllm-project/vllm/pull/30132)) by @fadara01
* [AMD][CI] Add ray[default] Dependency On ROCm To Pass v1/metrics/test_engine_logger_apis.py ([#30110](https://github.com/vllm-project/vllm/pull/30110)) by @micah-wil
* [CI/Build][AMD] Skip marlin, machete, and hadacore tests since these require _C functions not defined for ROCm ([#30109](https://github.com/vllm-project/vllm/pull/30109)) by @rasmith
* [ROCm][CI] Increase the memory threshold for test_deep_sleep_fp8_kvcache ([#30104](https://github.com/vllm-project/vllm/pull/30104)) by @charlifu
* [ROCm][CI] Add jiwer dependency for testing ([#30081](https://github.com/vllm-project/vllm/pull/30081)) by @charlifu
* [Core] Whisper enable `FULL_DECODE_ONLY` CudaGraph  ([#30072](https://github.com/vllm-project/vllm/pull/30072)) by @NickLucche
* [CPU] Support for Whisper ([#30062](https://github.com/vllm-project/vllm/pull/30062)) by @aditew01
* [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform ([#30020](https://github.com/vllm-project/vllm/pull/30020)) by @rasmith
* [Bug] Fix vLLM config is not set error ([#29999](https://github.com/vllm-project/vllm/pull/29999)) by @yewentao256
* [CI/Build][AMD] Use float16 in test_reset_prefix_cache_e2e to avoid accuracy issues ([#29997](https://github.com/vllm-project/vllm/pull/29997)) by @rasmith
* [Feature] Add Layer-wise NVTX Support ([#29990](https://github.com/vllm-project/vllm/pull/29990)) by @maxyanghu
* [CI/Build][AMD] Use ROCM_ATTN instead of FLASH_ATTN test for test_register_kv_caches for ROCm and update test for TRITON_ATTN ([#29985](https://github.com/vllm-project/vllm/pull/29985)) by @rasmith
* [Bugfix] Fix parse_output_message crash on commentary with no recipient ([#29972](https://github.com/vllm-project/vllm/pull/29972)) by @strinczer
* [PCP&DCP] move CUDAGraph check for PCP&DCP to the check func of platforms ([#29952](https://github.com/vllm-project/vllm/pull/29952)) by @pisceskkk
* Improve wvsplitK tile and balance heristics. ([#29937](https://github.com/vllm-project/vllm/pull/29937)) by @amd-hhashemi
* [ROCm][CI] Fix test_max_len.py for Rocm ([#29916](https://github.com/vllm-project/vllm/pull/29916)) by @charlifu
* [bugfix] fix MiniMaxM2ReasoningParser streaming output not separating reasoning_content. ([#29882](https://github.com/vllm-project/vllm/pull/29882)) by @JaviS-Rei
* [EPLB] Support EPLB w/ NVFP4 ([#29804](https://github.com/vllm-project/vllm/pull/29804)) by @andrewbriand
* [ROCm][MXFP4] Infer w4a4 quant method in rocm aiter fused moe ([#29775](https://github.com/vllm-project/vllm/pull/29775)) by @ZhiweiYan-96
* [ROCm] [Fused Moe EP] Use binary expert mask for aiter fused moe kernel ([#29773](https://github.com/vllm-project/vllm/pull/29773)) by @ZhiweiYan-96
* [Kernel]Support W4A8 Grouped GEMM on Hopper ([#29691](https://github.com/vllm-project/vllm/pull/29691)) by @czhu-cohere
* [Attention] Make seq_lens_cpu optional in CommonAttentionMetadata to enable true async spec-decode ([#29624](https://github.com/vllm-project/vllm/pull/29624)) by @LucasWilkinson
* [Bugfix] Correct num_q_heads on DCP for Flashinfer backends  ([#29487](https://github.com/vllm-project/vllm/pull/29487)) by @gjc0824
* [Compressed Tensors] Add XPU `wNa16` support ([#29484](https://github.com/vllm-project/vllm/pull/29484)) by @yiliu30
* [ROCm][CI] Skip NVIDIA-Only Prime-RL Test in AMD CI ([#29420](https://github.com/vllm-project/vllm/pull/29420)) by @micah-wil
* [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group ([#29358](https://github.com/vllm-project/vllm/pull/29358)) by @AndreasKaratzas
* [ci] Refactor CI file structure ([#29343](https://github.com/vllm-project/vllm/pull/29343)) by @khluu
* [CI/Build] Make test_mha_attn.py run on correct platform only and check for flash_attn_varlen_func in layer.py ([#29145](https://github.com/vllm-project/vllm/pull/29145)) by @rasmith
* [v1] Add PrefixLM support to FlexAttention backend ([#27938](https://github.com/vllm-project/vllm/pull/27938)) by @Isotr0py
* [DeepSeek v3.2] Make top-k work for any logit values. ([#27568](https://github.com/vllm-project/vllm/pull/27568)) by @dcampora
* [Rocm][torch.compile] Adding layernorm + fp8 block quant and silu + fp8 block quant for Aiter ([#25693](https://github.com/vllm-project/vllm/pull/25693)) by @charlifu
* [ROCm] Aiter Quant Kernels ([#25552](https://github.com/vllm-project/vllm/pull/25552)) by @vllmellm

## Refactoring & Core

* [responsesAPI][6] Fix multi turn MCP tokenization ([#30230](https://github.com/vllm-project/vllm/pull/30230)) by @qandrew
* [Frontend] Add MCP type support infrastructure to Responses API ([#30054](https://github.com/vllm-project/vllm/pull/30054)) by @daniel-salib
* Refactor example prompts fixture ([#29854](https://github.com/vllm-project/vllm/pull/29854)) by @nwaughachukwuma
* [responsesAPI][5] ResponsesParser with tools for full MCP python loop ([#29798](https://github.com/vllm-project/vllm/pull/29798)) by @qandrew
* simplify requires_files list creation ([#29656](https://github.com/vllm-project/vllm/pull/29656)) by @nwaughachukwuma
* [Core] Refactor `_build_attention_metadata` ([#29628](https://github.com/vllm-project/vllm/pull/29628)) by @LucasWilkinson
* [NIXL] Small cleanup of unused variables ([#29618](https://github.com/vllm-project/vllm/pull/29618)) by @NickLucche

## Build, CI & Testing

* Revert "[CI] Add Async Eplb nightly CI tests (#29385)" ([#30431](https://github.com/vllm-project/vllm/pull/30431)) by @SageMoore
* [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance ([#30367](https://github.com/vllm-project/vllm/pull/30367)) by @micah-wil
* [CI/Test] Fix FP8 per-tensor quant test reference scale shape ([#30352](https://github.com/vllm-project/vllm/pull/30352)) by @LucasWilkinson
* [DCP][Bugfix][CI] Fix accuracy issue of DCP when using FLASH_ATTN_MLA ([#30309](https://github.com/vllm-project/vllm/pull/30309)) by @FENP
* [BugFix] Fix non detected failing tests ([#30277](https://github.com/vllm-project/vllm/pull/30277)) by @ilmarkov
* [CI/Build]Temporary workaround for test_default_mm_loras timeout ([#30202](https://github.com/vllm-project/vllm/pull/30202)) by @jeejeelee
* [Misc] Fix circular import in vllm.transformers_utils.config ([#30179](https://github.com/vllm-project/vllm/pull/30179)) by @yeqcharlotte
* [CI] Re-use whisper_client for all tests ([#30148](https://github.com/vllm-project/vllm/pull/30148)) by @NickLucche
* [CI/Build] Update batch invariant test trigger ([#30080](https://github.com/vllm-project/vllm/pull/30080)) by @zhewenl
* [CI] Have pre-commit comment on a PR if pre-commit was not used ([#30077](https://github.com/vllm-project/vllm/pull/30077)) by @hmellor
* [CI] fix silent error in nightly wheel index generation script, add generation time to HTML index ([#30060](https://github.com/vllm-project/vllm/pull/30060)) by @Harry-Chen
* Gigachat 3 tool parser and tests ([#29905](https://github.com/vllm-project/vllm/pull/29905)) by @ajpqs
* [Compile] Add env `VLLM_FLOAT32_MATMUL_PRECISION` to fix torch warning `TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled` ([#29897](https://github.com/vllm-project/vllm/pull/29897)) by @yewentao256
* [CI] Prevents triggering of an inactive issue/PR check for forked repository. ([#29654](https://github.com/vllm-project/vllm/pull/29654)) by @wzshiming

## Documentation

* Give pooling examples better names ([#30488](https://github.com/vllm-project/vllm/pull/30488)) by @hmellor
* [Docs] Update EPLB docs ([#30426](https://github.com/vllm-project/vllm/pull/30426)) by @mgoin
* [Docs] Generate full list of metrics in user docs ([#30388](https://github.com/vllm-project/vllm/pull/30388)) by @markmc
* [Doc] update Intel GPU MM status in Feature x Hardware matrix ([#30294](https://github.com/vllm-project/vllm/pull/30294)) by @faaany
* [Frontend] Binary embedding response does not return metadata by setting encoding_format to bytes_only. ([#30249](https://github.com/vllm-project/vllm/pull/30249)) by @noooop
* kv_transfer: Rename the shared storage connectors ([#30201](https://github.com/vllm-project/vllm/pull/30201)) by @orozery
* [Misc] Rename CohereForAI references to CohereLabs ([#30147](https://github.com/vllm-project/vllm/pull/30147)) by @russellb
* [DOC]: Add kthena to integrations ([#29931](https://github.com/vllm-project/vllm/pull/29931)) by @hzxuzhonghu
* prefix caching design doc sha256 now default ([#29261](https://github.com/vllm-project/vllm/pull/29261)) by @redwrasse
* [Disagg] Support large batch size in proxy server and update NixlConnector doc for DP ([#28782](https://github.com/vllm-project/vllm/pull/28782)) by @minosfuture
* [docs] governance documents ([#24801](https://github.com/vllm-project/vllm/pull/24801)) by @simon-mo

## Miscellaneous

* [compile] Stop one-off setting enable_aot_compile and use context manager instead. ([#30503](https://github.com/vllm-project/vllm/pull/30503)) by @zhxchen17
* [Misc] Improve error message for `is_multimodal` ([#30483](https://github.com/vllm-project/vllm/pull/30483)) by @DarkLight1337
* Make the `httpx` logger less annoying when Transformers v5 is installed ([#30480](https://github.com/vllm-project/vllm/pull/30480)) by @hmellor
* [Misc] Add mcp to requirements ([#30474](https://github.com/vllm-project/vllm/pull/30474)) by @yeqcharlotte
* [BugFix][MM]support VLLM_RANDOMIZE_DP_DUMMY_INPUTS ([#30472](https://github.com/vllm-project/vllm/pull/30472)) by @charlotte12l
* [Fix] Update lazing loading of video loader backend ([#30444](https://github.com/vllm-project/vllm/pull/30444)) by @jeremyteboul
* [LMCache] Relax lmcache version requirement ([#30425](https://github.com/vllm-project/vllm/pull/30425)) by @njhill
* [IMPROVEMENT] Change MistralReasoningParser behavior ([#30391](https://github.com/vllm-project/vllm/pull/30391)) by @juliendenize
* [Fix]fix import error from lmcache ([#30376](https://github.com/vllm-project/vllm/pull/30376)) by @wz1qqx
* [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing ([#30344](https://github.com/vllm-project/vllm/pull/30344)) by @anker-c2
* [BUGFIX] Mistral tool call parser v11+ ([#30332](https://github.com/vllm-project/vllm/pull/30332)) by @juliendenize
* [Misc] Fix safetensors import for safe_open ([#30300](https://github.com/vllm-project/vllm/pull/30300)) by @hyongtao-code
* [Misc] Split the LoRA code ([#30253](https://github.com/vllm-project/vllm/pull/30253)) by @jeejeelee
* [LoRA]  Reduce the loading time of MoE LoRA ([#30243](https://github.com/vllm-project/vllm/pull/30243)) by @jeejeelee
* Bump actions/checkout from 6.0.0 to 6.0.1 ([#30233](https://github.com/vllm-project/vllm/pull/30233)) by @dependabot
* Address comment to mergify.yml in #30117 ([#30219](https://github.com/vllm-project/vllm/pull/30219)) by @ZhijianJiang
* [Bugfix] Skip generation config fallback for GGUF to prevent multi-process hang ([#30209](https://github.com/vllm-project/vllm/pull/30209)) by @kitaekatt
* [MISC]: change NIXL compatibility hash logging level to debug ([#30182](https://github.com/vllm-project/vllm/pull/30182)) by @AuruTus
* [Bugfix] fix fuse_allreduce_rms when tp =1 ([#30178](https://github.com/vllm-project/vllm/pull/30178)) by @ZJY0516
* [Misc] Move `disable_nccl_for_dp_synchronization` init logic into `VllmConfig` ([#30161](https://github.com/vllm-project/vllm/pull/30161)) by @njhill
* update torchao safetensors impl ([#30155](https://github.com/vllm-project/vllm/pull/30155)) by @liangel-02
* [Enc-Dec] Fix OOT tokenizer issue ([#30144](https://github.com/vllm-project/vllm/pull/30144)) by @NickLucche
* [BugFix] Fix DeepSeek-R1 hang with DP and MTP ([#30119](https://github.com/vllm-project/vllm/pull/30119)) by @LucasWilkinson
* [ez] move harmony utils to parser folder ([#30117](https://github.com/vllm-project/vllm/pull/30117)) by @qandrew
* Do not guard during noop elimination pass ([#30095](https://github.com/vllm-project/vllm/pull/30095)) by @laithsakka
* [bugfix] fix type[AttentionBackend] bug in kv_connector_base_v1 ([#30051](https://github.com/vllm-project/vllm/pull/30051)) by @HF-001
* [Misc][PCP&DCP] relocate PCP feature check ([#30050](https://github.com/vllm-project/vllm/pull/30050)) by @pisceskkk
* [MP executor] fix get device count for multi node of mp executor feature ([#30042](https://github.com/vllm-project/vllm/pull/30042)) by @weiguihua2
* [Feature] Batch-Invariant Support for FA2 and LoRA ([#30018](https://github.com/vllm-project/vllm/pull/30018)) by @quanliu1991
* [BugFix] Adding env variable to disable async grammar compilation ([#29996](https://github.com/vllm-project/vllm/pull/29996)) by @alecsolder
* [BugFix] Eagerly abort cancelled final-step requests ([#29987](https://github.com/vllm-project/vllm/pull/29987)) by @njhill
* [typing] fix type ([#29964](https://github.com/vllm-project/vllm/pull/29964)) by @andyxning
* [moe] Allow disabling DP chunking ([#29936](https://github.com/vllm-project/vllm/pull/29936)) by @minosfuture
* Lazy loading to avoid importing all files ([#29716](https://github.com/vllm-project/vllm/pull/29716)) by @yongming-qin
* [NIXL] Add remote_request_id to kv_transfer_params ([#29665](https://github.com/vllm-project/vllm/pull/29665)) by @markmc
* [Bugfix] Fix grouped_topk pytorch impl when num_experts can't be grouped properly ([#29439](https://github.com/vllm-project/vllm/pull/29439)) by @divakar-amd
* [bugfix] Pass globals to aot_compiled function ([#29428](https://github.com/vllm-project/vllm/pull/29428)) by @angelayi
* [Core] Whisper Enable Encoder Batching ([#29421](https://github.com/vllm-project/vllm/pull/29421)) by @NickLucche
* online fp8 quant with streaming weight post-processing ([#29196](https://github.com/vllm-project/vllm/pull/29196)) by @vkuzo
* [Feature] Batch invariant: Enable `TRITON_MLA` without prefix-caching ([#29125](https://github.com/vllm-project/vllm/pull/29125)) by @yewentao256
* Reduce validation to a warning ([#28749](https://github.com/vllm-project/vllm/pull/28749)) by @alecsolder
* [Quantization] FP8 Weight Reloading for Quantized RL Rollout ([#28480](https://github.com/vllm-project/vllm/pull/28480)) by @kylesayrs
* [KVConnector] Add KV events to KV Connectors ([#28309](https://github.com/vllm-project/vllm/pull/28309)) by @hickeyma
* [Bugfix] fix confusing OOM errors during v1 init ([#28051](https://github.com/vllm-project/vllm/pull/28051)) by @shivampr
* [KVConnector][Feature] Support KV connector cache reset via /reset_prefix_cache ([#27170](https://github.com/vllm-project/vllm/pull/27170)) by @ptovam
* [P/D] KV Load Failure Recovery/Abort Configuration ([#26813](https://github.com/vllm-project/vllm/pull/26813)) by @wseaton

## Breaking Changes

* [Refactor] Remove useless syncwarp ([#30510](https://github.com/vllm-project/vllm/pull/30510)) by @yewentao256
* [Deprecation] Remove missed fallback for `embed_input_ids` ([#30469](https://github.com/vllm-project/vllm/pull/30469)) by @DarkLight1337
* [Deprecation] Deprecation `--convert reward`, use `--convert embed` instead. ([#30463](https://github.com/vllm-project/vllm/pull/30463)) by @noooop
* [Deprecation] Remove fallbacks for `embed_input_ids` and `embed_multimodal` ([#30458](https://github.com/vllm-project/vllm/pull/30458)) by @DarkLight1337
* [Feature] AWQ marlin quantization support for fused moe with lora ([#30442](https://github.com/vllm-project/vllm/pull/30442)) by @princepride
* [ROCm] Fix broken import in platform attention backend dispatching ([#30432](https://github.com/vllm-project/vllm/pull/30432)) by @AndreasKaratzas
* [Chore] Fix torch precision warning ([#30428](https://github.com/vllm-project/vllm/pull/30428)) by @yewentao256
* {Deprecation] Remove tokenizer setter ([#30400](https://github.com/vllm-project/vllm/pull/30400)) by @DarkLight1337
* [Chore] Delay recent deprecations ([#30398](https://github.com/vllm-project/vllm/pull/30398)) by @DarkLight1337
* [Deprecation] Remove deprecated task, seed and MM settings ([#30397](https://github.com/vllm-project/vllm/pull/30397)) by @DarkLight1337
* [Deprecation] Remove deprecated plugin and compilation fields for v0.13 release ([#30396](https://github.com/vllm-project/vllm/pull/30396)) by @DarkLight1337
* Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` ([#30389](https://github.com/vllm-project/vllm/pull/30389)) by @hmellor
* [Bugfix] Cache added_vocab to avoid per-token overhead ([#30351](https://github.com/vllm-project/vllm/pull/30351)) by @scratch-ml
* [CMake][Build]: Remove unused ACL CMake env variables ([#30339](https://github.com/vllm-project/vllm/pull/30339)) by @Radu2k
* [Bugfix] Fix DeepGEMM after #29546  ([#30267](https://github.com/vllm-project/vllm/pull/30267)) by @zhewenl
* Bump actions/stale from 10.1.0 to 10.1.1 ([#30234](https://github.com/vllm-project/vllm/pull/30234)) by @dependabot
* [Perf] Remove sync point in vit torch sdpa attn backend ([#30232](https://github.com/vllm-project/vllm/pull/30232)) by @DamonJiang777
* [LMCache] Fix breakage due to new LMCache version ([#30216](https://github.com/vllm-project/vllm/pull/30216)) by @njhill
* [Misc][Core] Remove unused `req_index` increment in scheduler ([#30176](https://github.com/vllm-project/vllm/pull/30176)) by @ivanium
* [Chore] Deprecate `SupportsMultiModal.merge_by_field_config` ([#30170](https://github.com/vllm-project/vllm/pull/30170)) by @DarkLight1337
* [Frontend] Remove confusing -O.xx flag error ([#30169](https://github.com/vllm-project/vllm/pull/30169)) by @gmagogsfm
* [CI]: Remove unnecessary imports from test_lmache_integration ([#30157](https://github.com/vllm-project/vllm/pull/30157)) by @sammshen
* Better error when world size is larger than node and `distributed_executor_backend` is not set ([#30140](https://github.com/vllm-project/vllm/pull/30140)) by @hmellor
* Fix AWQ MoE marlin check issue in marlin_utils.py for AMD backend ([#30102](https://github.com/vllm-project/vllm/pull/30102)) by @yuttian1
* [FIX]Patch run-cluster.sh (fix for #28328) ([#30002](https://github.com/vllm-project/vllm/pull/30002)) by @evberrypi
* [Frontend] Remove deprecated -O.xx flag ([#29991](https://github.com/vllm-project/vllm/pull/29991)) by @gmagogsfm
* [responsesAPI][7] Browser, Container MCP tools for non harmony models ([#29989](https://github.com/vllm-project/vllm/pull/29989)) by @qandrew
* Support multiple image/audio embeddings per requests ([#29988](https://github.com/vllm-project/vllm/pull/29988)) by @jeremyteboul
* [Cleanup] Refactor profiling env vars into a CLI config ([#29912](https://github.com/vllm-project/vllm/pull/29912)) by @benchislett
* [Attention] Make `split_decodes_and_prefills(..., require_uniform=True)` support padding ([#29644](https://github.com/vllm-project/vllm/pull/29644)) by @LucasWilkinson
* [Perf] Enable cuda graph for deepepHT, 5.3% throughput improvement, 4.4% TTFT improvement ([#29558](https://github.com/vllm-project/vllm/pull/29558)) by @yewentao256
* [Perf] Deepgemm fused layout kernel for activations, 4.3% throughput improvement, 10.7% TTFT improvement. ([#29546](https://github.com/vllm-project/vllm/pull/29546)) by @yewentao256
* [NIXL] Add compatibility checking to NIXL KV connector handshake ([#29503](https://github.com/vllm-project/vllm/pull/29503)) by @markmc
* [MoE][Refactor] Remove most arguments to FusedMoEMethodBase.apply ([#29066](https://github.com/vllm-project/vllm/pull/29066)) by @bnellnm
* [Model][7/N] Improve all pooling task | Deprecation as_reward_model. Extract hidden states prefer using new multi-vector retrieval API ([#26686](https://github.com/vllm-project/vllm/pull/26686)) by @noooop
* [Attention][UX][1/N] Add AttentionConfig and change attention env vars to CLI arguments ([#26315](https://github.com/vllm-project/vllm/pull/26315)) by @MatthewBonanni
* [Compile] Conditional compilation. Introduce compile_ranges ([#24252](https://github.com/vllm-project/vllm/pull/24252)) by @ilmarkov

## Upgrade Notes

- <h3>Dependency Upgrades</h3>
- <li>Upgrade eslint-config-prettier from 8.10.0 to 10.1.8 by <a href="https://github.com/dependabot"><code>@​dependabot</code></a> in <a href="https://redirect.github.com/actions/stale/pull/1276">actions/stale#1276</a></li>
- <li>Upgrade <code>@​types/node</code> from 20.10.3 to 24.2.0 and document breaking changes in v10 by <a href="https://github.com/dependabot"><code>@​dependabot</code></a> in <a href="https://redirect.github.com/actions/stale/pull/1280">actions/stale#1280</a></li>
- - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- - upgrade nvshmem from 3.3.9 to 3.3.24 to support CUDA 13
- Download a quantized (note: *not* BF16) Qwen3 MoE model GGUF file.

## Contributors

@Alexei-V-Ivanov-AMD, @AndreasKaratzas, @AuruTus, @Bhanu068, @Concurrensee, @DamonJiang777, @DarkLight1337, @Edwardf0t1, @ElizaWszola, @Elm8116, @FENP, @HF-001, @Harry-Chen, @Isotr0py, @IwakuraRein, @JaviS-Rei, @Jialin, @LucasWilkinson, @MatthewBonanni, @NickLucche, @PatrykSaffer, @Radu2k, @Rohan138, @SageMoore, @WoosukKwon, @ZJY0516, @ZhijianJiang, @ZhiweiYan-96, @a4lg, @aditew01, @ajpqs, @alecsolder, @alexm-redhat, @amd-hhashemi, @amitz-nv, @andrewbriand, @andyxning, @angelayi, @anker-c2, @bangshengtang, @baonudesifeizhai, @benchislett, @bnellnm, @charlifu, @charlotte12l, @chaunceyjiang, @czhu-cohere, @daniel-salib, @dcampora, @debroy-rh, @dependabot, @divakar-amd, @dmitry-tokarev-nv, @dongbo910220, @dtrifiro, @eicherseiji, @evberrypi, @faaany, @fadara01, @gh-wf, @gjc0824, @gmagogsfm, @gnovack, @haoyangli-amd, @hdlj-h, @hickeyma, @hmellor, @hyongtao-code, @hzxuzhonghu, @ilmarkov, @ioghiban, @ivanium, @jcyang43, @jeejeelee, @jeremyteboul, @jiahanc, @jinzhen-lin, @juliendenize, @khluu, @kitaekatt, @kmaehashi, @kylesayrs, @laithsakka, @liangel-02, @markmc, @maxyanghu, @mgoin, @micah-wil, @minosfuture, @njhill, @noooop, @nvpohanh, @nwaughachukwuma, @orozery, @petersalas, @piood, @pisceskkk, @princepride, @ptovam, @qandrew, @quanliu1991, @rasmith, @redwrasse, @rjrock, @rogeryoungh, @roikoren755, @russellb, @sammshen, @scratch-ml, @shaharmor98, @shivampr, @simon-mo, @strinczer, @vkuzo, @vllmellm, @weiguihua2, @wilsonwu, @wseaton, @wz1qqx, @wzshiming, @xu-song, @xyDong0223, @yeqcharlotte, @yewentao256, @yiliu30, @yongming-qin, @yuttian1, @zhewenl, @zhxchen17, @ziliangpeng