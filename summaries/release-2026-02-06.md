# Weekly Release Report for vllm-project/vllm (2026-02-06)

This week merged 250 PRs from 134 contributors. Key areas: features 8, fixes 70, performance 20.

## Executive Summary

本周发布主要包含新功能引入、稳定性改进和性能优化。特性方面，引入了新的Completions和Tokenize API Renderer，以及用于分布式训练的Native Weight Syncing API (NCCL)。模型支持扩展至Mistral Large 3 (Flashinfer MoE) 和 ColBERT late interaction 模型。修复工作覆盖广泛，重点解决了DeepSeek V3.2/V2、Kimi-K2.5、Qwen Omni系列等多模型在权重加载、分词器、多模态处理和MoE路由等方面的关键问题，并修复了LoRA FP8、稀疏MLA等组件缺陷。

性能方面，针对torch.compile的冷启动时间进行了优化与调整，并优化了流式返回性能。本次更新还包含对Super B200 TP2的MoE配置支持，以及对Transformers v5的兼容性修复。整体致力于提升系统稳定性、扩展模型兼容性及优化运行时效率。

## Highlights

* [Frontend] Use new Renderer for Completions and Tokenize API ([#32863](https://github.com/vllm-project/vllm/pull/32863)) by @DarkLight1337
* [Feat][RL][1/2] Native Weight Syncing API: NCCL ([#31943](https://github.com/vllm-project/vllm/pull/31943)) by @hao-aaron
* Add support for Mistral Large 3 inference with Flashinfer MoE ([#33174](https://github.com/vllm-project/vllm/pull/33174)) by @dbari
* feat: Add ColBERT late interaction model support ([#33686](https://github.com/vllm-project/vllm/pull/33686)) by @ieBoytsov
* [CI][torch.compile] Reduce e2e fusion test time ([#33293](https://github.com/vllm-project/vllm/pull/33293)) by @ProExpertProg

## Features & Enhancements

* feat: Add ColBERT late interaction model support ([#33686](https://github.com/vllm-project/vllm/pull/33686)) by @ieBoytsov
* Add MoE config for Super B200 TP2 ([#33510](https://github.com/vllm-project/vllm/pull/33510)) by @shaharmor98
* Support clear mm and encoder cache ([#33452](https://github.com/vllm-project/vllm/pull/33452)) by @jma99fb
* support return prompt token ids in responses  ([#33378](https://github.com/vllm-project/vllm/pull/33378)) by @cmunley1
* Feat/add nemotron nano v3 tests ([#33345](https://github.com/vllm-project/vllm/pull/33345)) by @shaharmor98
* Support FP8 block quant for CompressedTensorsW8A16Fp8 ([#33280](https://github.com/vllm-project/vllm/pull/33280)) by @mgoin
* Add EAGLE3 support for AFMoE ([#33111](https://github.com/vllm-project/vllm/pull/33111)) by @AutumnAurelium
* feat(frontend): early-fail tokenization guard for user requests ([#31366](https://github.com/vllm-project/vllm/pull/31366)) by @scratch-ml

## Bug Fixes

* [Bugfix] Fix DSV3.2 NVFP4 ([#33932](https://github.com/vllm-project/vllm/pull/33932)) by @MatthewBonanni
* Fix tokenizer test for renamed attr on Transformers v5 ([#33902](https://github.com/vllm-project/vllm/pull/33902)) by @hmellor
* [Bugfix] Fix corner case of sparse embedding  ([#33886](https://github.com/vllm-project/vllm/pull/33886)) by @noooop
* [BugFix] Fix LoRA Fp8 ([#33879](https://github.com/vllm-project/vllm/pull/33879)) by @danisereb
* [Bugfix] Fix Kimi-K2.5 NVFP4 checkpoints weight loading ([#33876](https://github.com/vllm-project/vllm/pull/33876)) by @Isotr0py
* [Bugfix] Kimi-K2 grouped_topk usage for Flashinfer monolithic kernels. ([#33858](https://github.com/vllm-project/vllm/pull/33858)) by @pavanimajety
* [CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so test_rocm_aiter_topk.py can run correctly ([#33840](https://github.com/vllm-project/vllm/pull/33840)) by @rasmith
* [Bugfix] Fix ScoreMultiModalParam multi-document scoring returning single result ([#33837](https://github.com/vllm-project/vllm/pull/33837)) by @AndreasKaratzas
* [Bugfix] Fix DeepSeek v3.2 tokenizer outputting None issue ([#33832](https://github.com/vllm-project/vllm/pull/33832)) by @wzhao18
* [Bugfix] Make MM batching more robust ([#33817](https://github.com/vllm-project/vllm/pull/33817)) by @DarkLight1337
* [Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss ([#33800](https://github.com/vllm-project/vllm/pull/33800)) by @simondanielsson
* [Bugfix] Fix swapped engine_ids in NIXL Llama 4 local attention path ([#33795](https://github.com/vllm-project/vllm/pull/33795)) by @zackyoray
* [Bugfix] Fix interns1-pro initialization and PP ([#33793](https://github.com/vllm-project/vllm/pull/33793)) by @Isotr0py
* [Bugfix] Define router_logits_dtype for remaining MoE models ([#33737](https://github.com/vllm-project/vllm/pull/33737)) by @mgoin
* [BugFix][Spec Decoding] Fix negative accepted tokens metric crash ([#33729](https://github.com/vllm-project/vllm/pull/33729)) by @njhill
* [CPU][BugFix] Allow w8a8 oneDNN quantized matmul to support 3D inputs ([#33727](https://github.com/vllm-project/vllm/pull/33727)) by @fadara01
* [Bugfix][ROCm] Include float8_e4m3fnuz in NCCL Dtype Dispatching ([#33713](https://github.com/vllm-project/vllm/pull/33713)) by @micah-wil
* [Bugfix] Fix torchrun PP broadcast deadlock with async scheduling ([#33701](https://github.com/vllm-project/vllm/pull/33701)) by @Isotr0py
* [Bugfix] Fix startup hang for Granite Speech ([#33699](https://github.com/vllm-project/vllm/pull/33699)) by @DarkLight1337
* [Bugfix] Fix ubatch wrapper num_tokens calculate ([#33694](https://github.com/vllm-project/vllm/pull/33694)) by @jiangkuaixue123
* [Bugfix] Fix step3p5 parser when using mtp ([#33690](https://github.com/vllm-project/vllm/pull/33690)) by @mariohong128
* Fix Gemma3n audio encoder for Transformers v5 ([#33673](https://github.com/vllm-project/vllm/pull/33673)) by @hmellor
* [Bugfix] Do not add extra \n for image-only cases when constructing multimodal text prompts. ([#33647](https://github.com/vllm-project/vllm/pull/33647)) by @noooop
* [Bugfix] fix qwen3-asr response error ([#33644](https://github.com/vllm-project/vllm/pull/33644)) by @jesse996
* [Bugfix][Model] Fix DeepSeek-OCR-2 chat template to include BOS token ([#33642](https://github.com/vllm-project/vllm/pull/33642)) by @l4b4r4b4b4
* [Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200 ([#33637](https://github.com/vllm-project/vllm/pull/33637)) by @chaunceyjiang
* [Bugfix] Interleaved thinking keeps compatibility with reasoning_content ([#33635](https://github.com/vllm-project/vllm/pull/33635)) by @chaunceyjiang
* [Bugfix] Fix mm budget setting for Qwen Omni models ([#33634](https://github.com/vllm-project/vllm/pull/33634)) by @ywang96
* [torch.compile] Don't do the fast moe cold start optimization if there is speculative decoding ([#33624](https://github.com/vllm-project/vllm/pull/33624)) by @zou3519
* [Bugfix] Disable RoutingMethodType.[Renormalize,RenormalizeNaive] for TRTLLM per-tensor FP8 MoE ([#33620](https://github.com/vllm-project/vllm/pull/33620)) by @mgoin
* [Bugfix] Disable TRTLLM FP8 MoE if router_logits_dtype==float32 and routing_method!=DeepSeekV3 ([#33613](https://github.com/vllm-project/vllm/pull/33613)) by @mgoin
* [Bugfix][Model] Fix audio-in-video support for Qwen2.5-Omni and Qwen3-Omni       ([#33605](https://github.com/vllm-project/vllm/pull/33605)) by @linyueqian
* [Bugfix] Fix sparse MLA metadata building ([#33579](https://github.com/vllm-project/vllm/pull/33579)) by @MatthewBonanni
* [Bugfix] Enable Kimi k25 processor test ([#33562](https://github.com/vllm-project/vllm/pull/33562)) by @Isotr0py
* [CI][Bugfix] Fix flaky `tests/v1/kv_connector/unit/test_multi_connector.py::test_multi_example_connector_consistency` ([#33555](https://github.com/vllm-project/vllm/pull/33555)) by @NickLucche
* Fix mistral sliding window parsing ([#33521](https://github.com/vllm-project/vllm/pull/33521)) by @andylolu2
* fix(ROCm): Make flash_attn import optional in MLA attention ([#33511](https://github.com/vllm-project/vllm/pull/33511)) by @rabi
* Fix DeepSeek V2 RoPE initialization error ([#33501](https://github.com/vllm-project/vllm/pull/33501)) by @catswe
* fix: only include Authorization header when OPENAI_API_KEY is set ([#33488](https://github.com/vllm-project/vllm/pull/33488)) by @zack041
* fix QERL attention import path ([#33432](https://github.com/vllm-project/vllm/pull/33432)) by @vkuzo
* [Bugfix] Fix typo in read_offset variable name ([#33426](https://github.com/vllm-project/vllm/pull/33426)) by @bet0x
* fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels ([#33417](https://github.com/vllm-project/vllm/pull/33417)) by @renehonig
* Fix `test_moe.py` for Transformers v5 ([#33413](https://github.com/vllm-project/vllm/pull/33413)) by @hmellor
* [BUGFIX] Pixtral cannot be loaded with --limit-mm-per-prompt 0 ([#33406](https://github.com/vllm-project/vllm/pull/33406)) by @juliendenize
* [BugFix][LoRA] TritonExperts is ModularMoEPath for FP8 models ([#33393](https://github.com/vllm-project/vllm/pull/33393)) by @dcmaddix
* [Bugfix][Async][Connector] avoid vllm-side double free during async scheduling + request abort + async KV cache transfer ([#33377](https://github.com/vllm-project/vllm/pull/33377)) by @KuntaiDu
* fix: allow LFM2 MoE prefix caching (align) ([#33376](https://github.com/vllm-project/vllm/pull/33376)) by @tianshu-Michael-yu
* [Bugfix][ROCm] Fixing the skinny gemm dispatch logic from #32831 ([#33366](https://github.com/vllm-project/vllm/pull/33366)) by @gshtras
* [BugFix] Fix whisper FA2 + full cudagraphs ([#33360](https://github.com/vllm-project/vllm/pull/33360)) by @LucasWilkinson
* Fix `tie_word_embeddings` for multimodal models in Transformers v5 ([#33359](https://github.com/vllm-project/vllm/pull/33359)) by @hmellor
* [BugFix] Disable async scheduling for Mamba prefix caching ([#33352](https://github.com/vllm-project/vllm/pull/33352)) by @peakcrosser7
* [MISC] Fix Tensor Parallelism for Quantized Mamba Models with n_groups=1 ([#33257](https://github.com/vllm-project/vllm/pull/33257)) by @vadiklyutiy
* Fix encoder-decoder model disabling mm processor cache ([#33236](https://github.com/vllm-project/vllm/pull/33236)) by @hmellor
* [Bugfix] GLM-4 tool parser: incremental string streaming ([#33218](https://github.com/vllm-project/vllm/pull/33218)) by @QwertyJack
* [Bugfix] Handle Asym W4A16 (ConchLinearKernel) for CT ([#33200](https://github.com/vllm-project/vllm/pull/33200)) by @mgehre-amd
* [Bugfix] Disable TRTLLM attention when KV transfer is enabled ([#33192](https://github.com/vllm-project/vllm/pull/33192)) by @ZhanqiuHu
* Fix grammar ([#33121](https://github.com/vllm-project/vllm/pull/33121)) by @smashyalts
* [Bugfix] Early-reject requests with MM data longer than encode cache capacity ([#33110](https://github.com/vllm-project/vllm/pull/33110)) by @YunzhuLu
* [BugFix] Add synchronize in CutlassW4A8LinearKernel to ensure data is ready for use. ([#33078](https://github.com/vllm-project/vllm/pull/33078)) by @ayrnb
* [BUGFIX] Fix hipErrorIllegalState in Qwen3-Omni during startup profiling allow inference Omni on ROCM ([#33077](https://github.com/vllm-project/vllm/pull/33077)) by @JartX
* [BugFix][Router Replay] Capture Logical Experts with EPLB ([#33013](https://github.com/vllm-project/vllm/pull/33013)) by @HollowMan6
* [Bugfix]: Fix display errors in TORCH_CHECK messages ([#32942](https://github.com/vllm-project/vllm/pull/32942)) by @lingebeng
* [CI][Bugfix]: return McpCall for built-in MCP tools in non-streaming mode ([#32762](https://github.com/vllm-project/vllm/pull/32762)) by @AndreasKaratzas
* [BugFix] DPMetadata raises assert error for dense model ([#32739](https://github.com/vllm-project/vllm/pull/32739)) by @River12
* Fix quantized Falcon-H1 model loading issues ([#32728](https://github.com/vllm-project/vllm/pull/32728)) by @shengliangxu
* Fix accessing hidden_act from model config ([#32686](https://github.com/vllm-project/vllm/pull/32686)) by @grzegorz-k-karch
* [BugFix] scheduler: Delay freeing blocks of aborted async loads ([#32255](https://github.com/vllm-project/vllm/pull/32255)) by @orozery
* fix cutlass_3x_gemm_fp8_blockwise on sm103a ([#32224](https://github.com/vllm-project/vllm/pull/32224)) by @IwakuraRein
* fix memory for online fp8 quantization with streaming weight load ([#31914](https://github.com/vllm-project/vllm/pull/31914)) by @vkuzo
* [Bugfix] Suppress non-TTY color output on the process name part of the log ([#29714](https://github.com/vllm-project/vllm/pull/29714)) by @a4lg

## Performance

* Revert "[Attention][FA3] Update FA3 to include new swizzle optimization" ([#33841](https://github.com/vllm-project/vllm/pull/33841)) by @ProExpertProg
* Revert "[torch.compile] Significantly speed up cold start times" ([#33820](https://github.com/vllm-project/vllm/pull/33820)) by @zou3519
* [Perf] Optimize chat completion streaming performance ([#33782](https://github.com/vllm-project/vllm/pull/33782)) by @chaunceyjiang
* [torch.compile] Significantly speed up cold start times ([#33641](https://github.com/vllm-project/vllm/pull/33641)) by @zou3519
* Save startup benchmark results as a list of values ([#33629](https://github.com/vllm-project/vllm/pull/33629)) by @huydhn
* [Perf] Optimize spec decoding + async scheduling, 1.5% Throughput improvement ([#33612](https://github.com/vllm-project/vllm/pull/33612)) by @yewentao256
* [Perf] Disable clean_logits in deepgemm fp8_mqa_logits kernel ([#33568](https://github.com/vllm-project/vllm/pull/33568)) by @xyang16
* [Perf] Optimize the performance of structured output + reasoning ([#33557](https://github.com/vllm-project/vllm/pull/33557)) by @chaunceyjiang
* Adds padding and perf improvements to wvSplitK_fp8 ([#33527](https://github.com/vllm-project/vllm/pull/33527)) by @amd-hhashemi
* Change defaults for vllm bench startup ([#33489](https://github.com/vllm-project/vllm/pull/33489)) by @ProExpertProg
* [cohere] [misc] support arbitrary MM datasets in spec dec bench ([#33486](https://github.com/vllm-project/vllm/pull/33486)) by @kkt-cohere
* [ModelRunner V2] Misc minor simplifications and optimizations ([#33467](https://github.com/vllm-project/vllm/pull/33467)) by @njhill
* [PERF] Change GDN Attention State Layout from [N, HV, K, V] to [N, HV, V, K] ([#33291](https://github.com/vllm-project/vllm/pull/33291)) by @vadiklyutiy
* Add support for Mistral Large 3 inference with Flashinfer MoE ([#33174](https://github.com/vllm-project/vllm/pull/33174)) by @dbari
* [Quantization][ROCm] Fix MoE weight loading to be robust (Qwen3_MoE/Qwen3_next as example models) ([#33173](https://github.com/vllm-project/vllm/pull/33173)) by @xuebwang-amd
* Indicate compile mode in the benchmark results ([#32990](https://github.com/vllm-project/vllm/pull/32990)) by @huydhn
* [Spec Decode] Unified Parallel Drafting ([#32887](https://github.com/vllm-project/vllm/pull/32887)) by @benchislett
* Add unpermute-aware fused MoE LoRA path ([#32655](https://github.com/vllm-project/vllm/pull/32655)) by @RunkaiTao
* [perf] Integrate flashinfer concat_mla_k ([#31171](https://github.com/vllm-project/vllm/pull/31171)) by @jiahanc
* [Attention][FA3] Update FA3 to include new swizzle optimization ([#23465](https://github.com/vllm-project/vllm/pull/23465)) by @LucasWilkinson

## Model Support

* [Models] Consolidate Deepseek-OCR2 processor ([#33909](https://github.com/vllm-project/vllm/pull/33909)) by @Isotr0py
* [Misc] Update code for encoder-decoder models ([#33900](https://github.com/vllm-project/vllm/pull/33900)) by @DarkLight1337
* [Model] Apply #32631 for recent models ([#33785](https://github.com/vllm-project/vllm/pull/33785)) by @DarkLight1337
* [Models] Intern-S1-Pro ([#33636](https://github.com/vllm-project/vllm/pull/33636)) by @CUHKSZzxy
* [Voxtral models] Skip warm-up to skip confusing error message in warm-up ([#33576](https://github.com/vllm-project/vllm/pull/33576)) by @patrickvonplaten
* [Model] Use explicit types in `get_generation_prompt` ([#33551](https://github.com/vllm-project/vllm/pull/33551)) by @DarkLight1337
* [Models] Step-3.5-Flash ([#33523](https://github.com/vllm-project/vllm/pull/33523)) by @csy0225
* [cohere] [misc] skip target model mm emb in draft proposal step when draft is text-only ([#33437](https://github.com/vllm-project/vllm/pull/33437)) by @kkt-cohere
* [Misc] Algin Qwen3-VL-embedding image example outputs with HF repo example ([#33419](https://github.com/vllm-project/vllm/pull/33419)) by @Isotr0py
* [CI] Qwen3-ASR transcriptios tests ([#33414](https://github.com/vllm-project/vllm/pull/33414)) by @NickLucche
* [ModelRunner V2] Fix spec decoding + logprobs ([#33391](https://github.com/vllm-project/vllm/pull/33391)) by @njhill
* [ModelRunner V2] Support spec decode with structured outputs ([#33374](https://github.com/vllm-project/vllm/pull/33374)) by @njhill
* [Models] Refactor Kimi-K2.5 weight loading ([#33346](https://github.com/vllm-project/vllm/pull/33346)) by @Isotr0py
* [Doc] add missing model entries in supported_models.md ([#33220](https://github.com/vllm-project/vllm/pull/33220)) by @pacoxu
* Refactor NVFP4 Linear utils for ModelOpt and CT ([#33201](https://github.com/vllm-project/vllm/pull/33201)) by @mgoin
* [Model] Support DeepSeek-OCR-2 ([#33165](https://github.com/vllm-project/vllm/pull/33165)) by @LiuLi1998
* [Model] Use mm_position to compute mrope positions for GLM-4.xV ([#33039](https://github.com/vllm-project/vllm/pull/33039)) by @KKSK-DON
* [Model][Multimodal] Add explicit MusicFlamingo adapter ([#32696](https://github.com/vllm-project/vllm/pull/32696)) by @WangHaoyuuu
* [ROCM] Enable aiter attn backend for qwen3-next model ([#32492](https://github.com/vllm-project/vllm/pull/32492)) by @jennyyyyzhen
* [model] Add support for openPangu7B-VL ([#32449](https://github.com/vllm-project/vllm/pull/32449)) by @hujiaxin0
* [Model] Add transcription support for Qwen3-Omni ([#29828](https://github.com/vllm-project/vllm/pull/29828)) by @mu-hashmi

## Hardware & Backend

* [CI/Build] Fix CPU CI test case title ([#33870](https://github.com/vllm-project/vllm/pull/33870)) by @bigPYJ1151
* [CI/Build] Parallelize CPU CI tests ([#33778](https://github.com/vllm-project/vllm/pull/33778)) by @bigPYJ1151
* Apply #33621 to main ([#33758](https://github.com/vllm-project/vllm/pull/33758)) by @DarkLight1337
* Patch aiohttp for CVE-2025-69223 ([#33621](https://github.com/vllm-project/vllm/pull/33621)) by @zaristei2
* [UX] Format attention backend log line ([#33570](https://github.com/vllm-project/vllm/pull/33570)) by @MatthewBonanni
* Update huggingface-hub again ([#33567](https://github.com/vllm-project/vllm/pull/33567)) by @hmellor
* [ROCm][CI] Update huggingface-hub pin ([#33492](https://github.com/vllm-project/vllm/pull/33492)) by @AndreasKaratzas
* [Misc] Fix flashinfer related tests ([#33462](https://github.com/vllm-project/vllm/pull/33462)) by @esmeetu
* [Doc] [ROCm] Update Documentation to reflect v0.15.0 release ([#33388](https://github.com/vllm-project/vllm/pull/33388)) by @vllmellm
* [Moe Refactor] Make Inplace Flag for FusedMoEModularKernel part of the constructor ([#33375](https://github.com/vllm-project/vllm/pull/33375)) by @bnellnm
* [CI][AMD] Skip 4 GPUs testgroup ray tests ([#33305](https://github.com/vllm-project/vllm/pull/33305)) by @rjrock
* [Attention] Move MLA `forward` from backend to layer ([#33284](https://github.com/vllm-project/vllm/pull/33284)) by @MatthewBonanni
* [2/N] move responses/serving _make_response_output_items logic to parser ([#33281](https://github.com/vllm-project/vllm/pull/33281)) by @qandrew
* [ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_state_loader to reduce flakiness ([#33277](https://github.com/vllm-project/vllm/pull/33277)) by @micah-wil
* [CPU][IBM Z][Dockerfile] Fix IBM Z builds ([#33243](https://github.com/vllm-project/vllm/pull/33243)) by @R3hankhan123
* [Kernel] [Helion] [3/N] Helion kernel registry ([#33203](https://github.com/vllm-project/vllm/pull/33203)) by @gmagogsfm
* [CPU][Feat] Enable KleidiAI accelerated int4 dynamic quant with BF16 activations on Arm CPUs ([#33122](https://github.com/vllm-project/vllm/pull/33122)) by @fadara01
* [W8A8 Block Linear Refactor][1/N] Keep all quantization types into `QuantFP8` class. ([#33047](https://github.com/vllm-project/vllm/pull/33047)) by @maralbahari
* [Kernel] [Helion] [2/N] Helion kernel wrapper ([#32964](https://github.com/vllm-project/vllm/pull/32964)) by @gmagogsfm
* [Hardware][AMD][CI] Refactor AMD tests to properly use BuildKite parallelism ([#32745](https://github.com/vllm-project/vllm/pull/32745)) by @mawong-amd
* [Kernel] [Helion] [1/N] Add Helion ConfigManager ([#32740](https://github.com/vllm-project/vllm/pull/32740)) by @gmagogsfm
* [Hardware][SM100] Add TRTLLM Kernel for INT4 W4A16 Kernel. ([#32437](https://github.com/vllm-project/vllm/pull/32437)) by @pavanimajety
* [Doc] Enhance documentation around CPU container images ([#32286](https://github.com/vllm-project/vllm/pull/32286)) by @nathan-weinberg
* [CPU] Split attention dispatch by head_dim alignment ([#32161](https://github.com/vllm-project/vllm/pull/32161)) by @R3hankhan123
* [CI/Build] add directions for CPU image upload to Docker Hub ([#32032](https://github.com/vllm-project/vllm/pull/32032)) by @nathan-weinberg
*   Reduce the kernel overhead when num of active loras is smaller than max loras. Multiple cuda graphs are captured for each num of active-loras. ([#32005](https://github.com/vllm-project/vllm/pull/32005)) by @yugong333

## Refactoring & Core

* [Refactor] Move `task` outside of `PoolingParams.verify` ([#33796](https://github.com/vllm-project/vllm/pull/33796)) by @DarkLight1337
* [Refactor] Clean up pooling serial utils ([#33665](https://github.com/vllm-project/vllm/pull/33665)) by @DarkLight1337
* [Refactor] Move profiling methods to MM budget ([#33559](https://github.com/vllm-project/vllm/pull/33559)) by @DarkLight1337
* [Refactor] Make Renderer an abstract class ([#33479](https://github.com/vllm-project/vllm/pull/33479)) by @DarkLight1337
* [Refactor] Move MM data parsing outside processor ([#33408](https://github.com/vllm-project/vllm/pull/33408)) by @DarkLight1337
* [Refactor] Move MM item count validation outside of processor ([#33396](https://github.com/vllm-project/vllm/pull/33396)) by @DarkLight1337
* [Realtime API] Adds minimal realtime API based on websockets ([#33187](https://github.com/vllm-project/vllm/pull/33187)) by @patrickvonplaten
* [1/N] Initial Implementation of Parser for ResponsesAPI ([#32712](https://github.com/vllm-project/vllm/pull/32712)) by @qandrew
* [Frontend] Add sampling parameters to Responses API ([#32609](https://github.com/vllm-project/vllm/pull/32609)) by @DanielMe

## Build, CI & Testing

* [CI/Build] Investigate torchrun distributed tests hanging issue ([#33650](https://github.com/vllm-project/vllm/pull/33650)) by @Isotr0py
* [CI] Add DeepSeek V3.2 nightly eval ([#33566](https://github.com/vllm-project/vllm/pull/33566)) by @MatthewBonanni
* use ORJSONResponse when available to improve the efficiency of request process ([#33548](https://github.com/vllm-project/vllm/pull/33548)) by @staugust
* Explicitly set `return_dict` for `apply_chat_template` ([#33372](https://github.com/vllm-project/vllm/pull/33372)) by @hmellor
* [CI][HPU]accelerate hpu test by skip python re-install and clean container name ([#33286](https://github.com/vllm-project/vllm/pull/33286)) by @xuechendi
* [CI] Enable mypy import following for `vllm/spec_decode` ([#33282](https://github.com/vllm-project/vllm/pull/33282)) by @Lucaskabela
* [Feature] OTEL tracing during loading ([#31162](https://github.com/vllm-project/vllm/pull/31162)) by @emricksini-h

## Documentation

* [Docs] Add reo analytics ([#33957](https://github.com/vllm-project/vllm/pull/33957)) by @simon-mo
* [Docs] Add bart-plugin to docs ([#33905](https://github.com/vllm-project/vllm/pull/33905)) by @NickLucche
* [docs] fix unintentional misspellings ([#33863](https://github.com/vllm-project/vllm/pull/33863)) by @rinbaro
* [Voxtral Realtime] Change name ([#33716](https://github.com/vllm-project/vllm/pull/33716)) by @patrickvonplaten
* [Feature] Enable `TRITON_ATTN` for Batch Invariance ([#33688](https://github.com/vllm-project/vllm/pull/33688)) by @frankwang28
* [torch.compile] Document the workaround to standalone_compile failing ([#33571](https://github.com/vllm-project/vllm/pull/33571)) by @zou3519
* Document NixlConnector backend selection via kv_connector_extra_config ([#33552](https://github.com/vllm-project/vllm/pull/33552)) by @KrxGu
* [UX] Use gguf `repo_id:quant_type` syntax for examples and docs ([#33371](https://github.com/vllm-project/vllm/pull/33371)) by @mgoin
* Enable Cross layers KV cache layout at NIXL Connector V2 ([#33339](https://github.com/vllm-project/vllm/pull/33339)) by @liranschour
* [Frontend][4/n] Make pooling entrypoints request schema consensus | ScoreRequest ([#33060](https://github.com/vllm-project/vllm/pull/33060)) by @noooop

## Miscellaneous

* [Misc] Add debug logs ([#33931](https://github.com/vllm-project/vllm/pull/33931)) by @NickLucche
* [Misc] Rename `translations` to `speech_to_text` for OAI serving component ([#33904](https://github.com/vllm-project/vllm/pull/33904)) by @NickLucche
* [Minor] Include `StreamingInput` in inputs package ([#33856](https://github.com/vllm-project/vllm/pull/33856)) by @njhill
* [release] Minor fixes to release annotation ([#33849](https://github.com/vllm-project/vllm/pull/33849)) by @khluu
* [MM] Align the prefix of MMEncoderAttention with Attention ([#33750](https://github.com/vllm-project/vllm/pull/33750)) by @shen-shanshan
* [MM] Pass `prefix` parameter to MMEncoderAttention ([#33674](https://github.com/vllm-project/vllm/pull/33674)) by @shen-shanshan
* [Core] Don't schedule spec tokens with prefill chunks ([#33652](https://github.com/vllm-project/vllm/pull/33652)) by @njhill
* Patch Protobuf for CVE 2026-0994 ([#33619](https://github.com/vllm-project/vllm/pull/33619)) by @zaristei2
* [Release] Fix format and cherry-pick ([#33618](https://github.com/vllm-project/vllm/pull/33618)) by @zhewenl
* [Minor] Some code simplification in `scheduler.py` ([#33597](https://github.com/vllm-project/vllm/pull/33597)) by @njhill
* [compile] Clean up AOT compile bypass on evaluate_guards. ([#33578](https://github.com/vllm-project/vllm/pull/33578)) by @zhxchen17
* [Voxtral Realtime] Introduce global log mel max ([#33574](https://github.com/vllm-project/vllm/pull/33574)) by @patrickvonplaten
* [Feature][Core] Support Fabric detection to adapt the MNNVL protocol for the GB series ([#33540](https://github.com/vllm-project/vllm/pull/33540)) by @kebe7jun
* Update get_expert_mapping to include self parameter ([#33525](https://github.com/vllm-project/vllm/pull/33525)) by @Otsutsukii
* [Redo] #33110 with threading limit ([#33502](https://github.com/vllm-project/vllm/pull/33502)) by @DarkLight1337
* [Critical] Revert #33110 ([#33500](https://github.com/vllm-project/vllm/pull/33500)) by @DarkLight1337
* [Minor] Sort safetensors files to ensure deterministic loading order ([#33491](https://github.com/vllm-project/vllm/pull/33491)) by @Lumosis
* Update `huggingface-hub` pin for the last time before Transformers v5 ([#33473](https://github.com/vllm-project/vllm/pull/33473)) by @hmellor
* [Misc] offest -> offset in comments and variable names ([#33444](https://github.com/vllm-project/vllm/pull/33444)) by @russellb
* [fix][torch.compile] Fix cold-start compilation time increase by adding kv cache update to splitting ops ([#33441](https://github.com/vllm-project/vllm/pull/33441)) by @ProExpertProg
* pin LMCache to v0.3.9 or greater with vLLM v0.15.0 ([#33440](https://github.com/vllm-project/vllm/pull/33440)) by @Gregory-Pereira
* [Attention] Clarify comment explaining attn_logits +1 dimension ([#33427](https://github.com/vllm-project/vllm/pull/33427)) by @fuscof-ibm
* [Metrics] Add labeled prompt token metrics for P/D disaggregation ([#33290](https://github.com/vllm-project/vllm/pull/33290)) by @ZhanqiuHu
* Improve Mistral format checks. ([#33253](https://github.com/vllm-project/vllm/pull/33253)) by @juliendenize
* [Misc] support collect_env for endpoint /server_info ([#33246](https://github.com/vllm-project/vllm/pull/33246)) by @muma378
* Move decode context parallel validationn to `ParallelConfig` ([#33239](https://github.com/vllm-project/vllm/pull/33239)) by @hmellor
* [ez] Add structured torch.compile logs ([#33213](https://github.com/vllm-project/vllm/pull/33213)) by @angelayi
* [MoE] Enable Shared/Routed Overlap For Latent MoE (Nemotron-H) ([#32790](https://github.com/vllm-project/vllm/pull/32790)) by @danielafrimi
* Turn `@config` into a `dataclass_transform` ([#31541](https://github.com/vllm-project/vllm/pull/31541)) by @hmellor
* [KV Connector][Metrics] Do not count local prefix cache hits in connector queries ([#30522](https://github.com/vllm-project/vllm/pull/30522)) by @markmc

## Breaking Changes

* [Misc] Delay deprecation of CommonAttentionMetadata properties ([#33801](https://github.com/vllm-project/vllm/pull/33801)) by @LucasWilkinson
* [Bugfix] Fix `normalize` still being passed to `PoolerConfig` ([#33794](https://github.com/vllm-project/vllm/pull/33794)) by @DarkLight1337
* [XPU] remove common path warning log ([#33769](https://github.com/vllm-project/vllm/pull/33769)) by @jikunshang
* [Deprecation] Remove `_get_data_parser` in MM processor ([#33757](https://github.com/vllm-project/vllm/pull/33757)) by @DarkLight1337
* Implement zero-copy GQA for multimodal and CPU ([#33732](https://github.com/vllm-project/vllm/pull/33732)) by @voidbag
* [Deprecation] Deprecate profiling envs ([#33722](https://github.com/vllm-project/vllm/pull/33722)) by @yewentao256
* [Refactor] Remove unused dead code ([#33718](https://github.com/vllm-project/vllm/pull/33718)) by @yewentao256
* [compile] Remove runner type from ignored caching factor list. ([#33712](https://github.com/vllm-project/vllm/pull/33712)) by @zhxchen17
* [Refactor] Clean up input preprocessing ([#33687](https://github.com/vllm-project/vllm/pull/33687)) by @DarkLight1337
* Fix Gemma3 GGUF for Transformers v5 ([#33683](https://github.com/vllm-project/vllm/pull/33683)) by @hmellor
* Fix offline test for Transformers v5 ([#33682](https://github.com/vllm-project/vllm/pull/33682)) by @hmellor
* [XPU][2/N] add support unquantized moe support for xpu  ([#33659](https://github.com/vllm-project/vllm/pull/33659)) by @jikunshang
* [Misc] Update default image format of `encode_base64` ([#33656](https://github.com/vllm-project/vllm/pull/33656)) by @DarkLight1337
* [Release] patch step3p5 attention class in v0.15.1 release ([#33602](https://github.com/vllm-project/vllm/pull/33602)) by @zhewenl
* Change the type signature of MixtureOfExperts.expert_weights to MutableSequence[Sequence[Tensor]] ([#33573](https://github.com/vllm-project/vllm/pull/33573)) by @SageMoore
* Remove incorrect tokenizer info test ([#33565](https://github.com/vllm-project/vllm/pull/33565)) by @hmellor
* [CI/Build] Remove hardcoded America/Los_Angeles timezone from Dockerfiles ([#33553](https://github.com/vllm-project/vllm/pull/33553)) by @carlory
* [Chore] Remove redundant input parsing methods ([#33542](https://github.com/vllm-project/vllm/pull/33542)) by @DarkLight1337
* [Misc] Remove deprecated profiler environment variables ([#33536](https://github.com/vllm-project/vllm/pull/33536)) by @carlory
* [Misc] Remove deprecated VLLM_ALL2ALL_BACKEND environment variable ([#33535](https://github.com/vllm-project/vllm/pull/33535)) by @carlory
* [Nightly CI] Remove CT Model ([#33530](https://github.com/vllm-project/vllm/pull/33530)) by @robertgshaw2-redhat
* [Fix] prefix cache hit rate == 0 bug with gpt-oss style models ([#33524](https://github.com/vllm-project/vllm/pull/33524)) by @ivanium
* [Doc]: update paths for Offline/Online/Others example sections ([#33494](https://github.com/vllm-project/vllm/pull/33494)) by @soyr-redhat
* [Bugfix] Fix inconsistent handling of cache reset ([#33481](https://github.com/vllm-project/vllm/pull/33481)) by @DarkLight1337
* [Deprecation] Remove deprecated items related to pooling ([#33477](https://github.com/vllm-project/vllm/pull/33477)) by @DarkLight1337
* [Doc] Update plugin deprecation notices ([#33476](https://github.com/vllm-project/vllm/pull/33476)) by @DarkLight1337
* [Bugfix] Fix incompatibility between #33372 and #32863 ([#33475](https://github.com/vllm-project/vllm/pull/33475)) by @DarkLight1337
* [Misc] Replace deprecated interface seed_everything ([#33474](https://github.com/vllm-project/vllm/pull/33474)) by @esmeetu
* [Voxtral Streaming -> Voxtral Realtime] Rename all voxtral related classes, fn, files ([#33415](https://github.com/vllm-project/vllm/pull/33415)) by @patrickvonplaten
* [Bugfix] Fix `Qwen3ASR` language asr tag in output  ([#33410](https://github.com/vllm-project/vllm/pull/33410)) by @NickLucche
* Remove deprecated `reasoning_content` message field ([#33402](https://github.com/vllm-project/vllm/pull/33402)) by @hmellor
* [XPU][1/N] Deprecate ipex and switch to vllm-xpu-kernels for xpu platform ([#33379](https://github.com/vllm-project/vllm/pull/33379)) by @jikunshang
* [Models]: lfm2_siglip2 return intermediate encoder layers ([#33370](https://github.com/vllm-project/vllm/pull/33370)) by @lalo
* move spec decode slow test to test_areas.yaml ([#33365](https://github.com/vllm-project/vllm/pull/33365)) by @shanjiaz
* [Deprecation] Deprecate `seed_everything` and `scatter_mm_placeholders` in v0.15 ([#33362](https://github.com/vllm-project/vllm/pull/33362)) by @yewentao256
* [Dependency] Remove comments of ray in dependency files ([#33351](https://github.com/vllm-project/vllm/pull/33351)) by @yewentao256
* [Misc] Replace Optional[X] with X | None syntax ([#33332](https://github.com/vllm-project/vllm/pull/33332)) by @carlory
* [Misc] Clean up HIDDEN_DEPRECATED_METRICS after metric removal ([#33323](https://github.com/vllm-project/vllm/pull/33323)) by @carlory
* [rocm][ray] Fix: Unify Ray device visibility handling across CUDA and ROCm ([#33308](https://github.com/vllm-project/vllm/pull/33308)) by @kouroshHakha
* [CI][torch.compile] Reduce e2e fusion test time ([#33293](https://github.com/vllm-project/vllm/pull/33293)) by @ProExpertProg
* fix[ROCm]: Remove unconditional aiter import ([#32902](https://github.com/vllm-project/vllm/pull/32902)) by @rabi
* [Frontend] Use new Renderer for Completions and Tokenize API ([#32863](https://github.com/vllm-project/vllm/pull/32863)) by @DarkLight1337
* [perf] v1/spec_decode: skip softmax for all-greedy rejection sampling ([#32852](https://github.com/vllm-project/vllm/pull/32852)) by @caozuoba
* [ROCm][Bugfix][CI] Fix hybrid models and their tests (Mamba/Jamba/Bamba) ([#32710](https://github.com/vllm-project/vllm/pull/32710)) by @AndreasKaratzas
* Disable Cascade Attention for Batch Invariance ([#32561](https://github.com/vllm-project/vllm/pull/32561)) by @frankwang28
* [QeRL] Layerwise Reloading ([#32133](https://github.com/vllm-project/vllm/pull/32133)) by @kylesayrs
* [Feat][RL][1/2] Native Weight Syncing API: NCCL ([#31943](https://github.com/vllm-project/vllm/pull/31943)) by @hao-aaron
* [P/D] rework mooncake connector and introduce its bootstrap server ([#31034](https://github.com/vllm-project/vllm/pull/31034)) by @dtcccc
* [Feature][CPU Backend]: Optimize ARM vectorization backend ([#30329](https://github.com/vllm-project/vllm/pull/30329)) by @Radu2k

## Upgrade Notes

- As described in https://github.com/huggingface/transformers/blob/main/MIGRATION_GUIDE_V5.md#remote-code-incompatibility, `tokenization_utils` and `tokenization_utils_fast` have been removed and are aliased for backward compatibility.
- upgrade aiohttp to bypass [CVE-2025-69223](https://github.com/advisories/GHSA-6mq8-rvhq-8wgg) for the 0.15.1 release.
- ## Migration
- this PR also upgrade dependency to oneapi 2025.3 and pytorch 2.10 for xpu platform.
- No breaking change - default behavior unchanged.
- > **Note:** The `-1` adjustment is applied by the scheduler when all prompt tokens are cached (from local cache or KV transfer). This forces the model to recompute the last prompt token locally, since the model needs at least one input token to run a forward pass.
- NOTE: this is a manually stacked PR, each commit is reviewed separately. **For this PR, please only review the top commit**: [Core] Add register_kernel decorator and global kernel registry
- Note: No unit test added - we could not find a small asymmetric CT W4A16 model.

## Contributors

@AndreasKaratzas, @AutumnAurelium, @CUHKSZzxy, @DanielMe, @DarkLight1337, @Gregory-Pereira, @HollowMan6, @Isotr0py, @IwakuraRein, @JartX, @KKSK-DON, @KrxGu, @KuntaiDu, @LiuLi1998, @LucasWilkinson, @Lucaskabela, @Lumosis, @MatthewBonanni, @NickLucche, @Otsutsukii, @ProExpertProg, @QwertyJack, @R3hankhan123, @Radu2k, @River12, @RunkaiTao, @SageMoore, @WangHaoyuuu, @YunzhuLu, @ZhanqiuHu, @a4lg, @amd-hhashemi, @andylolu2, @angelayi, @ayrnb, @benchislett, @bet0x, @bigPYJ1151, @bnellnm, @caozuoba, @carlory, @catswe, @chaunceyjiang, @cmunley1, @csy0225, @danielafrimi, @danisereb, @dbari, @dcmaddix, @dtcccc, @emricksini-h, @esmeetu, @fadara01, @frankwang28, @fuscof-ibm, @gmagogsfm, @grzegorz-k-karch, @gshtras, @hao-aaron, @hmellor, @hujiaxin0, @huydhn, @ieBoytsov, @ivanium, @jennyyyyzhen, @jesse996, @jiahanc, @jiangkuaixue123, @jikunshang, @jma99fb, @juliendenize, @kebe7jun, @khluu, @kkt-cohere, @kouroshHakha, @kylesayrs, @l4b4r4b4b4, @lalo, @lingebeng, @linyueqian, @liranschour, @maralbahari, @mariohong128, @markmc, @mawong-amd, @mgehre-amd, @mgoin, @micah-wil, @mu-hashmi, @muma378, @nathan-weinberg, @njhill, @noooop, @orozery, @pacoxu, @patrickvonplaten, @pavanimajety, @peakcrosser7, @qandrew, @rabi, @rasmith, @renehonig, @rinbaro, @rjrock, @robertgshaw2-redhat, @russellb, @scratch-ml, @shaharmor98, @shanjiaz, @shen-shanshan, @shengliangxu, @simon-mo, @simondanielsson, @smashyalts, @soyr-redhat, @staugust, @tianshu-Michael-yu, @vadiklyutiy, @vkuzo, @vllmellm, @voidbag, @wzhao18, @xuebwang-amd, @xuechendi, @xyang16, @yewentao256, @yugong333, @ywang96, @zack041, @zackyoray, @zaristei2, @zhewenl, @zhxchen17, @zou3519