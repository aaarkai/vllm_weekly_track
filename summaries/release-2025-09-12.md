# Weekly Release Report for vllm-project/vllm (2025-09-12)

This week merged 266 PRs from 126 contributors. Key areas: features 10, fixes 14, performance 31.

## Executive Summary

本周发布聚焦于模型支持扩展与性能优化。新增了对qwen3-next混合注意力模型、Motif-1-Tiny、Whisper编码器-解码器模型及NemotronH Nano VLM的支持。性能方面针对Kimi K2和QWEN3的MoE内核进行了优化配置，并改进了多项基准测试与CUDA内存管理。修复了包括Inductor图形输出形状、BLOOM模型实现一致性、异步任务泄漏在内的关键问题。模型配置新增多款Qwen3-Next的MoE硬件适配方案。需注意本次更新涉及部分后端默认参数调整（如VLLM_ALLREDUCE_USE_SYMM_MEM设为False），建议测试环境充分验证后部署。

## Highlights

* Add the support for the qwen3 next model (a hybrid attention model). ([#24526](https://github.com/vllm-project/vllm/pull/24526)) by @sighingnow
* [Model] New model support for Motif-1-Tiny ([#23414](https://github.com/vllm-project/vllm/pull/23414)) by @ca1207
* [v1] Add Whisper model support (encoder-decoder) ([#21088](https://github.com/vllm-project/vllm/pull/21088)) by @russellb
* Support for NemotronH Nano VLM ([#23644](https://github.com/vllm-project/vllm/pull/23644)) by @danielafrimi
* Kimi K2 Fused MoE kernels Optimization configs ([#24597](https://github.com/vllm-project/vllm/pull/24597)) by @samanamp

## Features & Enhancements

* Add FLASHINFER_MLA to backend selector test ([#24753](https://github.com/vllm-project/vllm/pull/24753)) by @MatthewBonanni
* Add @heheda12345 to CODEOWNERS of KVCacheManager related code ([#24546](https://github.com/vllm-project/vllm/pull/24546)) by @heheda12345
* Add the support for the qwen3 next model (a hybrid attention model). ([#24526](https://github.com/vllm-project/vllm/pull/24526)) by @sighingnow
* Add @chaunceyjiang to codeowner for reasoning Reasoning and Tool parser ([#24406](https://github.com/vllm-project/vllm/pull/24406)) by @chaunceyjiang
* Add @luccafong to codeowner for spec decode ([#24397](https://github.com/vllm-project/vllm/pull/24397)) by @luccafong
* Add @benchislett to codeowner for spec decode and structured outputs ([#24362](https://github.com/vllm-project/vllm/pull/24362)) by @benchislett
* Add renderer-based prompt processing for embedding and classification endpoints ([#24356](https://github.com/vllm-project/vllm/pull/24356)) by @sfeng33
* Add @22quinn as code reviewer for RL related components ([#24346](https://github.com/vllm-project/vllm/pull/24346)) by @22quinn
* Add data_parallel_size to VllmConfig string representation ([#24298](https://github.com/vllm-project/vllm/pull/24298)) by @Prowindy
* Support for NemotronH Nano VLM ([#23644](https://github.com/vllm-project/vllm/pull/23644)) by @danielafrimi

## Bug Fixes

* [Compilation Bug] Fix Inductor Graph Output with Shape Issue ([#24772](https://github.com/vllm-project/vllm/pull/24772)) by @yewentao256
* [Bugfix] Set `VLLM_ALLREDUCE_USE_SYMM_MEM` default to False ([#24696](https://github.com/vllm-project/vllm/pull/24696)) by @yewentao256
* Fix implementation divergence for BLOOM models between vLLM and HuggingFace when using prompt embeds ([#24686](https://github.com/vllm-project/vllm/pull/24686)) by @qthequartermasterman
* [BugFix] Fix tokenize asyncio task leak ([#24677](https://github.com/vllm-project/vllm/pull/24677)) by @njhill
* [Bug] Fix Layer `weight_block_size` Assertion Issue ([#24674](https://github.com/vllm-project/vllm/pull/24674)) by @yewentao256
* Fix model name included in responses ([#24663](https://github.com/vllm-project/vllm/pull/24663)) by @hmellor
* Fix typing for `safetensors_load_strategy` ([#24641](https://github.com/vllm-project/vllm/pull/24641)) by @hmellor
* [BugFix] Fix pipeline parallel ([#24621](https://github.com/vllm-project/vllm/pull/24621)) by @njhill
* fix some typos ([#24616](https://github.com/vllm-project/vllm/pull/24616)) by @co63oc
* [BugFix] Fix async core engine client finalizer ([#24540](https://github.com/vllm-project/vllm/pull/24540)) by @njhill
* [BugFix] Ensure integrity of reused CPU tensors during async scheduling ([#24527](https://github.com/vllm-project/vllm/pull/24527)) by @njhill
* Enable conversion of multimodal models to pooling tasks ([#24451](https://github.com/vllm-project/vllm/pull/24451)) by @maxdebayser
* Fix Auto_Round Quatization Loading on SM75 and Lower GPUs ([#24217](https://github.com/vllm-project/vllm/pull/24217)) by @RoadToNowhereX
* fix some typos ([#24167](https://github.com/vllm-project/vllm/pull/24167)) by @co63oc

## Performance

* [Bench] Add qwen-next in benchmark_moe.py ([#24661](https://github.com/vllm-project/vllm/pull/24661)) by @jeejeelee
* Kimi K2 Fused MoE kernels Optimization configs ([#24597](https://github.com/vllm-project/vllm/pull/24597)) by @samanamp
* [Perf] Convert np array to torch tensor to index into block table for attn chunking ([#24474](https://github.com/vllm-project/vllm/pull/24474)) by @sarckk
* [Benchmark] Add option to skip oversampling in benchmark ([#24457](https://github.com/vllm-project/vllm/pull/24457)) by @ekagra-ranjan
* [Benchmark] Update bench doc with mtbench, blazedit, spec bench ([#24450](https://github.com/vllm-project/vllm/pull/24450)) by @ekagra-ranjan
* [Performance][MM] Building the inverse permutation in O(n) time in Qwen2_5_VisionTransformer ([#24443](https://github.com/vllm-project/vllm/pull/24443)) by @david6666666
* [Misc] Support bench serve long context ([#24373](https://github.com/vllm-project/vllm/pull/24373)) by @minosfuture
* QWEN3 Thinking Fused MoE kernels Optimization configs ([#24330](https://github.com/vllm-project/vllm/pull/24330)) by @samanamp
* [Spec Decode] Fix offline spec_decode.py ([#24257](https://github.com/vllm-project/vllm/pull/24257)) by @ekagra-ranjan
* [CI] Speed up model unit tests in CI ([#24253](https://github.com/vllm-project/vllm/pull/24253)) by @afeldman-nm
* [VLM] Optimize GLM4.5-V-style video processing to only decode necessary frames ([#24161](https://github.com/vllm-project/vllm/pull/24161)) by @Isotr0py
* [gpt-oss] Cache permute indices for faster MXFP4 MoE layer loading ([#24154](https://github.com/vllm-project/vllm/pull/24154)) by @frank-wei
* [Core] Run garbage collector after CUDA graph capture to fix throughput regression ([#24128](https://github.com/vllm-project/vllm/pull/24128)) by @micah-wil
* update spec decode metrics to use throughput ([#24127](https://github.com/vllm-project/vllm/pull/24127)) by @qandrew
* [Compilation][WideEP] Enable Piecewise CUDAGraph for DeepEPHT ([#24123](https://github.com/vllm-project/vllm/pull/24123)) by @yewentao256
* [Kernels] Enable Torch Symmetric Memory All-Reduce By Default ([#24111](https://github.com/vllm-project/vllm/pull/24111)) by @ilmarkov
* [Bugfix] Fix Qwen3-coder moe tuned config ([#24072](https://github.com/vllm-project/vllm/pull/24072)) by @jeejeelee
* Feature/vit attention unification# 23880 ([#23978](https://github.com/vllm-project/vllm/pull/23978)) by @baonudesifeizhai
* [Benchmark] Allow arbitrary headers to be passed to benchmarked endpoints ([#23937](https://github.com/vllm-project/vllm/pull/23937)) by @smarterclayton
* [Benchmark] add benchmark for custom activation op ([#23908](https://github.com/vllm-project/vllm/pull/23908)) by @ZJY0516
*  Adding int4 and int8 models for CPU benchmarking ([#23709](https://github.com/vllm-project/vllm/pull/23709)) by @louie-tsai
* [Kernel][B200] `mxfp4` fused cutlass moe ([#23696](https://github.com/vllm-project/vllm/pull/23696)) by @djmmoss
* [Core] Use sha256 bytes instead of BlockHash to reduce GC overhead ([#23673](https://github.com/vllm-project/vllm/pull/23673)) by @linzebing
* [Flashinfer] Support Flashinfer TRTLLM FP8-qkv BF16/FP16-out Attention Kernel ([#23647](https://github.com/vllm-project/vllm/pull/23647)) by @elvischenv
* [Spec Decode][Benchmark] Add Blitzedit dataset ([#23605](https://github.com/vllm-project/vllm/pull/23605)) by @ekagra-ranjan
* [Perf][V1] Fully overlap model execution ([#23569](https://github.com/vllm-project/vllm/pull/23569)) by @benchislett
* [Spec Decode][Benchmark] Add Spec Bench Dataset for benchmarking ([#23563](https://github.com/vllm-project/vllm/pull/23563)) by @ekagra-ranjan
* Migrate Qwen2 inputs to TensorSchema ([#23475](https://github.com/vllm-project/vllm/pull/23475)) by @bbeckca
* [Model] New model support for Motif-1-Tiny ([#23414](https://github.com/vllm-project/vllm/pull/23414)) by @ca1207
* Allow users to specify kv cache memory size ([#21489](https://github.com/vllm-project/vllm/pull/21489)) by @BoyuanFeng
* [CI/Build] Add bc-linter to vLLM CI ([#21234](https://github.com/vllm-project/vllm/pull/21234)) by @zhewenl

## Model Support

* [Models] Optimise and simplify `_validate_and_reshape_mm_tensor` ([#24742](https://github.com/vllm-project/vllm/pull/24742)) by @lgeiger
* [Qwen3-Next] MoE configs for H100 TP=1,2 and TP2/EP ([#24739](https://github.com/vllm-project/vllm/pull/24739)) by @elvircrn
* [Model] Switch to Fused RMSNorm in GLM-4.1V model ([#24733](https://github.com/vllm-project/vllm/pull/24733)) by @SamitHuang
* [Misc][gpt-oss] Add gpt-oss label to PRs that mention harmony or related to builtin tool call ([#24717](https://github.com/vllm-project/vllm/pull/24717)) by @heheda12345
* [BugFix] Fix Qwen3-Next PP ([#24709](https://github.com/vllm-project/vllm/pull/24709)) by @njhill
* [Qwen3-Next] MoE configs for H20 TP=1,2,4,8 ([#24707](https://github.com/vllm-project/vllm/pull/24707)) by @jeejeelee
* [Qwen3-Next] MOE configs for H100 TP4 ([#24699](https://github.com/vllm-project/vllm/pull/24699)) by @heheda12345
* [Qwen3-Next] Add B200 MoE configs for Qwen3-next ([#24698](https://github.com/vllm-project/vllm/pull/24698)) by @vadiklyutiy
* [Qwen3-Next] MoE configs for H200 TP=1,2,4 ([#24695](https://github.com/vllm-project/vllm/pull/24695)) by @WoosukKwon
* [Qwen3-Next] Add MoE Config for H200 ([#24688](https://github.com/vllm-project/vllm/pull/24688)) by @WoosukKwon
* [Ultravox] Use wrapped_model_config to instantiate inner model ([#24679](https://github.com/vllm-project/vllm/pull/24679)) by @petersalas
* [Qwen3Next] Fixes the cuda graph capture conditions under large batch sizes (#24660) ([#24667](https://github.com/vllm-project/vllm/pull/24667)) by @sighingnow
* [Docs] Add transcription support to model ([#24664](https://github.com/vllm-project/vllm/pull/24664)) by @NickLucche
* [Bugifx] Fix qwen-next packed_modules_mapping ([#24656](https://github.com/vllm-project/vllm/pull/24656)) by @jeejeelee
* [Docs] Fixes a typo in the qwen3next model name. ([#24654](https://github.com/vllm-project/vllm/pull/24654)) by @sighingnow
* [CI] Fix flaky test  v1/worker/test_gpu_model_runner.py::test_kv_cache_stride_order          ([#24640](https://github.com/vllm-project/vllm/pull/24640)) by @heheda12345
* [CI Failure] fix models/language/pooling/test_auto_prefix_cache_support.py ([#24636](https://github.com/vllm-project/vllm/pull/24636)) by @noooop
* [CI] Split mteb test from Language Models Test ([#24634](https://github.com/vllm-project/vllm/pull/24634)) by @noooop
* [Bug] [Spec Decode] Fix model_initialization test and mismatch in aux_hidden_layers ([#24613](https://github.com/vllm-project/vllm/pull/24613)) by @wwl2755
* [Doc] Add documentation for GLM-4.5 series models: tool-calling and reasoning parser ([#24589](https://github.com/vllm-project/vllm/pull/24589)) by @WangErXiao
* [BugFix][easy] Fix flaky test test_gpt_oss_multi_turn_chat ([#24549](https://github.com/vllm-project/vllm/pull/24549)) by @lacora
* [Model] Limit CPU threads for image transformations in InternVL to reduce cpu contention. ([#24519](https://github.com/vllm-project/vllm/pull/24519)) by @li-jinpeng
* [LoRA]: Add LoRA support to Mistral's Voxtral models ([#24517](https://github.com/vllm-project/vllm/pull/24517)) by @pratapyash
* [Bugfix] Fix  hidden_size for multimodal classification model ([#24501](https://github.com/vllm-project/vllm/pull/24501)) by @jeejeelee
* [CI] Add PPL test for generation models ([#24485](https://github.com/vllm-project/vllm/pull/24485)) by @noooop
* [gpt-oss] raise error for flashinfer backend without trtllm ([#24482](https://github.com/vllm-project/vllm/pull/24482)) by @heheda12345
* Update reviewers for modelopt related files ([#24468](https://github.com/vllm-project/vllm/pull/24468)) by @Edwardf0t1
* [CI] Enable encoder model compilation test ([#24442](https://github.com/vllm-project/vllm/pull/24442)) by @ZJY0516
* [Doc] Fix issues in integrations/llamastack.md ([#24428](https://github.com/vllm-project/vllm/pull/24428)) by @windsonsea
* [Bugfix] Fix get_quant_config when using modelscope ([#24421](https://github.com/vllm-project/vllm/pull/24421)) by @Potabk
* [Model] Enable BNB support for qwen2_5_omni_thinker ([#24420](https://github.com/vllm-project/vllm/pull/24420)) by @jeejeelee
* [gpt-oss][Responses API] Fix the function call id format ([#24409](https://github.com/vllm-project/vllm/pull/24409)) by @chaunceyjiang
* [BugFix][Spec Decode] Fix out-of-range index triggered by eagle3; re-enable test for LlamaForCausalLMEagle3 ([#24392](https://github.com/vllm-project/vllm/pull/24392)) by @wwl2755
* [Bugfix] Fix test_mixtral_moe ([#24371](https://github.com/vllm-project/vllm/pull/24371)) by @jeejeelee
* [VLM] Migrate remain DP-supported ViT models to use `disable_tp` ([#24363](https://github.com/vllm-project/vllm/pull/24363)) by @Isotr0py
* [Bugfix] Guard `_may_reorder_batch` for encoder-only models on CPU (#24319) ([#24348](https://github.com/vllm-project/vllm/pull/24348)) by @comsky
* refactor: Turn GPUModelRunner.inputs_embeds to a CpuGpuBuffer ([#24345](https://github.com/vllm-project/vllm/pull/24345)) by @qthequartermasterman
* [Fix] [gpt-oss] fix non-tool calling path for chat completion ([#24324](https://github.com/vllm-project/vllm/pull/24324)) by @aarnphm
* [New Model]: google/embeddinggemma-300m ([#24318](https://github.com/vllm-project/vllm/pull/24318)) by @noooop
* [gpt-oss][Bugfix]Fix streamableparser for missing handling of certain token_ids ([#24306](https://github.com/vllm-project/vllm/pull/24306)) by @chaunceyjiang
* break execute_model in gpu_model_runner into sub-functions for custom scopes ([#24265](https://github.com/vllm-project/vllm/pull/24265)) by @bangshengtang
* [Models][Quantization] Add quantization configuration update in Voxtral model ([#24122](https://github.com/vllm-project/vllm/pull/24122)) by @anmarques
* [BugFix][Model] Fix Ernie4.5-VL hanging on long inputs ([#24074](https://github.com/vllm-project/vllm/pull/24074)) by @CSWYF3634076
* [Model loader]: support multi-thread model weight loading ([#23928](https://github.com/vllm-project/vllm/pull/23928)) by @BraveY
* [gpt-oss] Validate gpt-oss python tool during initialization ([#23856](https://github.com/vllm-project/vllm/pull/23856)) by @heheda12345
* [Bugfix] Update Run:AI Model Streamer Loading Integration ([#23845](https://github.com/vllm-project/vllm/pull/23845)) by @pwschuurman
* [Model] Systematic support for fp32 head, pooling models part ([#23810](https://github.com/vllm-project/vllm/pull/23810)) by @noooop
* [Bugfix] Fix mamba2 prefill chunking ([#23279](https://github.com/vllm-project/vllm/pull/23279)) by @tomeras91
* [gpt-oss] tool parser supports for /chat/completions [1/n] ([#22386](https://github.com/vllm-project/vllm/pull/22386)) by @aarnphm

## Hardware & Backend

* [Bugfix] Fix MRoPE dispatch on XPU ([#24724](https://github.com/vllm-project/vllm/pull/24724)) by @yma11
* [Bugfix] Fix MRoPE dispatch on CPU ([#24712](https://github.com/vllm-project/vllm/pull/24712)) by @bigPYJ1151
* [Attention][FlashInfer] Enable FP8 FlashInfer (TRTLLM) MLA decode ([#24705](https://github.com/vllm-project/vllm/pull/24705)) by @MatthewBonanni
* [Kernel] [CPU] refactor `cpu_attn.py:_run_sdpa_forward` for better memory access ([#24701](https://github.com/vllm-project/vllm/pull/24701)) by @ignaciosica
* [Bugfix] fixes the causal_conv1d_update kernel update non-speculative decoding cases ([#24680](https://github.com/vllm-project/vllm/pull/24680)) by @sighingnow
* [HybridKVCache][Platform] Add support_hybrid_kv_cache for platform ([#24646](https://github.com/vllm-project/vllm/pull/24646)) by @MengqingCao
* [XPU] add missing dependency tblib for XPU CI ([#24639](https://github.com/vllm-project/vllm/pull/24639)) by @faaany
* [Bugfix][Frontend] Fix `--enable-log-outputs` does not match the documentation ([#24626](https://github.com/vllm-project/vllm/pull/24626)) by @kebe7jun
* [Bugfix] Add missing VIT backend dispatch on CPU ([#24623](https://github.com/vllm-project/vllm/pull/24623)) by @bigPYJ1151
* [Kernels] Add Flash Linear Attention Kernels ([#24518](https://github.com/vllm-project/vllm/pull/24518)) by @youkaichao
* [Doc]: fixing typos to improve docs ([#24480](https://github.com/vllm-project/vllm/pull/24480)) by @didier-durand
* [TPU] Fix tpu structured decoding in mixed batches ([#24458](https://github.com/vllm-project/vllm/pull/24458)) by @Chenyaaang
* [CI/Build] Disable flaky test_structured_output tests ([#24404](https://github.com/vllm-project/vllm/pull/24404)) by @22quinn
* [rocm] enable torchao quantization for rocm ([#24400](https://github.com/vllm-project/vllm/pull/24400)) by @draftbk
* [Kernel] Support decode context parallelism on Blackwell with CUTLASS MLA ([#24385](https://github.com/vllm-project/vllm/pull/24385)) by @minosfuture
* [CI][Fix] deterministic seed for flaky CI runs on structured outputs ([#24380](https://github.com/vllm-project/vllm/pull/24380)) by @aarnphm
* [CI] Disable flaky structured output test from CI ([#24366](https://github.com/vllm-project/vllm/pull/24366)) by @ywang96
* [Multi Modal] Add FA3 in VIT ([#24347](https://github.com/vllm-project/vllm/pull/24347)) by @wwl2755
* [Frontend][Responses API] Support reporting tool output tokens and fix reasoning token count ([#24285](https://github.com/vllm-project/vllm/pull/24285)) by @yeqcharlotte
* [ROCm][CI/Build] Sync ROCm dockerfiles with the ROCm fork ([#24279](https://github.com/vllm-project/vllm/pull/24279)) by @gshtras
* [ROCm][Feature] Enable Pipeline Parallelism with Ray Compiled Graph on ROCm ([#24275](https://github.com/vllm-project/vllm/pull/24275)) by @charlifu
* [flashinfer] [kernel] support for fp8 kv cache for trtllm prefill attention ([#24197](https://github.com/vllm-project/vllm/pull/24197)) by @mxz297
* [Hardware][Apple-CPU] Enable native bfloat16 on Apple Silicon (M2 and later) ([#24129](https://github.com/vllm-project/vllm/pull/24129)) by @ignaciosica
* [Docs] Fix warnings in `mkdocs build` (continued) ([#24092](https://github.com/vllm-project/vllm/pull/24092)) by @Zerohertz
* [Attention] FlashAttention MLA cudagraph support ([#23958](https://github.com/vllm-project/vllm/pull/23958)) by @MatthewBonanni
* [xpu] upgrade ipex/python3.12 for xpu ([#23830](https://github.com/vllm-project/vllm/pull/23830)) by @yma11
* [Feature] Support Decode Context Parallel (DCP) for MLA ([#23734](https://github.com/vllm-project/vllm/pull/23734)) by @youzhedian
* [ROCm][Bugfix] Fix Aiter RMSNorm  ([#23412](https://github.com/vllm-project/vllm/pull/23412)) by @vllmellm
* [P/D] Add a shutdown method to the Connector API ([#22699](https://github.com/vllm-project/vllm/pull/22699)) by @chaunceyjiang
* [XPU][P/D] Add XPU support in NixlConnector ([#22436](https://github.com/vllm-project/vllm/pull/22436)) by @zhenwei-intel
* [Kernel] Flashinfer MLA (trtllm-gen) decode kernel integration ([#21078](https://github.com/vllm-project/vllm/pull/21078)) by @hjjq
* [torch.compile][ROCm][V1] Enable attention output FP8 fusion for V1 attention backends ([#19767](https://github.com/vllm-project/vllm/pull/19767)) by @gshtras
* [Doc] Clarify cudagraph capture size logic and default behavior in scheduler ([#18698](https://github.com/vllm-project/vllm/pull/18698)) by @Zazzle516

## Refactoring & Core

* [Docs] Improve organisation of API Reference nav ([#24569](https://github.com/vllm-project/vllm/pull/24569)) by @hmellor
* [Core] Simplify and unify mm uuid handling & auto-generated mm hash overrides processing.  ([#24271](https://github.com/vllm-project/vllm/pull/24271)) by @huachenheli

## Build, CI & Testing

* [Docs] Fix warnings in mkdocs build (continued) ([#24740](https://github.com/vllm-project/vllm/pull/24740)) by @Zerohertz
* [CI] Split pooling from entrypoints Test ([#24632](https://github.com/vllm-project/vllm/pull/24632)) by @noooop
* [CI] Add ci_envs for convenient local testing ([#24630](https://github.com/vllm-project/vllm/pull/24630)) by @noooop
* [CI] Fix tensorizer test assertion ([#24545](https://github.com/vllm-project/vllm/pull/24545)) by @pwschuurman
* [CI] Retry flaky fp8 cutlass mla tests ([#24536](https://github.com/vllm-project/vllm/pull/24536)) by @njhill
* [CI] Adjust threshold for flaky ngram spec decoding test ([#24528](https://github.com/vllm-project/vllm/pull/24528)) by @njhill
* [CI] execute all piecewise compilation tests together ([#24502](https://github.com/vllm-project/vllm/pull/24502)) by @ZJY0516
* [Bugfix] Fix platform-specific routing in CustomOp implementations ([#24444](https://github.com/vllm-project/vllm/pull/24444)) by @kzawora-intel
* [CI/Build] Fix local image inputs in test_pixtral.py ([#24401](https://github.com/vllm-project/vllm/pull/24401)) by @huachenheli
* [Bugfix] Fix unstable silu_mul+nvfp4 quant fusion test ([#24370](https://github.com/vllm-project/vllm/pull/24370)) by @elvischenv
* [Bugfix] Fix silu_mul+quant fusion test ([#24341](https://github.com/vllm-project/vllm/pull/24341)) by @elvischenv
* [build] add torch to tool.uv no-build-isolation-package ([#24303](https://github.com/vllm-project/vllm/pull/24303)) by @youkaichao
* [CI] Add timeouts to tests ([#24260](https://github.com/vllm-project/vllm/pull/24260)) by @rafvasq
* [CI/Build] bump timm dependency ([#24189](https://github.com/vllm-project/vllm/pull/24189)) by @dtrifiro
* [CI] Add nightly multiarch manifests to dockerhub ([#24102](https://github.com/vllm-project/vllm/pull/24102)) by @csahithi
* [CI] Fail subprocess tests with root-cause error ([#23795](https://github.com/vllm-project/vllm/pull/23795)) by @njhill
* [V1] feat:add engine v1 tracing ([#20372](https://github.com/vllm-project/vllm/pull/20372)) by @RichardoMrMu

## Documentation

* [Docs] Fix formatting of transcription doc ([#24676](https://github.com/vllm-project/vllm/pull/24676)) by @hmellor
* [Doc] Fix Markdown Pre-commit Error ([#24670](https://github.com/vllm-project/vllm/pull/24670)) by @yewentao256
* [Docs] Fix typos in EP deployment doc ([#24669](https://github.com/vllm-project/vllm/pull/24669)) by @hmellor
* [Doc]: fixing doc typos ([#24635](https://github.com/vllm-project/vllm/pull/24635)) by @didier-durand
* [Docs] Use 1-2-3 list for deploy steps in deployment/frameworks/ ([#24633](https://github.com/vllm-project/vllm/pull/24633)) by @windsonsea
* [distributed] update known issues ([#24624](https://github.com/vllm-project/vllm/pull/24624)) by @youkaichao
* [Docs] Update V1 doc to reflect whisper support ([#24606](https://github.com/vllm-project/vllm/pull/24606)) by @russellb
* [docs] promo pytorch conf and ray summit ([#24562](https://github.com/vllm-project/vllm/pull/24562)) by @simon-mo
* [Docs] Document the extra memory footprint overhead when using EPLB ([#24537](https://github.com/vllm-project/vllm/pull/24537)) by @tlrmchlsmth
* [Docs] Gemma3n `transcriptions` endpoint support ([#24512](https://github.com/vllm-project/vllm/pull/24512)) by @NickLucche
* [Docs] Revise frameworks/anything-llm.md ([#24489](https://github.com/vllm-project/vllm/pull/24489)) by @windsonsea
* [Doc] mention fpdb for multiprocess breakpoints ([#24452](https://github.com/vllm-project/vllm/pull/24452)) by @mickaelseznec
* [Bugfix] Fix Apertus HF repo name ([#24447](https://github.com/vllm-project/vllm/pull/24447)) by @DarkLight1337
* [Doc]: fix 2 hyperlinks leading to Ray site after they changed Ray's doc structure ([#24438](https://github.com/vllm-project/vllm/pull/24438)) by @didier-durand
* [Docs] Move feature compatibility tables to README ([#24431](https://github.com/vllm-project/vllm/pull/24431)) by @hmellor
* [Docs] Fix a tip indentation and typo ([#24419](https://github.com/vllm-project/vllm/pull/24419)) by @windsonsea
* [Doc] Fix UTF-8 encoding issues in documentation generation on Windows ([#24361](https://github.com/vllm-project/vllm/pull/24361)) by @alhridoy
* [Misc] Terratorch related fixes ([#24337](https://github.com/vllm-project/vllm/pull/24337)) by @christian-pinto
* [docs] add shenzhen meetup ([#24326](https://github.com/vllm-project/vllm/pull/24326)) by @youkaichao
* [RL] fast weight update with zmq + ipc handles ([#24295](https://github.com/vllm-project/vllm/pull/24295)) by @weixiao-huang
* [Doc]: fix typos in Python comments ([#24294](https://github.com/vllm-project/vllm/pull/24294)) by @didier-durand
* [Docs]add eplb_config param use docs ([#24213](https://github.com/vllm-project/vllm/pull/24213)) by @lengrongfu
* [Docs] Enable relative links in examples to function when rendered in the docs ([#24041](https://github.com/vllm-project/vllm/pull/24041)) by @hmellor
* [Sampler] Support returning all prompt logprobs ([#23868](https://github.com/vllm-project/vllm/pull/23868)) by @charlotte12l
* [Frontend] User-provided uuids for medias in chat. (RFC #22044) ([#23449](https://github.com/vllm-project/vllm/pull/23449)) by @huachenheli
* [Core] Shared memory based object store for Multimodal data caching and IPC ([#20452](https://github.com/vllm-project/vllm/pull/20452)) by @dongluw

## Miscellaneous

* Invert pattern order to make sure that out_proj layers are identified ([#24781](https://github.com/vllm-project/vllm/pull/24781)) by @anmarques
* [Bugfix] Fix BNB name match ([#24735](https://github.com/vllm-project/vllm/pull/24735)) by @jeejeelee
* [sleep mode] save memory for on-the-fly quantization ([#24731](https://github.com/vllm-project/vllm/pull/24731)) by @youkaichao
* [Startup] Make DeepGEMM warmup scale with max-num-batched-tokens ([#24693](https://github.com/vllm-project/vllm/pull/24693)) by @LucasWilkinson
* [Bugfix][Attention] Fix FlashInfer MLA block size logic ([#24692](https://github.com/vllm-project/vllm/pull/24692)) by @MatthewBonanni
* [Misc] Add @NickLucche to codeowners ([#24647](https://github.com/vllm-project/vllm/pull/24647)) by @NickLucche
* Move `LoRAConfig` from `config/__init__.py` to `config/lora.py` ([#24644](https://github.com/vllm-project/vllm/pull/24644)) by @hmellor
* [Bugfix] Enable FP8 KV cache for FlashInfer and Triton backend on non-sm100 GPUs ([#24577](https://github.com/vllm-project/vllm/pull/24577)) by @gau-nernst
* [Core] Split LoRA layers ([#24574](https://github.com/vllm-project/vllm/pull/24574)) by @jeejeelee
* Move `LoadConfig` from `config/__init__.py` to `config/load.py` ([#24566](https://github.com/vllm-project/vllm/pull/24566)) by @hmellor
* [Bugfix] Fix _synced_weight_loader ([#24565](https://github.com/vllm-project/vllm/pull/24565)) by @kyuyeunk
* [BugFix][Multi Modal] Fix TensorSchema shape mismatch in Molmo ([#24559](https://github.com/vllm-project/vllm/pull/24559)) by @wwl2755
* Consolidate rendering parameters into RenderConfig dataclass ([#24543](https://github.com/vllm-project/vllm/pull/24543)) by @sfeng33
* [Bugfix] Fix for 24530. Fix naive all2all shared expert overlap. ([#24538](https://github.com/vllm-project/vllm/pull/24538)) by @bnellnm
* [Bugfix] Improve EPLB config validation error message ([#24524](https://github.com/vllm-project/vllm/pull/24524)) by @tlrmchlsmth
* [Misc] Make timeout passable in init_distributed_environment ([#24522](https://github.com/vllm-project/vllm/pull/24522)) by @jberkhahn
* [Feature] Disallow FlashMLA on Blackwell ([#24521](https://github.com/vllm-project/vllm/pull/24521)) by @yewentao256
* [Misc] Add Codex settings to gitignore ([#24493](https://github.com/vllm-project/vllm/pull/24493)) by @ywang96
* [Misc] Add claude settings to gitignore ([#24492](https://github.com/vllm-project/vllm/pull/24492)) by @yeqcharlotte
* [Core] feat: Add --safetensors-load-strategy flag for faster safetensors loading from Lustre ([#24469](https://github.com/vllm-project/vllm/pull/24469)) by @shengshiqi-google
* [Attention] add DCP support for FLASH_ATTN_MLA backend ([#24453](https://github.com/vllm-project/vllm/pull/24453)) by @LucasWilkinson
* Move `KVTransferConfig` from `config/__init__.py` to `config/kv_transfer.py` ([#24434](https://github.com/vllm-project/vllm/pull/24434)) by @hmellor
* Move `KVEventsConfig` from `config/__init__.py` to `config/kv_events.py` ([#24433](https://github.com/vllm-project/vllm/pull/24433)) by @hmellor
* [P/D] MultiConnector supports shutdown ([#24425](https://github.com/vllm-project/vllm/pull/24425)) by @chaunceyjiang
* Extend renderer with embedding support and integrate completion endpoint ([#24405](https://github.com/vllm-project/vllm/pull/24405)) by @sfeng33
* Skip MM Encoder for non-first PP ranks ([#24387](https://github.com/vllm-project/vllm/pull/24387)) by @WoosukKwon
* [Misc] collect flashinfer version in collect_env.py ([#24378](https://github.com/vllm-project/vllm/pull/24378)) by @yeqcharlotte
* [attention][DCP] use AttentionImpl.need_to_return_lse_for_decode ([#24372](https://github.com/vllm-project/vllm/pull/24372)) by @youkaichao
* [Bugfix] Fix broken deepseek fp8 TP weights loading ([#24367](https://github.com/vllm-project/vllm/pull/24367)) by @Isotr0py
* [Bugfix] Catch and log invalid token ids in detokenizer ([#24351](https://github.com/vllm-project/vllm/pull/24351)) by @njhill
* [KV Sharing] Raise error if using eagle with fast prefill ([#24350](https://github.com/vllm-project/vllm/pull/24350)) by @sarckk
* [Logging] allow config logging stream ([#24336](https://github.com/vllm-project/vllm/pull/24336)) by @842974287
* [Bugfix] Avoid uninitialized usage of azp_val when AZP is false. ([#24335](https://github.com/vllm-project/vllm/pull/24335)) by @mohankku
* [Multimodal] Improve max video embedding length estimation in V1 ([#24312](https://github.com/vllm-project/vllm/pull/24312)) by @ywang96
* [Core] Support configuration parsing plugin ([#24277](https://github.com/vllm-project/vllm/pull/24277)) by @charlotte12l
* [Misc] update log level debug to warning when process port is used by ([#24226](https://github.com/vllm-project/vllm/pull/24226)) by @lengrongfu
* [Core] Support async scheduling with uniproc executor  ([#24219](https://github.com/vllm-project/vllm/pull/24219)) by @njhill
* [Bugfix][Wide EP] Fix redundant work when using DeepEP, TP Attn, and EP MoE ([#24134](https://github.com/vllm-project/vllm/pull/24134)) by @tlrmchlsmth
* [Ultravox] Fix Gemma instantiation, support quantization via --hf-overrides ([#24131](https://github.com/vllm-project/vllm/pull/24131)) by @petersalas
* [BugFix] `python collect_env.py` and `vllm collect-env` compatibility with uv venv ([#24066](https://github.com/vllm-project/vllm/pull/24066)) by @yankay
* [Hardware][IBM Z] Fix Outlines Core issue for s390x ([#24034](https://github.com/vllm-project/vllm/pull/24034)) by @R3hankhan123
* [Bugfix] Handle the edge case in detokenizer where processed tokens contain both `stop` str and `eos` token ([#23938](https://github.com/vllm-project/vllm/pull/23938)) by @dtransposed
* [Log] Use a relative path in debug-level logs to distinguish files with identical names ([#23846](https://github.com/vllm-project/vllm/pull/23846)) by @ZJY0516
* [KV Connector] More async support for `get_num_new_matched_tokens` ([#23620](https://github.com/vllm-project/vllm/pull/23620)) by @ApostaC
* [Bugfix] Fix DeepEP config for DP4TP4 ([#23619](https://github.com/vllm-project/vllm/pull/23619)) by @minosfuture
* [Platform] Custom ops support for LMhead and LogitsProcessor ([#23564](https://github.com/vllm-project/vllm/pull/23564)) by @zzhx1
* [RFC] allow cancelation after shutdown in blocking collective_rpc ([#23390](https://github.com/vllm-project/vllm/pull/23390)) by @842974287
* [Core] Allow disabling TP sharding for parallel Linear layer ([#23024](https://github.com/vllm-project/vllm/pull/23024)) by @Isotr0py
* [Misc] Improve Worker process title and logging prefix ([#22205](https://github.com/vllm-project/vllm/pull/22205)) by @22quinn
* [torchao] Support quantization configs using module swap ([#21982](https://github.com/vllm-project/vllm/pull/21982)) by @jerryzh168

## Breaking Changes

* [CI] Trigger BC Linter when labels are added/removed ([#24767](https://github.com/vllm-project/vllm/pull/24767)) by @zhewenl
* [Bugfix] Fix incompatibility between #20452 and #24548 ([#24754](https://github.com/vllm-project/vllm/pull/24754)) by @DarkLight1337
* [Models] Prevent CUDA sync in Qwen2.5-VL ([#24741](https://github.com/vllm-project/vllm/pull/24741)) by @lgeiger
* [Doc]: fix typos in various files ([#24726](https://github.com/vllm-project/vllm/pull/24726)) by @didier-durand
* [CI/Build] Skip prompt embeddings tests on V1-only CPU backend ([#24721](https://github.com/vllm-project/vllm/pull/24721)) by @bigPYJ1151
* [DOCs] Update ROCm installation docs section ([#24691](https://github.com/vllm-project/vllm/pull/24691)) by @gshtras
* [Doc] Remove Useless Comments ([#24687](https://github.com/vllm-project/vllm/pull/24687)) by @yewentao256
* [Bugfix] Fix incorrect import of CacheConfig ([#24631](https://github.com/vllm-project/vllm/pull/24631)) by @DarkLight1337
* [CI]Add transformers_utils to Async Engine, Inputs, Utils, Worker Test ([#24615](https://github.com/vllm-project/vllm/pull/24615)) by @charlotte12l
* [UX] Remove AsyncLLM torch profiler disabled log ([#24609](https://github.com/vllm-project/vllm/pull/24609)) by @mgoin
* Enable --profile in 'vllm bench throughput' ([#24575](https://github.com/vllm-project/vllm/pull/24575)) by @tomasruizt
* [Engine][Chore] use local variable and remove output var assignment ([#24554](https://github.com/vllm-project/vllm/pull/24554)) by @GuyStone
* [Multimodal] Remove legacy multimodal fields in favor of MultiModalFeatureSpec  ([#24548](https://github.com/vllm-project/vllm/pull/24548)) by @sfeng33
* [Model] Remove quantized mixtral ([#24437](https://github.com/vllm-project/vllm/pull/24437)) by @jeejeelee
* [CI/Build] split true unit tests to Entrypoints Unit Tests ([#24418](https://github.com/vllm-project/vllm/pull/24418)) by @yeqcharlotte
* [Doc]: fix typos in Python comments ([#24417](https://github.com/vllm-project/vllm/pull/24417)) by @didier-durand
* Bump actions/setup-python from 5.4.0 to 6.0.0 ([#24414](https://github.com/vllm-project/vllm/pull/24414)) by @dependabot
* Bump actions/github-script from 7.0.1 to 8.0.0 ([#24413](https://github.com/vllm-project/vllm/pull/24413)) by @dependabot
* Bump actions/stale from 9.1.0 to 10.0.0 ([#24412](https://github.com/vllm-project/vllm/pull/24412)) by @dependabot
* [CI/Build][Doc] Fully deprecate old bench scripts for serving / throughput / latency ([#24411](https://github.com/vllm-project/vllm/pull/24411)) by @yeqcharlotte
* [TPU] Remove TopKTopPSampler dependency for TPU sampler ([#24391](https://github.com/vllm-project/vllm/pull/24391)) by @WoosukKwon
* [Misc] bump outlines_core to fix the version conflicts with outlines >= 1.2.0 ([#24368](https://github.com/vllm-project/vllm/pull/24368)) by @serihiro
* Lora bias(enable_lora_bias) deprecate warning ([#24339](https://github.com/vllm-project/vllm/pull/24339)) by @ashwin-phadke
* [Model] Remove unnecessary CUDA sync of Qwen2VL image and video preprocess ([#24334](https://github.com/vllm-project/vllm/pull/24334)) by @what-in-the-nim
* [Model] Remove unnecessary CUDA sync of GLM-4.1V image and video preprocess ([#24332](https://github.com/vllm-project/vllm/pull/24332)) by @what-in-the-nim
* [doc] update `vllm serve` cli args documentation ([#24329](https://github.com/vllm-project/vllm/pull/24329)) by @cjackal
* [Bugfix] fix modelopt exclude_modules name mapping ([#24178](https://github.com/vllm-project/vllm/pull/24178)) by @tomeras91
* Remove redundant all gather + split ([#23441](https://github.com/vllm-project/vllm/pull/23441)) by @chenxi-yang
* [Perf] Warmup FlashInfer attention during startup ([#23439](https://github.com/vllm-project/vllm/pull/23439)) by @mgoin
* [gpt-oss] Harmony changes with container tool support ([#23386](https://github.com/vllm-project/vllm/pull/23386)) by @morgendave
* [Perf] Use upstream CUTLASS for SM90 Block FP8 kernel ([#23280](https://github.com/vllm-project/vllm/pull/23280)) by @mgoin
* [Bugfix] Disable the statslogger if the api_server_count is greater than 1 ([#22227](https://github.com/vllm-project/vllm/pull/22227)) by @chaunceyjiang
* [V0 deprecation] Deprecate V0 Neuron backend ([#21159](https://github.com/vllm-project/vllm/pull/21159)) by @WoosukKwon
* [v1] Add Whisper model support (encoder-decoder) ([#21088](https://github.com/vllm-project/vllm/pull/21088)) by @russellb

## Upgrade Notes

- This PR completes the migration to `MultiModalFeatureSpec` by removing legacy fields (`mm_positions`, `mm_kwargs`, `mm_hashes`) that were temporarily kept for backward compatibility. This is a follow-up to #23779 which introduced the unified `MultiModalFeatureSpec` data structure.
- <h3>Breaking Changes</h3>
- <li>Upgrade to node 24 by <a href="https://github.com/salmanmkc"><code>@​salmanmkc</code></a> in <a href="https://redirect.github.com/actions/setup-python/pull/1164">actions/setup-python#1164</a></li>
- <li>Upgrade idna from 2.9 to 3.7 in /<strong>tests</strong>/data by <a href="https://github.com/dependabot"><code>@​dependabot</code></a>[bot] in <a href="https://redirect.github.com/actions/setup-python/pull/843">actions/setup-python#843</a></li>
- <li>Upgrade husky to v9 by <a href="https://github.com/benelan"><code>@​benelan</code></a> in <a href="https://redirect.github.com/actions/github-script/pull/482">actions/github-script#482</a></li>
- <li>Upgrade IA Publish by <a href="https://github.com/Jcambass"><code>@​Jcambass</code></a> in <a href="https://redirect.github.com/actions/github-script/pull/486">actions/github-script#486</a></li>
- - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- <li>Upgrade to node 24 by <a href="https://github.com/salmanmkc"><code>@​salmanmkc</code></a> in <a href="https://redirect.github.com/actions/stale/pull/1279">actions/stale#1279</a>

## Contributors

@22quinn, @842974287, @ApostaC, @BoyuanFeng, @BraveY, @CSWYF3634076, @Chenyaaang, @DarkLight1337, @Edwardf0t1, @GuyStone, @Isotr0py, @LucasWilkinson, @MatthewBonanni, @MengqingCao, @NickLucche, @Potabk, @Prowindy, @R3hankhan123, @RichardoMrMu, @RoadToNowhereX, @SamitHuang, @WangErXiao, @WoosukKwon, @ZJY0516, @Zazzle516, @Zerohertz, @aarnphm, @afeldman-nm, @alhridoy, @anmarques, @ashwin-phadke, @bangshengtang, @baonudesifeizhai, @bbeckca, @benchislett, @bigPYJ1151, @bnellnm, @ca1207, @charlifu, @charlotte12l, @chaunceyjiang, @chenxi-yang, @christian-pinto, @cjackal, @co63oc, @comsky, @csahithi, @danielafrimi, @david6666666, @dependabot, @didier-durand, @djmmoss, @dongluw, @draftbk, @dtransposed, @dtrifiro, @ekagra-ranjan, @elvircrn, @elvischenv, @faaany, @frank-wei, @gau-nernst, @gshtras, @heheda12345, @hjjq, @hmellor, @huachenheli, @ignaciosica, @ilmarkov, @jberkhahn, @jeejeelee, @jerryzh168, @kebe7jun, @kyuyeunk, @kzawora-intel, @lacora, @lengrongfu, @lgeiger, @li-jinpeng, @linzebing, @louie-tsai, @luccafong, @maxdebayser, @mgoin, @micah-wil, @mickaelseznec, @minosfuture, @mohankku, @morgendave, @mxz297, @njhill, @noooop, @petersalas, @pratapyash, @pwschuurman, @qandrew, @qthequartermasterman, @rafvasq, @russellb, @samanamp, @sarckk, @serihiro, @sfeng33, @shengshiqi-google, @sighingnow, @simon-mo, @smarterclayton, @tlrmchlsmth, @tomasruizt, @tomeras91, @vadiklyutiy, @vllmellm, @weixiao-huang, @what-in-the-nim, @windsonsea, @wwl2755, @yankay, @yeqcharlotte, @yewentao256, @yma11, @youkaichao, @youzhedian, @ywang96, @zhenwei-intel, @zhewenl, @zzhx1