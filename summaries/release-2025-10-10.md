# Weekly Release Report for vllm-project/vllm (2025-10-10)

This week merged 261 PRs from 127 contributors. Key areas: features 13, fixes 18, performance 29.

## Executive Summary

本周发布主要围绕代码质量改进和功能扩展展开。在代码格式化方面，彻底移除了对 `yapf` 的依赖，全面转向 `ruff` 工具。新增了对 QuTLASS 量化方案的支持，并重构了 MistralTokenizer 以提升兼容性。V0 版本弃用进程继续推进，从测试中移除了 `VLLM_USE_V1` 相关代码。功能增强包括优化 CPUFusedMOE 内核、扩展专家并行支持、完善 RLHF 文档，并新增了多种模型解析器与内核覆盖。

在错误修复方面，解决了 `mypy` 类型检查问题、CUDA 图选择错误、令牌处理异常及多模态缓存等多个关键缺陷。性能优化涵盖 Python 3.10 升级、Qwen 模型张量操作改进、FlashInfer 后端解码全图支持及量化内核加速。模型支持新增了 Lfm2Moe、ModernBertForTokenClassification 等架构，并强化了多模态模型的配置合并机制。升级时需注意 V0 功能弃用可能带来的兼容性影响，建议全面测试。

## Highlights

* Remove all references to `yapf` as it's no longer used ([#26251](https://github.com/vllm-project/vllm/pull/26251)) by @hmellor
* Convert formatting to use `ruff` instead of `yapf` + `isort` ([#26247](https://github.com/vllm-project/vllm/pull/26247)) by @hmellor
* [Transform] [Quantization] Add QuTLASS support to vLLM ([#24440](https://github.com/vllm-project/vllm/pull/24440)) by @LopezCastroRoberto
* Refactor MistralTokenizer ([#26358](https://github.com/vllm-project/vllm/pull/26358)) by @juliendenize
* [V0 Deprecation] Remove `VLLM_USE_V1` from tests ([#26341](https://github.com/vllm-project/vllm/pull/26341)) by @DarkLight1337

## Features & Enhancements

* Add more libraries to rlhf.md ([#26374](https://github.com/vllm-project/vllm/pull/26374)) by @mgoin
* Add SwigluOAI implementation for CPUFusedMOE ([#26347](https://github.com/vllm-project/vllm/pull/26347)) by @isharif168
* Add TRL example notebook to RLHF docs ([#26346](https://github.com/vllm-project/vllm/pull/26346)) by @sergiopaniego
* Add bias handling to CPUFusedMOE kernel ([#26289](https://github.com/vllm-project/vllm/pull/26289)) by @cfRod
* Support expert parallel load balancing in Transformers backend ([#26287](https://github.com/vllm-project/vllm/pull/26287)) by @hmellor
* Add documentation for granite 4 tool calling ([#26175](https://github.com/vllm-project/vllm/pull/26175)) by @maxdebayser
* Support expert parallel in Transformers backend ([#26162](https://github.com/vllm-project/vllm/pull/26162)) by @hmellor
* Add Olmo 3 reasoning parser ([#26054](https://github.com/vllm-project/vllm/pull/26054)) by @soldni
* Support llama3 eagle3 head with llama4 verifier ([#25961](https://github.com/vllm-project/vllm/pull/25961)) by @rahul-tuli
* Add topk logits torch op for DS3.2. ([#25945](https://github.com/vllm-project/vllm/pull/25945)) by @dcampora
* Add gather_indexer_k_quant_cache kernel ([#25931](https://github.com/vllm-project/vllm/pull/25931)) by @Barry-Delaney
* Add batch invariant kernel override for FlashInfer backend [2/n] ([#25769](https://github.com/vllm-project/vllm/pull/25769)) by @bwasti
* Add Qwen3-Omni moe thinker ([#25550](https://github.com/vllm-project/vllm/pull/25550)) by @wangxiongts

## Bug Fixes

* Fix some typing issues found by `mypy==1.18.2` ([#26596](https://github.com/vllm-project/vllm/pull/26596)) by @hmellor
* [Bug] Fix modular_kernel: ZeroDivisionError: integer division or modulo by zero ([#26528](https://github.com/vllm-project/vllm/pull/26528)) by @yewentao256
* fix test_simple_inductor_graph_partition ([#26522](https://github.com/vllm-project/vllm/pull/26522)) by @BoyuanFeng
* [Bugfix] Fix CUDA graph selection bug in FlashInfer at high concurrency ([#26499](https://github.com/vllm-project/vllm/pull/26499)) by @benchislett
* [Bugfix] Catch and log invalid token ids in detokenizer #2 ([#26445](https://github.com/vllm-project/vllm/pull/26445)) by @njhill
* [Bugfix] Fix MTP+FlashInfer crash when trtllm kernels are available but disabled ([#26361](https://github.com/vllm-project/vllm/pull/26361)) by @benchislett
* Fix `DotsOCR` tensor type ([#26281](https://github.com/vllm-project/vllm/pull/26281)) by @what-in-the-nim
* Fix per file ruff ignores related to line length ([#26262](https://github.com/vllm-project/vllm/pull/26262)) by @hmellor
* Fix per file ruff ignores related to simplification ([#26259](https://github.com/vllm-project/vllm/pull/26259)) by @hmellor
* Fix per file ruff ignores related to typing ([#26254](https://github.com/vllm-project/vllm/pull/26254)) by @hmellor
* Fix tensor device and dtype placement in Qwen2VL model ([#26219](https://github.com/vllm-project/vllm/pull/26219)) by @yuafng
* fix[DP][v1]: Prevent hangs from mismatched worker configurations ([#26218](https://github.com/vllm-project/vllm/pull/26218)) by @ayushsatyam146
* Fix issue of using only the part of video frame [Nemotron Nano] ([#26186](https://github.com/vllm-project/vllm/pull/26186)) by @BloodAxe
* Fix V1 engine serialization error with Ray distributed executor ([#26148](https://github.com/vllm-project/vllm/pull/26148)) by @nrghosh
* Fix undefined symbol: cutlass_moe_mm_sm100 ([#26098](https://github.com/vllm-project/vllm/pull/26098)) by @jasl
* [Bugfix] Allow skipping MoE in NVFP4 (fix for MTP) ([#25987](https://github.com/vllm-project/vllm/pull/25987)) by @benchislett
* [Bugfix] Enable padded FP4 quantization ([#25947](https://github.com/vllm-project/vllm/pull/25947)) by @roikoren755
* fix(v1/kv_cache): resolve async KV transfer bug in cascade attention ([#23485](https://github.com/vllm-project/vllm/pull/23485)) by @ayushsatyam146

## Performance

* [Models][Qwen] Replace `pad` with `cat` for better performance ([#26486](https://github.com/vllm-project/vllm/pull/26486)) by @lgeiger
* [Misc] Upgrade more code to Python 3.10 ([#26463](https://github.com/vllm-project/vllm/pull/26463)) by @DarkLight1337
* [Bugfix] Incorrect another MM data format in vllm bench throughput ([#26462](https://github.com/vllm-project/vllm/pull/26462)) by @huydhn
* [Benchmarks] Add support for Qwen 3 VL MoE tuning ([#26419](https://github.com/vllm-project/vllm/pull/26419)) by @lgeiger
* [Benchmarks] Fix imports in FP8 tuning script ([#26407](https://github.com/vllm-project/vllm/pull/26407)) by @lgeiger
* [Bugfix] Incorrect MM data format in `vllm bench throughput` ([#26395](https://github.com/vllm-project/vllm/pull/26395)) by @DarkLight1337
* [Perf] Add decode full-graph support to FlashInfer-MLA backend ([#26313](https://github.com/vllm-project/vllm/pull/26313)) by @benchislett
* [Misc] auto_tune: kill specific vllm process ([#26304](https://github.com/vllm-project/vllm/pull/26304)) by @karan
* [Metrics] Log multi-modal cache stats and fix reset ([#26285](https://github.com/vllm-project/vllm/pull/26285)) by @DarkLight1337
* [Misc] Clean up unnecessary E501 ignore ([#26274](https://github.com/vllm-project/vllm/pull/26274)) by @ywang96
* [Benchmarking] Add disable_shuffle option for dataset loading ([#26258](https://github.com/vllm-project/vllm/pull/26258)) by @ymoslem
* Update `ruff` pre-commit hooks version ([#26255](https://github.com/vllm-project/vllm/pull/26255)) by @hmellor
* [Feature] Enable E8M0 by Default on Hopper for DeepGEMM, 5% E2E throughput improvement ([#26197](https://github.com/vllm-project/vllm/pull/26197)) by @yewentao256
* [Model] Apply shared experts overlap optimization to all models with shared experts ([#26145](https://github.com/vllm-project/vllm/pull/26145)) by @bnellnm
* [Bug][Benchmark] Fix duplicate req in oversampling ([#26140](https://github.com/vllm-project/vllm/pull/26140)) by @ekagra-ranjan
* [DeepSeek] Improve performance of DS MLA cache kernel ([#26132](https://github.com/vllm-project/vllm/pull/26132)) by @MatthewBonanni
* [Perf][Easy] Early stop in request_block_hasher ([#26112](https://github.com/vllm-project/vllm/pull/26112)) by @Jialin
* [Log] Optimize DeepGEMM Missing Log ([#26106](https://github.com/vllm-project/vllm/pull/26106)) by @yewentao256
* [Refactor] Optimize FP8 MOE Backend Choice and Log ([#26044](https://github.com/vllm-project/vllm/pull/26044)) by @yewentao256
* [Misc] Add penalties sampling parameters to serve tool ([#25974](https://github.com/vllm-project/vllm/pull/25974)) by @southfreebird
* [Quantization/NVFP4] Speed up TRTLLM NVFP4 MOE weight loading and fix K/V scale loading for MLA Attn ([#25968](https://github.com/vllm-project/vllm/pull/25968)) by @pavanimajety
* [Perf] Optimize `reshape_and_cache` CUDA Kernel ([#25955](https://github.com/vllm-project/vllm/pull/25955)) by @ZJY0516
* [cpu][perf] Accelerate unquantized-linear for AArch64 through oneDNN/ACL and weight prepack ([#25948](https://github.com/vllm-project/vllm/pull/25948)) by @fadara01
* [torch.compile] Make inductor partition rules respect splitting_ops #25691 ([#25845](https://github.com/vllm-project/vllm/pull/25845)) by @baonudesifeizhai
* Optimize KV cache distribution for asymmetric pipeline parallelism ([#25164](https://github.com/vllm-project/vllm/pull/25164)) by @gholmes829
* Silu v2 ([#25074](https://github.com/vllm-project/vllm/pull/25074)) by @elvircrn
* [Transform] [Quantization] Add QuTLASS support to vLLM ([#24440](https://github.com/vllm-project/vllm/pull/24440)) by @LopezCastroRoberto
* [Feature][OCP MX] Support mxfp6 and mixed mxfp6-mxfp4 ([#21166](https://github.com/vllm-project/vllm/pull/21166)) by @fxmarty-amd
* [V1] Logit processors for rejection sampler ([#19482](https://github.com/vllm-project/vllm/pull/19482)) by @southfreebird

## Model Support

* [BUG] Qwen3-next MTP. Fix attn metadata build bug ([#26564](https://github.com/vllm-project/vllm/pull/26564)) by @vadiklyutiy
* [Core] Small simplification in `GPUModelRunner._update_states()` ([#26508](https://github.com/vllm-project/vllm/pull/26508)) by @njhill
* [CI/Build] Fix model nightly tests ([#26466](https://github.com/vllm-project/vllm/pull/26466)) by @DarkLight1337
* Update Dockerfile and install runai-model-streamer[gcs] package ([#26464](https://github.com/vllm-project/vllm/pull/26464)) by @pwschuurman
* [Models][Qwen3VL] Optimise `_validate_and_reshape_mm_tensor` ([#26426](https://github.com/vllm-project/vllm/pull/26426)) by @lgeiger
* [Models] Improve iteration over layers ([#26425](https://github.com/vllm-project/vllm/pull/26425)) by @lgeiger
* [CI] Pooling models mteb test disable enforce_eager  ([#26408](https://github.com/vllm-project/vllm/pull/26408)) by @noooop
* [Bugfix] Set the minimum python version for gpt-oss ([#26392](https://github.com/vllm-project/vllm/pull/26392)) by @jeejeelee
* [Model] Lfm2Moe ([#26344](https://github.com/vllm-project/vllm/pull/26344)) by @paulpak58
* [Model] Add support for ModernBertForTokenClassification ([#26340](https://github.com/vllm-project/vllm/pull/26340)) by @antrec
* [CI] Add Qwen3 MoE NVFP4 to Blackwell lm-eval ([#26316](https://github.com/vllm-project/vllm/pull/26316)) by @mgoin
* [Model] Use `merge_by_field_config` for MM models (Ovis family) ([#26308](https://github.com/vllm-project/vllm/pull/26308)) by @Isotr0py
* [Model] Use `merge_by_field_config` for MM models (Llava family) ([#26280](https://github.com/vllm-project/vllm/pull/26280)) by @DarkLight1337
* [Model] EVS support for nano_nemotron_vl ([#26269](https://github.com/vllm-project/vllm/pull/26269)) by @tomeras91
* [Model] Define merge_by_field_config MM interface (U-Z) ([#26261](https://github.com/vllm-project/vllm/pull/26261)) by @ayushsatyam146
* [Model] Define merge_by_field_config MM interface (R-T) ([#26260](https://github.com/vllm-project/vllm/pull/26260)) by @ayushsatyam146
* [CI] fix mamba kernel test ([#26250](https://github.com/vllm-project/vllm/pull/26250)) by @ZJY0516
* [Model] Use `merge_by_field_config` for MM models (H-L) ([#26230](https://github.com/vllm-project/vllm/pull/26230)) by @DarkLight1337
* [V1] [Hybrid] Some additional clean-up in Mamba2 prefix caching ([#26222](https://github.com/vllm-project/vllm/pull/26222)) by @tdoublep
* [Model] Support nested structures for TensorSchema ([#26212](https://github.com/vllm-project/vllm/pull/26212)) by @DarkLight1337
* [Bugfix] Fix qwen3 vl dummy data generation with overrides ([#26193](https://github.com/vllm-project/vllm/pull/26193)) by @ywang96
* [Model] Gemma3: Fix GGUF loading and quantization ([#26189](https://github.com/vllm-project/vllm/pull/26189)) by @lucianommartins
* [Core] Enable decode of context length equal to max model length ([#26168](https://github.com/vllm-project/vllm/pull/26168)) by @yannicks1
* [Model] Use `merge_by_field_config` for MM models (InternVL family) ([#26153](https://github.com/vllm-project/vllm/pull/26153)) by @DarkLight1337
* [BUG] Reorder model config creation ([#26124](https://github.com/vllm-project/vllm/pull/26124)) by @ahao-anyscale
* [BugFix][QWEN-VL]fix wrong apply_rotary_emb_torch selection introduced by #24642 ([#26123](https://github.com/vllm-project/vllm/pull/26123)) by @xuechendi
* [Model] Use `merge_by_field_config` for MM models (G) ([#26117](https://github.com/vllm-project/vllm/pull/26117)) by @DarkLight1337
* [CI] Add Blackwell LM Eval Small Models test to nightly ([#26052](https://github.com/vllm-project/vllm/pull/26052)) by @mgoin
* [Model] Fixed stream generator for gpt-oss + spec-decoding ([#26027](https://github.com/vllm-project/vllm/pull/26027)) by @astralord
* [Model] CLIP Embedding Support ([#26010](https://github.com/vllm-project/vllm/pull/26010)) by @DarkLight1337
* [Bugfix] Relax tokenizer regex for mixtral to include 'tokenizer.model' ([#25964](https://github.com/vllm-project/vllm/pull/25964)) by @BowenBao
* [Bugfix][Flashinfer] fix VLLM_USE_TRTLLM_ATTENTION issue for models with diff hyperparameters ([#25924](https://github.com/vllm-project/vllm/pull/25924)) by @elvischenv
* [Model] Supplement to PR 24862: Pass param prefix to LLMHead ([#25805](https://github.com/vllm-project/vllm/pull/25805)) by @whx-sjtu
* [gpt-oss] disable tool server initialization if no tool in request ([#25790](https://github.com/vllm-project/vllm/pull/25790)) by @qandrew
* [Flashinfer][gpt-oss] Support FP8-qkv Flashinfer TRTLLM Sinks Attention ([#25674](https://github.com/vllm-project/vllm/pull/25674)) by @elvischenv
* [GPT-OSS] Add support for arrays  at tool message content ([#25593](https://github.com/vllm-project/vllm/pull/25593)) by @luis5tb
* [GPTOSS][DP/EP][Marlin] Enable GPTOSS DP/EP using Marlin kernels ([#25488](https://github.com/vllm-project/vllm/pull/25488)) by @varun-sundar-rabindranath
* [Model] Add FlexOlmo model implementation ([#24923](https://github.com/vllm-project/vllm/pull/24923)) by @2015aroras
* Re-enable prefill of max model length ([#24446](https://github.com/vllm-project/vllm/pull/24446)) by @yannicks1
* [CI][gpt-oss] Enable python tool tests in CI ([#24315](https://github.com/vllm-project/vllm/pull/24315)) by @wuhang2014

## Hardware & Backend

* Update CUDA architecture list in build pipeline for 12.9.1 wheels ([#26592](https://github.com/vllm-project/vllm/pull/26592)) by @wseaton
* [deepseek] kernel block size for UniformTypeKVCacheSpecs ([#26559](https://github.com/vllm-project/vllm/pull/26559)) by @heheda12345
* [Bugfix] Convert untraceable GroupShape to list for AMD impl ([#26535](https://github.com/vllm-project/vllm/pull/26535)) by @Lucaskabela
* [UX] Add FlashInfer as default CUDA dependency ([#26443](https://github.com/vllm-project/vllm/pull/26443)) by @mgoin
* [Hardware][AMD] Enable FlexAttention backend on ROCm ([#26439](https://github.com/vllm-project/vllm/pull/26439)) by @mawong-amd
* Tidy `vllm/config/__init__.py` to only add classes and functions ([#26405](https://github.com/vllm-project/vllm/pull/26405)) by @hmellor
* [Docs] Fix broken table in moe_kernel_features doc ([#26314](https://github.com/vllm-project/vllm/pull/26314)) by @varun-sundar-rabindranath
* [Kernel] Centralize platform kernel import in `current_platform.import_kernels` ([#26286](https://github.com/vllm-project/vllm/pull/26286)) by @NickLucche
* [TPU] Rename tpu_commons to tpu_inference ([#26279](https://github.com/vllm-project/vllm/pull/26279)) by @utkarshsharma1
* [CI] Add comment about the single cudagraph capture size that is used ([#26252](https://github.com/vllm-project/vllm/pull/26252)) by @tdoublep
* Revert "Add batch invariant kernel override for FlashInfer backend [2/n]" ([#26220](https://github.com/vllm-project/vllm/pull/26220)) by @DarkLight1337
* Avoid division by zero in cache DS MLA kernel ([#26174](https://github.com/vllm-project/vllm/pull/26174)) by @MatthewBonanni
* [ROCm] [VL] [Bugfix] Fix vit flash attn dispatcher logic for ROCm ([#26104](https://github.com/vllm-project/vllm/pull/26104)) by @tjtanaa
* [CPU] Refine batch reorder of CPU attention backend ([#26096](https://github.com/vllm-project/vllm/pull/26096)) by @bigPYJ1151
* [Bugfix] Fix import `gemm_afp4wfp4` failure on AMD ([#26068](https://github.com/vllm-project/vllm/pull/26068)) by @zhewenl
* [NVIDIA] flashinfer TRTLLM attention prefill token limit ([#25998](https://github.com/vllm-project/vllm/pull/25998)) by @jasonlizhengjian
* [CI/Build] do not enforce precompilation on tpu ci tests ([#25992](https://github.com/vllm-project/vllm/pull/25992)) by @sixiang-google
* Quick fix for IMA with the Prefix Prefill kernel during graph capture ([#25983](https://github.com/vllm-project/vllm/pull/25983)) by @SageMoore
* [NIXL][non-cuda] Add install script for nixl with non-cuda ucx ([#25959](https://github.com/vllm-project/vllm/pull/25959)) by @xuechendi
* [Attention] Implement universal BACKEND_MAP ([#25900](https://github.com/vllm-project/vllm/pull/25900)) by @MatthewBonanni
* [Attention] Move Backend enum into registry ([#25893](https://github.com/vllm-project/vllm/pull/25893)) by @MatthewBonanni
* [Refactor][Kernel] support loading kernel from other place ([#25823](https://github.com/vllm-project/vllm/pull/25823)) by @ILikeIneine
* [TPU] update TPU benchmark threshold ([#25713](https://github.com/vllm-project/vllm/pull/25713)) by @jcyang43
* [Misc] Define EP kernel arch list in Dockerfile ([#25635](https://github.com/vllm-project/vllm/pull/25635)) by @simon-mo
* [Refactor] Refactor FP8 & INT8 Quant Folder inside `w8a8` ([#25293](https://github.com/vllm-project/vllm/pull/25293)) by @yewentao256
* [Spec-Decode] Support piecewise cudagraphs for Eagle head ([#25109](https://github.com/vllm-project/vllm/pull/25109)) by @LucasWilkinson
* Separate MLAAttention class from Attention ([#25103](https://github.com/vllm-project/vllm/pull/25103)) by @therealnaveenkamal
* [Attention][DCP] Support DCP with query length > 1 (MTP) with FA3 ([#25049](https://github.com/vllm-project/vllm/pull/25049)) by @minosfuture
* add(v1): RequestStatesStats to RequestOutput ([#24947](https://github.com/vllm-project/vllm/pull/24947)) by @huijjj
* [Kernels] Modular kernel refactor ([#24812](https://github.com/vllm-project/vllm/pull/24812)) by @bnellnm
* [Docs] add docs for cuda graph v1 ([#24374](https://github.com/vllm-project/vllm/pull/24374)) by @fhl2000
* [backends][short_conv] CUDA graph piecewise edits ([#24215](https://github.com/vllm-project/vllm/pull/24215)) by @paulpak58

## Refactoring & Core

* [Core] Simplify setting new_token_ids in CachedRequestData ([#26388](https://github.com/vllm-project/vllm/pull/26388)) by @njhill
* [responsesAPI][bugfix] serialize harmony messages ([#26185](https://github.com/vllm-project/vllm/pull/26185)) by @qandrew
* [Core] Simplify the Dp padding/should ubatch coordination logic ([#25768](https://github.com/vllm-project/vllm/pull/25768)) by @SageMoore
* [responsesAPI] add better error messaging for long prompts ([#25724](https://github.com/vllm-project/vllm/pull/25724)) by @qandrew

## Build, CI & Testing

* [Metrics] Add test for multi-modal cache stats logging ([#26588](https://github.com/vllm-project/vllm/pull/26588)) by @markmc
* [CI] fix ruff format ([#26579](https://github.com/vllm-project/vllm/pull/26579)) by @chaunceyjiang
* [CI] fix test_run_batch.py::test_completions - AssertionError ([#26578](https://github.com/vllm-project/vllm/pull/26578)) by @chaunceyjiang
* Added test_top_k_per_row to test-pipeline.yaml. ([#26569](https://github.com/vllm-project/vllm/pull/26569)) by @dcampora
* [Chore]: One pythonic tool parser test uses the wrong parser ([#26515](https://github.com/vllm-project/vllm/pull/26515)) by @bbrowning
* [CI] Fix Pre-commit Issue Cannot determine type of "rank" and "world_size" ([#26448](https://github.com/vllm-project/vllm/pull/26448)) by @yewentao256
* [CI Failure] Fix pre-commit issue for install_nixl_from_source_ubuntu.py ([#26424](https://github.com/vllm-project/vllm/pull/26424)) by @mgoin
* [ci] Rename `test_mxfp4_moe.py` to `test_ocp_mx_moe.py` ([#26364](https://github.com/vllm-project/vllm/pull/26364)) by @fxmarty-amd
* [Tests] conftest: Extending VllmRunner and HfRunner to accept token_ids as input ([#26295](https://github.com/vllm-project/vllm/pull/26295)) by @yannicks1
* [CI Bugfix] Make sure TRTLLM attention is available in test_blackwell_moe ([#26188](https://github.com/vllm-project/vllm/pull/26188)) by @mgoin
* [CI] Fix Pre-commit Mypy Error ([#26181](https://github.com/vllm-project/vllm/pull/26181)) by @yewentao256
* [test utils] correct wrong typing ([#26159](https://github.com/vllm-project/vllm/pull/26159)) by @yannicks1
* [CI] Fix distributed hybrid tests in CI ([#26155](https://github.com/vllm-project/vllm/pull/26155)) by @tdoublep
* [CI/Build] Conditionally register cutlass_fp4_group_mm to fix building on Hopper ([#26138](https://github.com/vllm-project/vllm/pull/26138)) by @mgoin
* [CI] Push multiarch manifests as nightly builds ([#25764](https://github.com/vllm-project/vllm/pull/25764)) by @csahithi

## Documentation

* [doc] add Volcengine as a compute sponsor ([#26477](https://github.com/vllm-project/vllm/pull/26477)) by @youkaichao
* [Docs] Have mergify leave a comment with the docs preview link ([#26412](https://github.com/vllm-project/vllm/pull/26412)) by @hmellor
* [Docs] Edit HF Inference Endpoints documentation ([#26275](https://github.com/vllm-project/vllm/pull/26275)) by @ariG23498
* [Doc] Edited minor typo ([#26266](https://github.com/vllm-project/vllm/pull/26266)) by @orangeng
* [MM][Doc] Add documentation for configurable mm profiling ([#26200](https://github.com/vllm-project/vllm/pull/26200)) by @wwl2755
* [DOC] Update production-stack.md ([#26177](https://github.com/vllm-project/vllm/pull/26177)) by @elieserr
* Add: Support for multiple hidden layers in Eagle3 ([#26164](https://github.com/vllm-project/vllm/pull/26164)) by @rahul-tuli
* [Docs][DBO] Add initial doc that describes the DBO implementation ([#26024](https://github.com/vllm-project/vllm/pull/26024)) by @SageMoore
* [Doc] Fixed shape description for fused_batched_moe.py ([#25668](https://github.com/vllm-project/vllm/pull/25668)) by @Egor-Krivov
* [Multi Modal] Configurable MM Profiling ([#25631](https://github.com/vllm-project/vllm/pull/25631)) by @wwl2755
* [Doc] add KAITO to integrations ([#25521](https://github.com/vllm-project/vllm/pull/25521)) by @abhisheksheth28
* [Renderer] Move Processor out of AsyncLLM  ([#24138](https://github.com/vllm-project/vllm/pull/24138)) by @KKSK-DON
* `FusedMoE` support for the Transformers backend ([#22650](https://github.com/vllm-project/vllm/pull/22650)) by @hmellor

## Miscellaneous

* Update `pre-commit` hook versions ([#26591](https://github.com/vllm-project/vllm/pull/26591)) by @hmellor
* Cache the environment variable check for batch invariance ([#26510](https://github.com/vllm-project/vllm/pull/26510)) by @bwasti
* [Bugfix] Disable moe inplace for torch >= 2.9 ([#26497](https://github.com/vllm-project/vllm/pull/26497)) by @bnellnm
* [Bugfix] fixed top_logprobs: -1 does not appear to work as intended ([#26470](https://github.com/vllm-project/vllm/pull/26470)) by @chaunceyjiang
* [BugFix] Make penalties and bad_words work with async scheduling ([#26467](https://github.com/vllm-project/vllm/pull/26467)) by @njhill
* [Core] Relax the LoRA  max rank ([#26461](https://github.com/vllm-project/vllm/pull/26461)) by @jeejeelee
* [Minor] Change warning->warning_once in preprocess ([#26455](https://github.com/vllm-project/vllm/pull/26455)) by @zhuohan123
* [Misc] Misc code simplifications ([#26450](https://github.com/vllm-project/vllm/pull/26450)) by @njhill
* [Attention] Register FLASHMLA_SPARSE ([#26441](https://github.com/vllm-project/vllm/pull/26441)) by @MatthewBonanni
* [Feature] Use pydantic validation in parallel.py config ([#26417](https://github.com/vllm-project/vllm/pull/26417)) by @simondanielsson
* [Feature] Use pydantic validation in lora.py and load.py configs ([#26413](https://github.com/vllm-project/vllm/pull/26413)) by @simondanielsson
* [Feature] Change cache.py with pydantic validation ([#26390](https://github.com/vllm-project/vllm/pull/26390)) by @vrdn-23
* [BugFix] Fix async scheduling + request preemption ([#26385](https://github.com/vllm-project/vllm/pull/26385)) by @njhill
* [Bugfix] Make DP padding optional in coordinate_batch_across_dp ([#26375](https://github.com/vllm-project/vllm/pull/26375)) by @SageMoore
* [Misc] add usedforsecurity=False in md5 hash call ([#26357](https://github.com/vllm-project/vllm/pull/26357)) by @dtrifiro
* Enable `RMSNorm` substitution for Transformers backend ([#26353](https://github.com/vllm-project/vllm/pull/26353)) by @hmellor
* Revert #24446 and #26168 ([#26332](https://github.com/vllm-project/vllm/pull/26332)) by @tdoublep
* [deepseek] add EP8 FusedMOE config for H200 and B200 ([#26331](https://github.com/vllm-project/vllm/pull/26331)) by @heheda12345
* Bump Flashinfer to v0.4.0 ([#26326](https://github.com/vllm-project/vllm/pull/26326)) by @elvischenv
* [Bugfix] Add missing sink tensor into flash attn cascade attn implementation ([#26325](https://github.com/vllm-project/vllm/pull/26325)) by @plliao
* [BUG] Fix file parsing for load_format runai_streamer_sharded ([#26324](https://github.com/vllm-project/vllm/pull/26324)) by @ahao-anyscale
* [Bug] Fix Shape Validation for Fallback while Enabling E8M0 for DeepGEMM ([#26322](https://github.com/vllm-project/vllm/pull/26322)) by @yewentao256
* [Bugfix] Respect min_tokens in scheduler stop check ([#26317](https://github.com/vllm-project/vllm/pull/26317)) by @elaineyz
* [Misc] Redact ray runtime env before logging ([#26302](https://github.com/vllm-project/vllm/pull/26302)) by @ruisearch42
* [bugfix][DCP] fix block_size of hash in DCP prefix caching ([#26296](https://github.com/vllm-project/vllm/pull/26296)) by @heheda12345
* [Frontend] Consolidate tokenizer init code ([#26276](https://github.com/vllm-project/vllm/pull/26276)) by @DarkLight1337
* Bump actions/stale from 10.0.0 to 10.1.0 ([#26272](https://github.com/vllm-project/vllm/pull/26272)) by @dependabot
* [MISC] Add heheda12345 to CODEOWNERS of vllm/config/cache.py ([#26270](https://github.com/vllm-project/vllm/pull/26270)) by @heheda12345
* [Bugfix] Always apply MM processor even when no MM items are passed ([#26240](https://github.com/vllm-project/vllm/pull/26240)) by @DarkLight1337
* [Bugfix] Allow `--skip-tokenizer-init` with `echo and return_token_ids` ([#26238](https://github.com/vllm-project/vllm/pull/26238)) by @DarkLight1337
* [Easy] Add str repr for IterationStats ([#26232](https://github.com/vllm-project/vllm/pull/26232)) by @22quinn
* [Bugfix][Hardware][RISC-V] Limit supported dtypes to float32 to avoid scheduler segfault ([#26228](https://github.com/vllm-project/vllm/pull/26228)) by @ihb2032
* [Frontend] Cache chat template kwargs resolution ([#26227](https://github.com/vllm-project/vllm/pull/26227)) by @Isotr0py
* Stop mergify from keeping stale PRs alive ([#26169](https://github.com/vllm-project/vllm/pull/26169)) by @hmellor
* [Renderer] Move Processor out of LLMEngine ([#26165](https://github.com/vllm-project/vllm/pull/26165)) by @DarkLight1337
* [Bug]: Limit num_reqs in dummy_run when max_num_seqs is small ([#26144](https://github.com/vllm-project/vllm/pull/26144)) by @benchislett
* [BugFix] Use async Mistral Tokenizer in Chat Completions ([#26134](https://github.com/vllm-project/vllm/pull/26134)) by @bbrowning
* [Misc] Clean up cruft from previous FlashMLA sparse implementation ([#26125](https://github.com/vllm-project/vllm/pull/26125)) by @LucasWilkinson
* [Bugfix] Fix mrope in Transformers Backend ([#26087](https://github.com/vllm-project/vllm/pull/26087)) by @zucchini-nlp
* [BugFix][torch.compile] Fix fused_scaled_matmul_reduce_scatter signature for PyTorch 2.8 ([#26038](https://github.com/vllm-project/vllm/pull/26038)) by @jasonlizhengjian
* [torchao] Add support for ModuleFqnToConfig using regex ([#26001](https://github.com/vllm-project/vllm/pull/26001)) by @jerryzh168
* [torchao] safetensors integration ([#25969](https://github.com/vllm-project/vllm/pull/25969)) by @liangel-02
* [Bugfix]: Assertion error when using FlashInfer backend ([#25933](https://github.com/vllm-project/vllm/pull/25933)) by @simondanielsson
* [NIXL][Misc] Expose metrics from NIXL for logging to CLI ([#25388](https://github.com/vllm-project/vllm/pull/25388)) by @NickLucche
* [Core][KVConnector] Propagate all tokens on resumed preemptions ([#24926](https://github.com/vllm-project/vllm/pull/24926)) by @QierLi
* [openai] Fix missing tool usage check (system message) ([#24768](https://github.com/vllm-project/vllm/pull/24768)) by @levunet
* [BugFix] Fix de-functionalization pass for rotary_embedding ([#23953](https://github.com/vllm-project/vllm/pull/23953)) by @angelayi
* [DP][ray] Support different VLLM_RAY_DP_PACK_STRATEGY ([#23849](https://github.com/vllm-project/vllm/pull/23849)) by @ruisearch42
* [Bugfix] Fix gemma3 with transformers backend ([#23178](https://github.com/vllm-project/vllm/pull/23178)) by @zucchini-nlp
* [Bugfix] Move current_platform import to avoid python import cache. ([#16601](https://github.com/vllm-project/vllm/pull/16601)) by @iwzbi

## Breaking Changes

* [Core] Remove unused `prev_sampled_token_ids_invalid_indices` input batch field ([#26514](https://github.com/vllm-project/vllm/pull/26514)) by @njhill
* [Model][Qwen3VL] Compute `cu_seqlens` on CPU to remove  ([#26496](https://github.com/vllm-project/vllm/pull/26496)) by @lgeiger
* Upgrade Pydantic to v2.12.0 and remove hack for Python 3.13 ([#26481](https://github.com/vllm-project/vllm/pull/26481)) by @hmellor
* [V0 deprecation] Remove `QKVCrossParallelLinear` implementation  ([#26475](https://github.com/vllm-project/vllm/pull/26475)) by @Isotr0py
* Revert  #26113 "[Frontend] CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops" ([#26472](https://github.com/vllm-project/vllm/pull/26472)) by @ZJY0516
* [BUGFIX] Add cu_tokens_across_sp to DPMetadata ([#26457](https://github.com/vllm-project/vllm/pull/26457)) by @SageMoore
* [BugFix] Fix failing test quantization/test_compressed_tensors.py::test_compressed_tensors_fp8_block_enabled ([#26436](https://github.com/vllm-project/vllm/pull/26436)) by @morrison-turnansky
* [Bugfix] Fix SHM cache initialization ([#26427](https://github.com/vllm-project/vllm/pull/26427)) by @DarkLight1337
* [Bug] Fix DeepGEMM Attention Test ([#26423](https://github.com/vllm-project/vllm/pull/26423)) by @yewentao256
* Remove Python 3.9 support ahead of PyTorch 2.9 in v0.11.1 ([#26416](https://github.com/vllm-project/vllm/pull/26416)) by @hmellor
* [Model] Allow passing custom number of max tiles to Nano 2 VL ([#26403](https://github.com/vllm-project/vllm/pull/26403)) by @BloodAxe
* [BugFix] Fix noop elimination edge case ([#26394](https://github.com/vllm-project/vllm/pull/26394)) by @andylolu2
* Refactor MistralTokenizer ([#26358](https://github.com/vllm-project/vllm/pull/26358)) by @juliendenize
* [Misc] Move `LRUCache` into its own file ([#26342](https://github.com/vllm-project/vllm/pull/26342)) by @DarkLight1337
* [V0 Deprecation] Remove `VLLM_USE_V1` from tests ([#26341](https://github.com/vllm-project/vllm/pull/26341)) by @DarkLight1337
* [V0 Deprecation] Remove `VLLM_USE_V1` from docs and scripts ([#26336](https://github.com/vllm-project/vllm/pull/26336)) by @DarkLight1337
* [Deprecation] Deprecate `LLM.set_tokenizer` ([#26333](https://github.com/vllm-project/vllm/pull/26333)) by @DarkLight1337
* [Benchmark] Enable MM Embedding benchmarks ([#26310](https://github.com/vllm-project/vllm/pull/26310)) by @DarkLight1337
* [BugFix] Update KV block hash type from BlockHash to ExternalBlockHash in kv_events_subscriber - #26264 ([#26265](https://github.com/vllm-project/vllm/pull/26265)) by @atalhens
* Remove all cases of `fmt: on/off` ([#26253](https://github.com/vllm-project/vllm/pull/26253)) by @hmellor
* Remove all references to `yapf` as it's no longer used ([#26251](https://github.com/vllm-project/vllm/pull/26251)) by @hmellor
* fix(tests): Resolve late binding of loop variable in assert message lambda ([#26249](https://github.com/vllm-project/vllm/pull/26249)) by @ihb2032
* Convert formatting to use `ruff` instead of `yapf` + `isort` ([#26247](https://github.com/vllm-project/vllm/pull/26247)) by @hmellor
* [Bugfix][Spec Decode] Fix wrong valid_mask for padded speculation when chunked prefill occurs ([#26231](https://github.com/vllm-project/vllm/pull/26231)) by @seven-mile
* [V1] [Hybrid] Remove code to override default CUDA graph configuration ([#26226](https://github.com/vllm-project/vllm/pull/26226)) by @tdoublep
* [Renderer] Clean up renderer code ([#26216](https://github.com/vllm-project/vllm/pull/26216)) by @DarkLight1337
* [Misc] Remove unused `executor.apply_model` ([#26215](https://github.com/vllm-project/vllm/pull/26215)) by @DarkLight1337
* [Misc] Require `merge_by_field_config` argument ([#26214](https://github.com/vllm-project/vllm/pull/26214)) by @DarkLight1337
* [BugFix] Pad input buffers in _dummy_run ([#26209](https://github.com/vllm-project/vllm/pull/26209)) by @varun-sundar-rabindranath
* [Perf] Remove hardcoded num_warps=1 ([#26183](https://github.com/vllm-project/vllm/pull/26183)) by @coreylowman
* [Misc] Remove typing.List ([#26150](https://github.com/vllm-project/vllm/pull/26150)) by @varun-sundar-rabindranath
* [Bug] Fix Test in Batch Invariant ([#26128](https://github.com/vllm-project/vllm/pull/26128)) by @yewentao256
* [Frontend] CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops ([#26113](https://github.com/vllm-project/vllm/pull/26113)) by @morrison-turnansky
* [Build/CI] Revert back to Ubuntu 20.04, install python 3.12 with uv ([#26103](https://github.com/vllm-project/vllm/pull/26103)) by @tlrmchlsmth
* [Input] Remove unused `prompt` field ([#26097](https://github.com/vllm-project/vllm/pull/26097)) by @DarkLight1337
* [CI Failure] fix_test_auto_prefix_cache_support ([#26053](https://github.com/vllm-project/vllm/pull/26053)) by @hl475
* [Bugfix] Fix `_reqs_to_process` leak on abort ([#26012](https://github.com/vllm-project/vllm/pull/26012)) by @NickLucche
* [Spec Decode] Enable efficient speculative decoding with FlashInfer-MLA ([#25984](https://github.com/vllm-project/vllm/pull/25984)) by @benchislett
* Remove LoRA bias support ([#25807](https://github.com/vllm-project/vllm/pull/25807)) by @ashwin-phadke
* [V1] [Hybrid] Mamba2 Automatic Prefix Caching ([#25752](https://github.com/vllm-project/vllm/pull/25752)) by @s3woz
* [UX] Support nested dicts in hf_overrides ([#25727](https://github.com/vllm-project/vllm/pull/25727)) by @mgoin
* [ROCm] Split AITER unified attention into its own backend ([#25507](https://github.com/vllm-project/vllm/pull/25507)) by @gshtras
* [Bugfix] Fix `vllm bench ...` on CPU-only head nodes ([#25283](https://github.com/vllm-project/vllm/pull/25283)) by @Aydin-ab
* [NIXL] Ignore abort on already-finished request ([#25067](https://github.com/vllm-project/vllm/pull/25067)) by @markmc
* [Hybrid]: Decouple Kernel Block Size from KV Page Size ([#24486](https://github.com/vllm-project/vllm/pull/24486)) by @zhiyuan1i
* [Attention] Remove unused reorder_batch method ([#24463](https://github.com/vllm-project/vllm/pull/24463)) by @MatthewBonanni
* AOT Compilation for torch.compile (Bundled) ([#24274](https://github.com/vllm-project/vllm/pull/24274)) by @zhxchen17

## Upgrade Notes

- Note: when `coordinate_batch_across_dp` decides to use microbatching, DP padding will be applied unconditionally regardless of the `allow_dp_padding` setting.
- # NOTE: Each image corresponds to 1 token so this is reasonable
- - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- NOTE: There were a couple of files which had `yapf` globally disabled, I think this caused ruff to ignore them too. That's why the lines changed count is so high.
- Note: This is a temporary workaround, not a root cause fix. The underlying scheduler bug in the Python-level scheduling logic still needs to be addressed separately.
- Note: all model types appear to be covered by tests/models/registry.py

## Contributors

@2015aroras, @22quinn, @Aydin-ab, @Barry-Delaney, @BloodAxe, @BowenBao, @BoyuanFeng, @DarkLight1337, @Egor-Krivov, @ILikeIneine, @Isotr0py, @Jialin, @KKSK-DON, @LopezCastroRoberto, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @NickLucche, @QierLi, @SageMoore, @ZJY0516, @abhisheksheth28, @ahao-anyscale, @andylolu2, @angelayi, @antrec, @ariG23498, @ashwin-phadke, @astralord, @atalhens, @ayushsatyam146, @baonudesifeizhai, @bbrowning, @benchislett, @bigPYJ1151, @bnellnm, @bwasti, @cfRod, @chaunceyjiang, @coreylowman, @csahithi, @dcampora, @dependabot, @dtrifiro, @ekagra-ranjan, @elaineyz, @elieserr, @elvircrn, @elvischenv, @fadara01, @fhl2000, @fxmarty-amd, @gholmes829, @gshtras, @heheda12345, @hl475, @hmellor, @huijjj, @huydhn, @ihb2032, @isharif168, @iwzbi, @jasl, @jasonlizhengjian, @jcyang43, @jeejeelee, @jerryzh168, @juliendenize, @karan, @levunet, @lgeiger, @liangel-02, @lucianommartins, @luis5tb, @markmc, @mawong-amd, @maxdebayser, @mgoin, @minosfuture, @morrison-turnansky, @njhill, @noooop, @nrghosh, @orangeng, @paulpak58, @pavanimajety, @plliao, @pwschuurman, @qandrew, @rahul-tuli, @roikoren755, @ruisearch42, @s3woz, @sergiopaniego, @seven-mile, @simon-mo, @simondanielsson, @sixiang-google, @soldni, @southfreebird, @tdoublep, @therealnaveenkamal, @tjtanaa, @tlrmchlsmth, @tomeras91, @utkarshsharma1, @vadiklyutiy, @varun-sundar-rabindranath, @vrdn-23, @wangxiongts, @what-in-the-nim, @whx-sjtu, @wseaton, @wuhang2014, @wwl2755, @xuechendi, @yannicks1, @yewentao256, @ymoslem, @youkaichao, @yuafng, @ywang96, @zhewenl, @zhiyuan1i, @zhuohan123, @zhxchen17, @zucchini-nlp