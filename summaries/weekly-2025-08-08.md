## Weekly Summary for vllm-project/vllm (2025-08-08)

* Fix pre-commit (#22487) by @DarkLight1337
* [PERF] Use pybase64 to more quickly decode prompt embeddings (#22469) by @qthequartermasterman
* Optimize MiniCPMO mask creation with vectorized implementation (#22464) by @skyloevil
* Fix pre-commit error in main (#22462) by @WoosukKwon
* not tie_word_embeddings for glm-4.5 and glm-4.5v (#22460) by @zRzRzRzRzRzRzR
* [Core] Simplify mm processing cache (#22457) by @DarkLight1337
* Remove `from_dict` from `SpeculativeConfig` (#22451) by @hmellor
* [Frontend] Use engine argument to control MM cache size (#22441) by @DarkLight1337
* [Docs] Add missing dependency for docs build (#22435) by @hmellor
* [Tool] Fix auto tool call (#22434) by @heheda12345
* Add H20-3e fused MoE kernel tuning configs for GLM-4.5 (#22433) by @JaceyShao
* [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 (#22426) by @nvpohanh
* [Misc] Enhance code formatting in mxfp4.py  (#22423) by @WoosukKwon
* [Bugfix] Fix wrong method name in Intern-S1 image processor (#22417) by @DarkLight1337
* [CI] Skip the pooling models that do not support transformers v4.55 (#22411) by @noooop
* [gpt-oss] Generate ResponseOutputItem from Harmony Message (#22410) by @heheda12345
* [Bench] Split serve.py:main into async/async versions (#22405) by @lk-chen
* [gpt-oss] Convert user input to harmony format (#22402) by @heheda12345
* [gpt-oss] fix model config with hf_config (#22401) by @zyongye
* [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue (#22399) by @yewentao256
* [gpt-oss] add demo tool server (#22393) by @heheda12345
* Update `flashinfer-python==0.2.10` (#22389) by @mgoin
* Use float32 for test_completion.py (#22385) by @mgoin
* [Doc] Fix link to prefix caching design (#22384) by @sarckk
* Fix trtllm-gen attention env and add attention sink (#22378) by @IwakuraRein
* [gpt-oss] Add loop for built-in tool call (#22374) by @WoosukKwon
* Optimize logger init performance by using module-level constants (#22373) by @skyloevil
* [Misc] normalize multiprocessing Queue usage (#22371) by @andyxning
* [Bugfix] Make condition in triton kernel constexpr (#22370) by @gshtras
* [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks (#22368) by @LucasWilkinson
* add the codes to check AMD Instinct GPU number (#22367) by @zhangnju
* [BugFix] Fix FA2 RuntimeError when sinks is provided (#22365) by @LucasWilkinson
* Update `hf_xet` pin to resolve hangs (#22356) by @hmellor
* [Bugfix] Add missing `packed_modules_mapping` to `DeepseekV2ForCausalLM` (#22352) by @fxmarty-amd
* [XPU]Fix `flash_attn_varlen_func` interface on xpu (#22350) by @jikunshang
* [Minor] Fix type  (#22347) by @WoosukKwon
* [gpt-oss] Support chat completion api (#22342) by @WoosukKwon
* [gpt-oss] Add Tool/ConversationContext classes and harmony_utils (#22340) by @WoosukKwon
* [gpt-oss] flashinfer mxfp4 (#22339) by @zyongye
* [gpt-oss] add model to supported models doc (#22336) by @ywang96
* [gpt-oss] attention sink init fix gemini (#22335) by @zyongye
* [gpt-oss] Add openai-harmony as default dependency (#22332) by @WoosukKwon
* [gpt-oss] flashinfer attention sink init (#22330) by @zyongye
* [ROCm] Add attention sink to use_rocm_custom_paged_attention (#22329) by @WoosukKwon
* Add GPT-OSS model code and config [1/N] (#22327) by @WoosukKwon
* [GptOss] Add GptOss reasoning parser to support structure output (#22322) by @heheda12345
* Add attention sink in attention backends (#22320) by @WoosukKwon
* Increase openai-python version (#22316) by @WoosukKwon
* [Bugfix] Add proper comparison for package versions (#22314) by @syedmba
* Upgrade FA3 for attention sink (#22313) by @WoosukKwon
* [Misc] Clean up duplicated hf overrides (#22311) by @Isotr0py
* [Bugfix] Remove faulty test for oot attention backend (#22286) by @mgoin
* [Bugfix] Fix 3D input passed into cutlass_scaled_mm (#22278) by @mgoin
* [Bugfix] Skip dead and non-GPU nodes for Ray DP engine allocation (#22275) by @ruisearch42
* Support encoder_only attention for FlexAttention (#22273) by @maxdebayser
* [CI][TPU] Fix docker clean up (#22271) by @lsy323
* [Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm (#22264) by @gshtras
* [Bugfix] Fix MoE BNB version (#22260) by @jeejeelee
* [bugfix] fix blackwell deepep installation (#22255) by @youkaichao
* [V0 Deprecation][TPU] Remove V1 flag check from tests (#22248) by @NickLucche
* [Docs][TPU] Highlight TPU Software version selection (#22242) by @NickLucche
* [CI/Build] Update flashinfer to 0.2.9 (#22233) by @mgoin
* [Core] Factor out common logic for MM budget calculation (#22228) by @DarkLight1337
* [Bugfix] Misaligned params in TreeAttentionImpl (#22226) by @DarkLight1337
* Revert "[Bugfix] V1 Fix the cursor leakage issue during request scheduling." (#22223) by @WoosukKwon
* [UX] Fail if an invalid attention backend is specified (#22217) by @mgoin
* preload heavy modules when mp method is forkserver (#22214) by @lionelvillard
* [Log] DeepGEMM Update Log for Unaligned Problem Size (#22208) by @yewentao256
* [V1] reduce block size for tree attention correctness test to fix 'ouâ€¦ (#22207) by @TheEpicDolphin
* Optimize configuration access with LRU cache in custom ops (#22204) by @skyloevil
* self.gate dtype update for GLM-4.5 (#22203) by @zRzRzRzRzRzRzR
* [ROCm][Bugfix] Compilation passes fix (#22202) by @gshtras
* [Refactor] Remove Unused Environment Variable `VLLM_NO_DEPRECATION_WARNING` (#22199) by @yewentao256
* [Core] Store only the keys for multi-modal data in P0 (#22198) by @DarkLight1337
* [FEAT] Refactor ROPE into module (#22192) by @tjtanaa
* [Doc] Update pooling model docs (#22186) by @DarkLight1337
* [Responses API] Ignore `store=True` and process the request by default (#22185) by @WoosukKwon
* [Model] Switch to Fused RMS norm in Qwen2.5_VL model. (#22184) by @vllmellm
* [Bugfix] Fix failing GGUF models test (#22174) by @Isotr0py
* [Misc] Modify the organization of GLM series  (#22171) by @jeejeelee
* [Bugfix] EPLB load statistics problem (#22167) by @david6666666
* [model] Support MiniCPM-V 4.0 (#22166) by @tc-mb
* [Docs] Update features/disagg_prefill, add v1 examples and development (#22165) by @david6666666
* [CI Bugfix] Fix wNa16 kernel not found for test_shared_storage_connector_hashes (#22163) by @tlrmchlsmth
* [RLHF] Fix torch.dtype not serializable in example (#22158) by @22quinn
* [refactor] improve ConstantList exception specificity (#22156) by @skyloevil
* [fix] fix correct assertion syntax error in attention utils. (#22154) by @skyloevil
* [Bugfix] Fix failing multimodal standard test (#22153) by @Isotr0py
* fix: kimi_k2 return empty tool call list (#22149) by @tlipoca9
* remove duplicate code within cleanup_dist_env_and_memory (#22147) by @andyxning
* [CI/Build][Bugfix] Fix Qwen2.5 tests in CPU CI via fallback silu_and_mul to torch native implementation (#22145) by @bigPYJ1151
* [Misc] log more detailed message for ensure_model_parallel_initialized (#22144) by @andyxning
* fuse fp32 for GLM-4.5 e_score_correction_bias (#22143) by @zRzRzRzRzRzRzR
* [Doc] add backend to doc string of initialize_model_parallel (#22142) by @andyxning
* [Responses API] Disable response store by default (#22137) by @WoosukKwon
* [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) (#22131) by @0xjunhao
* Use UV_LINK_MODE=copy in Dockerfile to avoid hardlink fail (#22128) by @mgoin
* [Misc] Bump ray to 2.48.0 (#22123) by @ruisearch42
* Revert "[compile][startup] Disable C++ compilation of symbolic shapes" (#22122) by @xiszishu
* [Frontend] Improve error message for too many mm items (#22114) by @DarkLight1337
* docs: remove deprecated disable-log-requests flag (#22113) by @ywang96
* [Fix] Fix llama4 modelopt weight loading error (#22107) by @jiahanc
* Remove index_put from MM embeddings merging (#22105) by @chenxi-yang
* Fix pre-commit failure for SECURTIY.md (#22102) by @mgoin
* [Misc] `VLLM_TARGET_DEVICE.lower()` (#22101) by @NickLucche
* [Frontend] Update OpenAI error response to upstream format (#22099) by @msanft
* [NVIDIA] Support Flashinfer TRT-LLM Prefill Attention Kernel (#22095) by @elvischenv
* [Docs] use `uv` in CPU installation docs (#22089) by @davidxia
* Revert "Update sampling_metadata.py (#21937)" (#22088) by @hmellor
* [BUG] [ROCm] Fix import bug on ROCm (#22083) by @tjtanaa
* [NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading (#22073) by @nvpohanh
* [FEAT][ROCm] Enable running Flash Attention as ViT attn backend for Qwen-VL models on ROCm platform. (#22069) by @vllmellm
* [Misc] Minor enhancement of benchmark_moe (#22068) by @jeejeelee
* [Bugfix] Fix glm4.1v video inference issue (#22067) by @Isotr0py
* [Model] Qwen2.5 VL SiLU-and-Mul (#22066) by @vllmellm
* [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match (#22065) by @chaunceyjiang
* [Doc] Add example for Step3-VL (#22061) by @ywang96
* [Misc] Remove upper bound in openai package version (#22060) by @WoosukKwon
* [Doc] Add Voxtral to Supported Models page (#22059) by @DarkLight1337
* Revert precompile wheel changes (#22055) by @simon-mo
* feat(multimodal): Add customizable background color for RGBA to RGB conversion (#22052) by @ahengljh
* [Doc] Added warning of speculating with draft model (#22047) by @david6666666
* [Doc] Fix a syntax error of example code in structured_outputs.md (#22045) by @hsliuustc0106
* [Misc] Getting and passing ray runtime_env to workers (#22040) by @ruisearch42
* Fix test_kv_sharing_fast_prefill flakiness (#22038) by @sarckk
* [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036) by @yewentao256
* [Bugfix] Mamba2 remove bugged initial state condition in chunk scan (#22034) by @cyang49
* [BugFix] Don't change title of top-level process (#22032) by @njhill
* [Misc] update doc comment for send (#22026) by @andyxning
* Fix `get_kwargs` for case where type hint is `list[Union[str, type]]` (#22016) by @hmellor
* [Bugfix]: Fix the streaming output for function calls in the minimax (#22015) by @qscqesze
* [Misc] Automatically resolve HF processor init kwargs (#22005) by @DarkLight1337
* for glm-4.1V update (#22000) by @zRzRzRzRzRzRzR
* [Misc] Support routing logic simulation (#21990) by @minosfuture
* Use `aiohttp` connection pool for benchmarking (#21981) by @eicherseiji
* [V1] [P/D] Refactor KV Connector Path (#21980) by @sdavidbd
* [Model] [Quantization] Support quantization for Gemma3n (#21974) by @kylesayrs
* Add lora test for tp>1 case for TPU. (#21970) by @vanbasten23
* Fix Flashinfer CUTLASS MOE Allgather (#21963) by @wenscarl
* [Feature] Non-contiguous Support for FP8 Quantization (#21961) by @yewentao256
* Move flashinfer-python to optional extra `vllm[flashinfer]` (#21959) by @mgoin
* [Misc] DeepGemmExperts : Avoid JIT generation in the hot-path (#21955) by @varun-sundar-rabindranath
* [Quantization] Enable BNB support for InternS1 (#21953) by @jeejeelee
* [BugFix] Update AttnFusionPass cache key (#21947) by @zou3519
* [Misc] correct static type check for GroupCoordinator (#21946) by @andyxning
* Update sampling_metadata.py (#21937) by @Aviadr-neureality
* Update transformers to `v4.55` (#21931) by @hmellor
* [PD] add test for chat completions endpoint (#21925) by @Abirdcfly
* [Qwen3] Enable dual-chunk-attention support for Qwen3 models. (#21924) by @sighingnow
* [Bugfix] fix when skip tokenizer init (#21922) by @lengrongfu
* [Misc] Remove pass_config from CompilationConfig dump_json excluded (#21911) by @elvischenv
* [Bugfix] Disable multi-modal preprocessor cache for DP (#21896) by @DarkLight1337
* [Bugfix] Check NVIDIA artifactory is accessible before using flashinfer cubin kernels (#21893) by @mgoin
* [Bugfix] Add log prefix in non-dp mode engine core (#21889) by @wuhang2014
* [CI] Initial tests for SM100 Blackwell runner (#21877) by @mgoin
* [Perf] Parallelize fill_bitmask to accelerate high-throughput guided decoding (#21862) by @benchislett
* [Bugfix][PD] set max_completion_tokens=1 if req has this value (#21841) by @Abirdcfly
* [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837) by @varun-sundar-rabindranath
* [Speculators][Speculative Decoding] Add Qwen Eagle3 Support (#21835) by @dsikka
* [Bugfix][V1][P/D]Fix the uneven polling issue in the toy proxy for P2pNcclConnector (#21819) by @Abatom
* [Sampler] Support returning all logprobs or logits (#21792) by @22quinn
* [Refactor] Remove Duplicate `per_block_cast_to_fp8`, Remove Dependencies of DeepGEMM (#21787) by @yewentao256
* [executor] feat: add supports_pp attr to executors (#21786) by @eric-haibin-lin
* [V0 deprecation][P/D] Deprecate v0 `KVConnectorBase` code (1/2) (#21785) by @lk-chen
* [Core] Avoid repeated len(block_token_ids) check in hash_request_tokens (#21781) by @linzebing
* Migrate KimiVLImagePixelInputs to TensorSchema (#21769) by @bbeckca
* Enable headless models for pooling in the Transformers backend (#21767) by @hmellor
* [Misc] Add tensor schema test coverage for multimodal models (#21754) by @Isotr0py
* Deprecate `--disable-log-requests` and replace with `--enable-log-requests` (#21739) by @hmellor
* feat: Add Support GPTQ Quantization MOE on ROCM vllm serve (#21733) by @JartX
* Fix Arcee model weight loading: Add custom load_weights (#21725) by @alyosha-swamy
* [Benchmark] Support ready check timeout in `vllm bench serve` (#21696) by @yeqcharlotte
* Introduce RayPPCommunicator for ray-based PP (#21660) by @ruisearch42
* [xpu]support moe models on XPU platform (#21643) by @yma11
* [Bug] Update auto_tune.sh to separate benchmarking and profiling. (#21629) by @ericehanley
* [BugFix] Improve internal DP load balancing (#21617) by @njhill
* [Attention] Support multiple attention metadata builders per kv_cache_spec  + proper local attention no hybrid kv cache fix (#21588) by @LucasWilkinson
* [Docs] Factor out troubleshooting to its own guide; add section for Ray Observability (#21578) by @crypdick
* [Test] Add Unit Test for Batched DeepGEMM (#21559) by @yewentao256
* [V1] [Hybrid] Validate compatibility of attention backend batch reordering at init time (#21557) by @tdoublep
* [BugFix] Harden distributed DP startup (#21538) by @njhill
* Add DeepGEMM to Dockerfile in vllm-base image (#21533) by @MatthewBonanni
* Improve documentation of `ModelConfig.try_get_generation_config` to prevent future confusion (#21526) by @hmellor
* [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine (#21496) by @tjtanaa
* [Refactor] Fix Compile Warning #1444-D (#21462) by @yewentao256
* [V1][CUDA] Full cudagraph support for FlashInfer (#21367) by @fhl2000
* [Speculative Decoding] Add `speculators` config support (#21345) by @dsikka
* [V1] port xformers backend to v1 (#21342) by @TheEpicDolphin
* Support Tensorrt-LLM MoE fp4 for low-latency (#21331) by @wenscarl
* Support CUTLASS NVFP4 (w4a4) for Blackwell Geforce GPUs (SM120) (#21309) by @LopezCastroRoberto
* [v1] - Mamba1 Attention Metadata (#21249) by @Josephasafg
* Add chat doc in quick start (#21213) by @TankNee
* [Bugfix] V1 Fix the cursor leakage issue during request scheduling. (#21173) by @CLFutureX
* [feat] move WEIGHT_SCALE_SUPPORTED into raise block to accelerate RLHF weight loading (#21164) by @weixiao-huang
* [Attention][DBO] Add support for "splitting" the CommonAttentionMetadata (#21153) by @SageMoore
* security policy: take 1 (#21119) by @sidhpurwala-huzaifa
* [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075) by @cyang49
* [Frontend] Align tool_choice="required" behavior with OpenAI when tools is empty (#21052) by @n0gu-furiosa
* [Bugfix] Fix: Fix multi loras with tp >=2 and LRU cache (#20873) by @charent
* [compile][startup] Disable C++ compilation of symbolic shapes (#20836) by @anijain2305
* feat: Add --enable-log-outputs flag for logging model generations (#20707) by @mizadri
* [Model] Pooling model activation supports per request control by PoolingParams (#20538) by @noooop
* Add tree attention backend for v1 (part 1) (#20401) by @TheEpicDolphin
* Add ModelOpt Qwen3 nvfp4 support (#20101) by @Edwardf0t1
* [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000) by @vadiklyutiy
* [BugFix] fix: aot passes kvcache dtype information (#19750) by @mickaelseznec
* [Doc] update docs for nightly benchmarks (#12022) by @andrewkchan
