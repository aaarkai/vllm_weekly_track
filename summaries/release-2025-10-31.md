# Weekly Release Report for vllm-project/vllm (2025-10-31)

This week merged 179 PRs from 105 contributors. Key areas: features 2, fixes 11, performance 15.

## Executive Summary

本周发布重点聚焦于代码重构、性能优化及模型扩展。主要特性包括提取 math 和 argparse 工具至独立模块、优化启动日志、新增 `vllm bench sweep` CLI 命令，并扩展了视频支持及批处理维度配置。修复方面解决了初始化导入、异步调度中未调度请求处理、恢复请求在 SharedStorageConnector 中的问题，以及 HuggingFace 模板参数和多模态模型占位符替换等关键缺陷。

性能优化涉及计算块保存逻辑、请求 ID 唯一性、并发专家执行及 CPU 利用率提升。模型支持新增 Kimi Linear、Ouro Model、Siglip2 和 MiniMax-M2，并修复了 Qwen 系列视觉语言模型的 FlashAttention 和编译问题。硬件兼容性改进涵盖 AMD、Intel Gaudi 3 及 AArch64 CPU。破坏性变更包括移除 VLLM_USE_V1、启用 CPU KV 卸载及清理过时代码，建议用户参考升级指南进行适配。

## Highlights

* [Chore]:Extract math and argparse utilities to separate modules ([#27188](https://github.com/vllm-project/vllm/pull/27188)) by @yeshsurya
* [Log] Optimize Startup Log ([#26740](https://github.com/vllm-project/vllm/pull/26740)) by @yewentao256
* [Misc] Clean up more utils ([#27567](https://github.com/vllm-project/vllm/pull/27567)) by @DarkLight1337
* Feature/video support in random mm dataset ([#25963](https://github.com/vllm-project/vllm/pull/25963)) by @BloodAxe
* [Frontend] Add `vllm bench sweep` to CLI ([#27639](https://github.com/vllm-project/vllm/pull/27639)) by @DarkLight1337

## Features & Enhancements

* Add more dims for batch invariant shims ([#27489](https://github.com/vllm-project/vllm/pull/27489)) by @bwasti
* Add load pattern configuration guide to benchmarks ([#26886](https://github.com/vllm-project/vllm/pull/26886)) by @mpashkovskii

## Bug Fixes

* [BugFix] Fix broken import in initialize_ray_cluster() ([#27838](https://github.com/vllm-project/vllm/pull/27838)) by @njhill
* [BugFix] Stopgap - Flashinfer Autotuner + GPT-OSS + DP/TP ([#27762](https://github.com/vllm-project/vllm/pull/27762)) by @varun-sundar-rabindranath
* [BugFix] Handle unscheduled requests properly when async scheduling ([#27756](https://github.com/vllm-project/vllm/pull/27756)) by @njhill
* [BugFix] Fix handling of resumed reqs in `SharedStorageConnector` ([#27719](https://github.com/vllm-project/vllm/pull/27719)) by @njhill
* fix: allow HuggingFace standard chat template params via **kwargs ([#27622](https://github.com/vllm-project/vllm/pull/27622)) by @wangln19
* fixing mm placeholder replacement issue with gemma3 ([#27538](https://github.com/vllm-project/vllm/pull/27538)) by @tingtingtang1992
* Fix MiniMax-M2 copyright ([#27537](https://github.com/vllm-project/vllm/pull/27537)) by @rogeryoungh
* fix m2 test ([#27536](https://github.com/vllm-project/vllm/pull/27536)) by @youkaichao
* Fix test named tool use ([#27458](https://github.com/vllm-project/vllm/pull/27458)) by @chaunceyjiang
* Fix EventPublisherFactory logic for disabled KV cache events ([#27419](https://github.com/vllm-project/vllm/pull/27419)) by @usberkeley
* Fix AArch64 CPU Docker pipeline ([#27331](https://github.com/vllm-project/vllm/pull/27331)) by @ioana-ghiban-arm

## Performance

* [Core][Perf] Only invoke save_new_computed_blocks when computed blocks are not empty ([#27799](https://github.com/vllm-project/vllm/pull/27799)) by @Jialin
* [benchmark] Make request IDs unique across clients by default ([#27723](https://github.com/vllm-project/vllm/pull/27723)) by @eicherseiji
* [Frontend] Add `vllm bench sweep` to CLI ([#27639](https://github.com/vllm-project/vllm/pull/27639)) by @DarkLight1337
* [Core][Bookkeeping Optimization] Update against numpy view of is_token_ids tensor ([#27618](https://github.com/vllm-project/vllm/pull/27618)) by @Jialin
* [perf] Enable concurrent execution of "shared_experts" and "selected_experts" in qwen3-next ([#27578](https://github.com/vllm-project/vllm/pull/27578)) by @ZJY0516
* [Bugfix] fix empty prompts for async-engine mode in benchmark throughput ([#27494](https://github.com/vllm-project/vllm/pull/27494)) by @luccafong
* [Chore] Optimize P2PNCCLEngine `http_address` ([#27488](https://github.com/vllm-project/vllm/pull/27488)) by @yewentao256
* [Benchmark] Enable benchmark to run with `encoding_format="bytes"` ([#27467](https://github.com/vllm-project/vllm/pull/27467)) by @DarkLight1337
* [Performance][LoRA] add context varying params to 'do_not_specialize' in fused moe lora ([#27445](https://github.com/vllm-project/vllm/pull/27445)) by @gnovack
* [cpu][perf] Fix low CPU utilization with VLLM_CPU_OMP_THREADS_BIND on AArch64 ([#27415](https://github.com/vllm-project/vllm/pull/27415)) by @fadara01
* [Chore]:Extract math and argparse utilities to separate modules ([#27188](https://github.com/vllm-project/vllm/pull/27188)) by @yeshsurya
* [CI/Build][Intel] Enable performance benchmarks for Intel Gaudi 3 ([#26919](https://github.com/vllm-project/vllm/pull/26919)) by @jakub-sochacki
* [Log] Optimize Startup Log ([#26740](https://github.com/vllm-project/vllm/pull/26740)) by @yewentao256
* Feature/video support in random mm dataset ([#25963](https://github.com/vllm-project/vllm/pull/25963)) by @BloodAxe
* [Misc][qwen2_5_vl][torch.compile] Enable `supports_torch_compile` on generic nn.Module and demonstrate speedup on Qwen Vision model ([#23207](https://github.com/vllm-project/vllm/pull/23207)) by @Lucaskabela

## Model Support

* [Bugfix] Fix 2 precommit issues - (mamba_block_size, kv_cache_config) ([#27811](https://github.com/vllm-project/vllm/pull/27811)) by @tlrmchlsmth
* [Model] Introduce Kimi Linear to vLLM ([#27809](https://github.com/vllm-project/vllm/pull/27809)) by @zhiyuan1i
* [Model][Ouro] Support Ouro Model ([#27794](https://github.com/vllm-project/vllm/pull/27794)) by @FlamingoPg
* [BugFix][VL] Fix FA selection on Qwen2.5-VL ([#27790](https://github.com/vllm-project/vllm/pull/27790)) by @zhewenl
* [Bugfix] mamba-block-size is set for vision language model ([#27773](https://github.com/vllm-project/vllm/pull/27773)) by @heheda12345
* [Temp fix] Disable torch.compile for Qwen2.5 VL's VisionBlock temporarily.  ([#27760](https://github.com/vllm-project/vllm/pull/27760)) by @huachenheli
* [CI Failure] Fix test_kv_cache_model_load_and_run ([#27717](https://github.com/vllm-project/vllm/pull/27717)) by @hl475
* [CI/Build][Bugfix]Fix Quantized Models Test on AMD ([#27712](https://github.com/vllm-project/vllm/pull/27712)) by @zhewenl
* [Model] Fix Qwen3VL and Qwen3Omni after torch.compile changes ([#27705](https://github.com/vllm-project/vllm/pull/27705)) by @lgeiger
* [Frontend] [gpt-oss] Mcp type bug ([#27689](https://github.com/vllm-project/vllm/pull/27689)) by @alecsolder
* [Frontend] [gpt-oss] Tool json call parsing error retry ([#27675](https://github.com/vllm-project/vllm/pull/27675)) by @alecsolder
* [Bugfix][CI] Fix config resolving logic with remote models ([#27610](https://github.com/vllm-project/vllm/pull/27610)) by @ywang96
* [Model] Siglip2 Model Support ([#27566](https://github.com/vllm-project/vllm/pull/27566)) by @piood
* [Bugfix] Limit the default value of `max_model_len` when it is not specified by users ([#27556](https://github.com/vllm-project/vllm/pull/27556)) by @shen-shanshan
* [Model] Use merge_by_field_config for MM models (Qwen series) ([#27546](https://github.com/vllm-project/vllm/pull/27546)) by @DarkLight1337
* [Model][MiniMax-M2] Support MiniMax-M2 Model ([#27535](https://github.com/vllm-project/vllm/pull/27535)) by @rogeryoungh
* [Bugfix] Fix processor initialization for model from modelscope instead of HF ([#27461](https://github.com/vllm-project/vllm/pull/27461)) by @lengrongfu
* [Bugfix] In LongRoPE, decide short vs long based on max_model_len ([#27431](https://github.com/vllm-project/vllm/pull/27431)) by @MatthewBonanni
* [Hybrid] Added supports_mamba_prefix_caching Protocol ([#27339](https://github.com/vllm-project/vllm/pull/27339)) by @Josephasafg
* [Hardware][AMD][Model] Triton MoE tuning configs for GLM-4.6 for MI300X ([#27323](https://github.com/vllm-project/vllm/pull/27323)) by @minatoaquaMK2
* [Model][Bugfix] fix ernie45 moe 300B SharedFusedMoE output tuple ([#27316](https://github.com/vllm-project/vllm/pull/27316)) by @CSWYF3634076
* [Hybrid] Add mamba_block_size to Engine Args ([#27289](https://github.com/vllm-project/vllm/pull/27289)) by @Josephasafg
* [BUGFIX][ROCM] ViT FlashAttention on ROCm (no GFX9) and contiguous on qwen3vl ROCm TORCH_SDPA ([#27190](https://github.com/vllm-project/vllm/pull/27190)) by @JartX
* [CI/Build]Add eval config for Qwen3-235B-A22B-Instruct-2507-FP8 ([#27113](https://github.com/vllm-project/vllm/pull/27113)) by @hl475
* [gpt-oss][2/N] Support input_messages in responsesRequest ([#26962](https://github.com/vllm-project/vllm/pull/26962)) by @qandrew
* [VLM] Add Qwen3-VL generation test ([#25185](https://github.com/vllm-project/vllm/pull/25185)) by @Isotr0py
* [XPU][bugfix] fix rope for llama4 and deepseek ([#25145](https://github.com/vllm-project/vllm/pull/25145)) by @yma11
* [Model] Use the same fused_moe configs for all H200 devices ([#23642](https://github.com/vllm-project/vllm/pull/23642)) by @bufferoverflow

## Hardware & Backend

* [Bugfix][CPU] Fix MRoPE dispatch on the CPU backend ([#27800](https://github.com/vllm-project/vllm/pull/27800)) by @bigPYJ1151
* [XPU] Update latest IPEX 2.8 release ([#27735](https://github.com/vllm-project/vllm/pull/27735)) by @jikunshang
* [Bugfix] Fix modular kernel tests ([#27707](https://github.com/vllm-project/vllm/pull/27707)) by @bnellnm
* [Build] Revert triton_kernels requirements ([#27659](https://github.com/vllm-project/vllm/pull/27659)) by @varun-sundar-rabindranath
* [Misc] Make `LayerBlockType` a `Literal` instead of `Enum` ([#27658](https://github.com/vllm-project/vllm/pull/27658)) by @DarkLight1337
* [NIXL][XPU] update name of nixl wheel ([#27631](https://github.com/vllm-project/vllm/pull/27631)) by @zhenwei-intel
* [ROCm][Platform] Add MI308X device id in _ROCM_DEVICE_ID_NAME_MAP ([#27623](https://github.com/vllm-project/vllm/pull/27623)) by @sammysun0711
* [ROCm] Update AITER branch for ROCm base docker ([#27586](https://github.com/vllm-project/vllm/pull/27586)) by @micah-wil
* Revert "[CI/Build] Use CPU for mm processing test on CI (#27522)" ([#27531](https://github.com/vllm-project/vllm/pull/27531)) by @DarkLight1337
* [Bugfix][CPU] Fallback oneDNN linear to torch linear to fix half gemm support on legecy platforms ([#27526](https://github.com/vllm-project/vllm/pull/27526)) by @bigPYJ1151
* [CI/Build] Use CPU for mm processing test on CI ([#27522](https://github.com/vllm-project/vllm/pull/27522)) by @Isotr0py
* [cpu][fix] Fix onednn_mm crash on consecutive matmuls with same M,K,N and different dtype ([#27472](https://github.com/vllm-project/vllm/pull/27472)) by @fadara01
* [Kernel] Enable moe LoRA kernel support FP16 ([#27468](https://github.com/vllm-project/vllm/pull/27468)) by @jeejeelee
* [Bugfix][CI] Move resolving cudagraph_mode before initializing attn_metadata_builder ([#27427](https://github.com/vllm-project/vllm/pull/27427)) by @fhl2000
* [Misc] Add TPU usage report when using tpu_inference. ([#27423](https://github.com/vllm-project/vllm/pull/27423)) by @hfan
* [CI] Add tests for cudagraph ([#27391](https://github.com/vllm-project/vllm/pull/27391)) by @ZJY0516
* [ROCm] [Doc] Update ROCm installation docs  ([#27327](https://github.com/vllm-project/vllm/pull/27327)) by @vllmellm
* [Kernel] Adding split_K implementation for fused_moe_lora ([#27291](https://github.com/vllm-project/vllm/pull/27291)) by @dcmaddix
* [Feat] Adds runai distributed streamer ([#27230](https://github.com/vllm-project/vllm/pull/27230)) by @bbartels
* kernels/moe test pruning ([#27053](https://github.com/vllm-project/vllm/pull/27053)) by @kfhfar
* [Core] Scheduler: Publish connector events after output ([#25875](https://github.com/vllm-project/vllm/pull/25875)) by @orozery

## Refactoring & Core

* [MTP] Refactor mtp predictor to avoid d2h operation ([#27643](https://github.com/vllm-project/vllm/pull/27643)) by @MengqingCao
* [Misc] Simplify max tokens in multimodal registry ([#27500](https://github.com/vllm-project/vllm/pull/27500)) by @DarkLight1337
* [CI/Build] Refactor processing tests ([#27470](https://github.com/vllm-project/vllm/pull/27470)) by @DarkLight1337
* [Refactor] move tool parsing logic from protocol.py to the tool parser ([#27383](https://github.com/vllm-project/vllm/pull/27383)) by @chaunceyjiang
* [NIXL][BUGFIX] delay done_recving queue cleanup to bottom of get_finished ([#27297](https://github.com/vllm-project/vllm/pull/27297)) by @xuechendi
* [Bugfix] Fix Pydantic union resolution for ResponseFunctionToolCall in Responses API ([#26706](https://github.com/vllm-project/vllm/pull/26706)) by @strinczer
* [Frontend][Doc][5/N] Improve all pooling task | Polish encode (pooling) api & Document. ([#25524](https://github.com/vllm-project/vllm/pull/25524)) by @noooop
* [EP/DP][API Server] Enable DP-aware routing in OpenAI API requests ([#24945](https://github.com/vllm-project/vllm/pull/24945)) by @Prowindy

## Build, CI & Testing

* [CI Failure] fix test_default_mm_loras ([#27795](https://github.com/vllm-project/vllm/pull/27795)) by @hl475
* [CI Test] Add Scheduled Integration Test ([#27765](https://github.com/vllm-project/vllm/pull/27765)) by @yewentao256
* [CI] Fix flaky `test_two_responses_with_same_prev_id` test  ([#27745](https://github.com/vllm-project/vllm/pull/27745)) by @NickLucche
* [CI/Build] Move pre-commit only scripts to `tools/pre_commit` ([#27657](https://github.com/vllm-project/vllm/pull/27657)) by @DarkLight1337
* [CI/Build] Test torchrun with 8 cards ([#27548](https://github.com/vllm-project/vllm/pull/27548)) by @22quinn
* [CI/Build] Update causal-conv1d installation ([#27529](https://github.com/vllm-project/vllm/pull/27529)) by @DarkLight1337
* [CI] Fix mypy for `vllm/v1/core` and `vllm/v1/engine` ([#27108](https://github.com/vllm-project/vllm/pull/27108)) by @yewentao256
* [Bugfix][CI] Fix v1 attention backend tests and add CI coverage ([#26597](https://github.com/vllm-project/vllm/pull/26597)) by @mmangkad
* [Chore]: Stream tokens vs characters in tool call parser tests ([#26513](https://github.com/vllm-project/vllm/pull/26513)) by @bbrowning

## Documentation

* Code quality improvements: version update, type annotation enhancement, and enum usage simplification ([#27581](https://github.com/vllm-project/vllm/pull/27581)) by @usberkeley
* [Doc] Slight improvement to M2 and beyond ([#27554](https://github.com/vllm-project/vllm/pull/27554)) by @jeejeelee
* [Docs] reemove the incorrect `enable_reasoning` parameter  ([#27550](https://github.com/vllm-project/vllm/pull/27550)) by @yyzxw
* [Docs] add Shanghai Meetup - 2025/10 ([#27545](https://github.com/vllm-project/vllm/pull/27545)) by @kebe7jun
* [Doc] Fix links to GH projects ([#27530](https://github.com/vllm-project/vllm/pull/27530)) by @DarkLight1337
* [Document] Add ms-swift library to rlhf.md ([#27469](https://github.com/vllm-project/vllm/pull/27469)) by @hjh0119
* [Doc] Fix minor issues in docs/design/metrics.md ([#27436](https://github.com/vllm-project/vllm/pull/27436)) by @draftbk
* use stringData in secret yaml to store huggingface token ([#25685](https://github.com/vllm-project/vllm/pull/25685)) by @yitingdc

## Miscellaneous

* [Misc] Make all tool scripts executable ([#27831](https://github.com/vllm-project/vllm/pull/27831)) by @MatthewBonanni
* [Fix] Skip `record_sleep_state` logic in `PrometheusStatsLogger` if not in dev mode ([#27789](https://github.com/vllm-project/vllm/pull/27789)) by @SumanthRH
* Reapply "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" ([#27768](https://github.com/vllm-project/vllm/pull/27768)) by @huydhn
* [BugFix] Reordering extend logic fix ([#27739](https://github.com/vllm-project/vllm/pull/27739)) by @LucasWilkinson
* Revert "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" ([#27714](https://github.com/vllm-project/vllm/pull/27714)) by @simon-mo
* `use_aot_compile` should respect `VLLM_DISABLE_COMPILE_CACHE` ([#27698](https://github.com/vllm-project/vllm/pull/27698)) by @BoyuanFeng
* [Bug] Fix DeepEP low latency `assert self.batched_router_logits.size(-1) == full_router_logits.size(-1)` Bug ([#27682](https://github.com/vllm-project/vllm/pull/27682)) by @yewentao256
* [KV cache] Fix lmcache connector ([#27681](https://github.com/vllm-project/vllm/pull/27681)) by @Shaoting-Feng
* [Bug] Fix deepep low latency use nvlink by default ([#27677](https://github.com/vllm-project/vllm/pull/27677)) by @yewentao256
* [Fix] import get_kv_cache_torch_dtype error in vllm_v1_adapter.py ([#27670](https://github.com/vllm-project/vllm/pull/27670)) by @KevinCheung2259
* [Bug] Fix DBO IMA issue for DeepEPHT ([#27666](https://github.com/vllm-project/vllm/pull/27666)) by @yewentao256
* [Misc] Raise error for missing video metadata in `MultiModalDataParser` ([#27664](https://github.com/vllm-project/vllm/pull/27664)) by @Isotr0py
* [FLA] Introduce Kimi Delta Attention(KDA) to VLLM ([#27654](https://github.com/vllm-project/vllm/pull/27654)) by @zhiyuan1i
* [Core][Bookkeeping] Update cu_num_accepted_tokens for all req_index ([#27629](https://github.com/vllm-project/vllm/pull/27629)) by @Jialin
* [AsyncScheduling] Make async overlap work with logprobs ([#27615](https://github.com/vllm-project/vllm/pull/27615)) by @njhill
* [nit]: lmcache integration import ([#27600](https://github.com/vllm-project/vllm/pull/27600)) by @sammshen
* Install pre-built xformers-0.0.32.post2 built with pt-2.9.0 ([#27598](https://github.com/vllm-project/vllm/pull/27598)) by @huydhn
* [Bugfix][Frontend] validate arg priority in frontend LLM class before add request ([#27596](https://github.com/vllm-project/vllm/pull/27596)) by @junpuf
* [Stability fix] turn off HMA allocator when connector is set ([#27592](https://github.com/vllm-project/vllm/pull/27592)) by @KuntaiDu
* [Bug] Fix shape issue for eplb expert weights ([#27589](https://github.com/vllm-project/vllm/pull/27589)) by @yewentao256
* [Misc] Separate out `utils.counter` and move `utils.Device` to engine ([#27588](https://github.com/vllm-project/vllm/pull/27588)) by @DarkLight1337
* [Bugfix] Fixed when return_token_ids=False, the first event still contains prompt_token_ids. ([#27561](https://github.com/vllm-project/vllm/pull/27561)) by @chaunceyjiang
* [Bugfix] fixed inconsistent finish_reason handling between V0 and V1 engines ([#27555](https://github.com/vllm-project/vllm/pull/27555)) by @chaunceyjiang
* [Bugfix][LoRA][FusedMoE] Select MxFP4 Backend based on LoRA Enablement ([#27487](https://github.com/vllm-project/vllm/pull/27487)) by @varun-sundar-rabindranath
* [Bugfix] Fix interns1-vit qk norm code path ([#27480](https://github.com/vllm-project/vllm/pull/27480)) by @Isotr0py
* [compile] Turn standalone_compile back on ([#27460](https://github.com/vllm-project/vllm/pull/27460)) by @zou3519
* [Misc] Avoid "PyTorch non-writable tensors" warning in RayPPCommunicator ([#27443](https://github.com/vllm-project/vllm/pull/27443)) by @ruisearch42
* [MM][Bugfix] Replace `PatchEmbed`'s conv3d to linear layer ([#27418](https://github.com/vllm-project/vllm/pull/27418)) by @Isotr0py
* [BugFix] Fix torchrun DP with LLM class ([#27395](https://github.com/vllm-project/vllm/pull/27395)) by @22quinn
* [Core] Enable async scheduling for external_launcher mode ([#27394](https://github.com/vllm-project/vllm/pull/27394)) by @22quinn
* [Misc] Make reorder batch also separate extends ([#27367](https://github.com/vllm-project/vllm/pull/27367)) by @LucasWilkinson
* [Bugfix] Fix MultiConnector stats reconstruction across process boundaries ([#27366](https://github.com/vllm-project/vllm/pull/27366)) by @kouroshHakha
* [compile] Add fallback path to AOT compile when serialization fails. ([#27350](https://github.com/vllm-project/vllm/pull/27350)) by @zhxchen17
* [Distributed] Basic set of configuration for large EP deployment on GB200 ([#27328](https://github.com/vllm-project/vllm/pull/27328)) by @wpc
* [compile] Disable dynamo guards check for AOT compilation. ([#27288](https://github.com/vllm-project/vllm/pull/27288)) by @zhxchen17
* [compile] Add enable_prompt_embeds to compile hash. ([#27285](https://github.com/vllm-project/vllm/pull/27285)) by @zhxchen17
* [Bugfix] Fix allocation & free logic of SingleWriterShmRingBuffer ([#27117](https://github.com/vllm-project/vllm/pull/27117)) by @imkero
* Granite 4.0 quark quantization support ([#26944](https://github.com/vllm-project/vllm/pull/26944)) by @xiao-llm
* [KVConnector] Add metrics to Prometheus-Grafana dashboard ([#26811](https://github.com/vllm-project/vllm/pull/26811)) by @NickLucche
* [Attention] Add MLA prefill backend: trtllm_ragged_attention_deepseek ([#26397](https://github.com/vllm-project/vllm/pull/26397)) by @minosfuture
* [Core][Hybrid allocator + kv connector 1/n] Enable hybrid allocator + KV cache connector ([#25712](https://github.com/vllm-project/vllm/pull/25712)) by @KuntaiDu
* [KVConnector] Migrate the LMCache integration code to be vLLM native ([#25542](https://github.com/vllm-project/vllm/pull/25542)) by @ApostaC
* [Core] Exposing engine sleep & wake_up state as prometheus metrics ([#24176](https://github.com/vllm-project/vllm/pull/24176)) by @dumb0002

## Breaking Changes

* [V0 deprecation] Remove VLLM_USE_V1 usage in config module ([#27784](https://github.com/vllm-project/vllm/pull/27784)) by @wangxiyuan
* [KV offload] Enable CPU KV offload on CUDA alike Platforms ([#27770](https://github.com/vllm-project/vllm/pull/27770)) by @zhewenl
* [Refactor] Remove `VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK` ([#27750](https://github.com/vllm-project/vllm/pull/27750)) by @yewentao256
* [FIXBUG] Qwen3VL hallucinations without Contiguous on Torch.SDPA ([#27744](https://github.com/vllm-project/vllm/pull/27744)) by @JartX
* [chore] Remove models weight on S3 logic ([#27725](https://github.com/vllm-project/vllm/pull/27725)) by @khluu
* [CI/Build] Skip cpu offloading test on AMD ([#27690](https://github.com/vllm-project/vllm/pull/27690)) by @zhewenl
* [Core] Early return in SlidingWindowManager.remove_skipped_blocks ([#27673](https://github.com/vllm-project/vllm/pull/27673)) by @Jialin
* [Feature] Batch invariant torch.compile ([#27660](https://github.com/vllm-project/vllm/pull/27660)) by @PaulZhang12
* Fix MiniMax-M2 rmsnorm precision and remove useless code ([#27627](https://github.com/vllm-project/vllm/pull/27627)) by @rogeryoungh
* [CI/Build] Fix amd model executor test ([#27612](https://github.com/vllm-project/vllm/pull/27612)) by @zhewenl
* [Bugfix] Fix non-contiguous tensor error in `rocm_unquantized_gemm_impl` ([#27605](https://github.com/vllm-project/vllm/pull/27605)) by @zhewenl
* [V0 Deprecation] Remove vestigial V0 logits_processors.py file ([#27601](https://github.com/vllm-project/vllm/pull/27601)) by @njhill
* [Misc] Clean up more utils ([#27567](https://github.com/vllm-project/vllm/pull/27567)) by @DarkLight1337
* Fix a robust parsing issue in KimiK2ToolParser that causes IndexError ([#27565](https://github.com/vllm-project/vllm/pull/27565)) by @wangln19
* [Misc] Replace CUDA_VISIBLE_DEVICES in DP with torch.cuda.set_device for device selection on cuda-like devices ([#27564](https://github.com/vllm-project/vllm/pull/27564)) by @ilmarkov
* [Misc] Clean up utils ([#27552](https://github.com/vllm-project/vllm/pull/27552)) by @DarkLight1337
* [Model] Deprecate `merge_by_field_config=False` ([#27551](https://github.com/vllm-project/vllm/pull/27551)) by @DarkLight1337
* [Doc] Remove Molmo warning ([#27527](https://github.com/vllm-project/vllm/pull/27527)) by @DarkLight1337
* Revert "[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selectio… ([#27502](https://github.com/vllm-project/vllm/pull/27502)) by @zhuohan123
* [Attention] Add missing kv cache scale setup ([#27490](https://github.com/vllm-project/vllm/pull/27490)) by @MatthewBonanni
* [Misc][DP] Guard mxfp4 implementation selection ([#27484](https://github.com/vllm-project/vllm/pull/27484)) by @varun-sundar-rabindranath
* [Test] Batch Invariant: Unit test using parameterized backend ([#27478](https://github.com/vllm-project/vllm/pull/27478)) by @yewentao256
* [Perf][Async Scheduling] Remove CPU->GPU sync in dummy_run ([#27455](https://github.com/vllm-project/vllm/pull/27455)) by @lhtin
* [Chore] remove structural tags logging lines ([#27451](https://github.com/vllm-project/vllm/pull/27451)) by @aarnphm
* [Docs] remove v1 column for embedding models ([#27446](https://github.com/vllm-project/vllm/pull/27446)) by @piood
* [Bug] Raise error explicitly if using incompatible backend ([#27424](https://github.com/vllm-project/vllm/pull/27424)) by @yewentao256
* Fix pooling adapters for Transformers backend  ([#27338](https://github.com/vllm-project/vllm/pull/27338)) by @hmellor
* [CI/Build] Fix test_torch_utils in AMD CI ([#27317](https://github.com/vllm-project/vllm/pull/27317)) by @zhewenl
* [Speculators] Move tests + fix integration ([#27308](https://github.com/vllm-project/vllm/pull/27308)) by @dsikka
* [BugFix] Also consider RAY_EXPERIMENTAL_NOSET_* when storing compilation cache ([#27294](https://github.com/vllm-project/vllm/pull/27294)) by @HollowMan6
* [Kernel] Add GPTQv2 format support for low-bit or asymmetric quantization, by adapting gptq_gemm ([#26092](https://github.com/vllm-project/vllm/pull/26092)) by @xxxxyu
* [MISC] `cudagraph_capture_sizes`  related improvements ([#26016](https://github.com/vllm-project/vllm/pull/26016)) by @fhl2000
* [Benchmark] Cleanup deprecated nightly benchmark and adjust the docstring for performance benchmark ([#25786](https://github.com/vllm-project/vllm/pull/25786)) by @KuntaiDu
* [Bugfix] Improve GPU validation logging in Ray fallback scenarios ([#25775](https://github.com/vllm-project/vllm/pull/25775)) by @sairampillai

## Upgrade Notes

- Note: I didn't test Qwen3OmniMoE since I didn't have a large enough GPU on hand today, but it's the same code changes as for Qwen3VL so I believe it should be fine.
- - Avoid breaking GPU -> NIC mapping which is required for deepep performance
- > Note: this change is safe since it only affect the usage on Ascend NPU devices.
- https://github.com/vllm-project/vllm/pull/26709 is breaking external launcher mode DP. We will add more tests to make sure it is catched in CI. Also in general I am confused on why this PR is written in the way that we need to hack the local dp rank when initializing devices.
- #25103 missed the kv cache scale setup when breaking out `MLAAttention`. This PR adds this to `MLAAttention.__init__`
- Note:
- **Note:** `VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE` requires a special DeepEP fork that supports MNNVL for intra-node kernel. The fork is available at: https://github.com/deepseek-ai/DeepEP/compare/main...fzyzcjy:DeepEP:feat/dev_20250914
- The speculator integration allows users to run speculative decoding without explicitly providing a `--speculative-config` by automatically detecting speculator models and extracting the configuration. This integration kept breaking on main because:

## Contributors

@22quinn, @ApostaC, @BloodAxe, @BoyuanFeng, @CSWYF3634076, @DarkLight1337, @FlamingoPg, @HollowMan6, @Isotr0py, @JartX, @Jialin, @Josephasafg, @KevinCheung2259, @KuntaiDu, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @MengqingCao, @NickLucche, @PaulZhang12, @Prowindy, @Shaoting-Feng, @SumanthRH, @ZJY0516, @aarnphm, @alecsolder, @bbartels, @bbrowning, @bigPYJ1151, @bnellnm, @bufferoverflow, @bwasti, @chaunceyjiang, @dcmaddix, @draftbk, @dsikka, @dumb0002, @eicherseiji, @fadara01, @fhl2000, @gnovack, @heheda12345, @hfan, @hjh0119, @hl475, @hmellor, @huachenheli, @huydhn, @ilmarkov, @imkero, @ioana-ghiban-arm, @jakub-sochacki, @jeejeelee, @jikunshang, @junpuf, @kebe7jun, @kfhfar, @khluu, @kouroshHakha, @lengrongfu, @lgeiger, @lhtin, @luccafong, @micah-wil, @minatoaquaMK2, @minosfuture, @mmangkad, @mpashkovskii, @njhill, @noooop, @orozery, @piood, @qandrew, @rogeryoungh, @ruisearch42, @sairampillai, @sammshen, @sammysun0711, @shen-shanshan, @simon-mo, @strinczer, @tingtingtang1992, @tlrmchlsmth, @usberkeley, @varun-sundar-rabindranath, @vllmellm, @wangln19, @wangxiyuan, @wpc, @xiao-llm, @xuechendi, @xxxxyu, @yeshsurya, @yewentao256, @yitingdc, @yma11, @youkaichao, @ywang96, @yyzxw, @zhenwei-intel, @zhewenl, @zhiyuan1i, @zhuohan123, @zhxchen17, @zou3519