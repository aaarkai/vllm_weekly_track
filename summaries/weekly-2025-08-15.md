# Weekly Release Notes for vllm-project/vllm (2025-08-15)

## What's Changed

### ‚ú® Features & Enhancements

* [FEATURE] support custom vllm tuned config path ([#22791](https://github.com/vllm-project/vllm/pull/22791)) by @vermouth1992
* [Feature] Full Cuda Graph Support for Cutlass MLA and 6% E2E Throughput Improvement ([#22763](https://github.com/vllm-project/vllm/pull/22763)) by @yewentao256
* Add more test scenario for tensor schema ([#22733](https://github.com/vllm-project/vllm/pull/22733)) by @teekenl
* Add hardware plugins to installation doc ([#22732](https://github.com/vllm-project/vllm/pull/22732)) by @mgoin
* Support more parallel styles in Transformers backend TP ([#22651](https://github.com/vllm-project/vllm/pull/22651)) by @hmellor
* [FEAT] [Performance] Add triton mrope to replace the torch code path ([#22375](https://github.com/vllm-project/vllm/pull/22375)) by @tjtanaa
* Support token_type_ids in V1 with less code changes ([#21985](https://github.com/vllm-project/vllm/pull/21985)) by @maxdebayser
* [Feature] Add `VLLM_USE_DEEP_GEMM_E8M0` Env to Control E8M0 Scale ([#21968](https://github.com/vllm-project/vllm/pull/21968)) by @yewentao256
* Support Tensorrt-LLM MoE fp4 for low-latency ([#21331](https://github.com/vllm-project/vllm/pull/21331)) by @wenscarl
* Add PrefixRepetitionRandomDataset to `vllm bench serve` datasets ([#20638](https://github.com/vllm-project/vllm/pull/20638)) by @eicherseiji
* Add ModelOpt Qwen3 nvfp4 support ([#20101](https://github.com/vllm-project/vllm/pull/20101)) by @Edwardf0t1

### üêõ Bug Fixes

* [BugFix] Fix regression caused by mamba state dtype PR ([#22998](https://github.com/vllm-project/vllm/pull/22998)) by @tdoublep
* [Fix] enable swap_ab for pplx problem size computation ([#22991](https://github.com/vllm-project/vllm/pull/22991)) by @shixianc
* [Bugfix] fix cuda 12.6 and 11.8 build ([#22952](https://github.com/vllm-project/vllm/pull/22952)) by @jinzhen-lin
* [Bugfix] use flash attn on sm90 ([#22933](https://github.com/vllm-project/vllm/pull/22933)) by @zyongye
* [Bugfix] Unquote file uri before reading image ([#22912](https://github.com/vllm-project/vllm/pull/22912)) by @sayandipdutta
* [BugFix] Fix initial DP request load imbalance ([#22910](https://github.com/vllm-project/vllm/pull/22910)) by @njhill
* [Bugfix] Fix parsing of `--disable-mm-preprocessor-cache` ([#22909](https://github.com/vllm-project/vllm/pull/22909)) by @DarkLight1337
* [BugFix] Threadsafe close async zmq sockets ([#22877](https://github.com/vllm-project/vllm/pull/22877)) by @njhill
* [Bugfix] Fix `PixtralHFImagePixelInputs` dynamic shape check ([#22827](https://github.com/vllm-project/vllm/pull/22827)) by @Isotr0py
* [Bugfix] Fix MiniCPMV Image input inference failed ([#22813](https://github.com/vllm-project/vllm/pull/22813)) by @jio-H
* [Bugfix][mamba] Fix type annotation of Mamba2Metadata ([#22787](https://github.com/vllm-project/vllm/pull/22787)) by @heheda12345
* [Bugfix] Replace custom Encoding class with BatchEncoding in MistralTokenizer ([#22786](https://github.com/vllm-project/vllm/pull/22786)) by @ZJY0516
* Fix GGUF loader for Qwen3 MoE. ([#22785](https://github.com/vllm-project/vllm/pull/22785)) by @Gh0u1L5
* [Bug] Fix Unexpected Keyword Argument 'w1_bias' ([#22757](https://github.com/vllm-project/vllm/pull/22757)) by @yewentao256
* [Bugfix] Fix Nemotron VL image processing ([#22739](https://github.com/vllm-project/vllm/pull/22739)) by @ducviet00
* [Bugfix] Fix default enable for CUTLASS MLA on SM100 ([#22738](https://github.com/vllm-project/vllm/pull/22738)) by @mgoin
* [BugFix][KVConn] Fix use of `get_required_kvcache_layout` ([#22734](https://github.com/vllm-project/vllm/pull/22734)) by @njhill
* [Bugfix][CI] Fix `test_remote_decode_lifecycle.py::test_short_prompt_lifecycle` ([#22727](https://github.com/vllm-project/vllm/pull/22727)) by @NickLucche
* [Bugfix] Add reset prefix cache for online serving ([#22726](https://github.com/vllm-project/vllm/pull/22726)) by @iAmir97
* Fix cuda illegal mem access with Llama4 TP8 + rms_norm custom op ([#22701](https://github.com/vllm-project/vllm/pull/22701)) by @nvpohanh
* Fix Transformers backend tensor parallel for multimodal models ([#22673](https://github.com/vllm-project/vllm/pull/22673)) by @hmellor
* [BugFix][Nixl][PD] Fix heterogenous TP ([#22663](https://github.com/vllm-project/vllm/pull/22663)) by @NickLucche
* Fix passing `SpeculativeConfig` from the CLI ([#22652](https://github.com/vllm-project/vllm/pull/22652)) by @hmellor
* [Bugfix] Fix ModernBert load & Enable sliding window attention for bidirectional attention. ([#22637](https://github.com/vllm-project/vllm/pull/22637)) by @noooop
* [BugFix] [Spec Decode] Remove LlamaForCausalLMEagle3 to fix CI ([#22611](https://github.com/vllm-project/vllm/pull/22611)) by @22quinn
* [Bugfix] Bump DeepGEMM Version to Fix SMXX Layout Issues ([#22606](https://github.com/vllm-project/vllm/pull/22606)) by @frankwang28
* [BugFix] Fix KVConnectorOutput TPU breakage ([#22598](https://github.com/vllm-project/vllm/pull/22598)) by @njhill
* [Bugfix][Kernel] Support partial rotary embedding for MRoPE triton kernel ([#22593](https://github.com/vllm-project/vllm/pull/22593)) by @Isotr0py
* [BugFix] Fix logits repetition penalty cuda check ([#22592](https://github.com/vllm-project/vllm/pull/22592)) by @PicoCreator
* [Bugfix] Fix basic models tests hanging due to mm processor creation ([#22571](https://github.com/vllm-project/vllm/pull/22571)) by @Isotr0py
* [Bugfix] Fix failing GPT-OSS initialization test ([#22557](https://github.com/vllm-project/vllm/pull/22557)) by @Isotr0py
* [Bugfix] Fix CI moe kernel failure ([#22556](https://github.com/vllm-project/vllm/pull/22556)) by @jeejeelee
* [Bugfix] Update FA commit hash ([#22546](https://github.com/vllm-project/vllm/pull/22546)) by @tdoublep
* Fix torch version check for SM100 mxfp4  ([#22535](https://github.com/vllm-project/vllm/pull/22535)) by @zifeitong
* Fix Llama4 FlashInfer FP4 MoE issues ([#22511](https://github.com/vllm-project/vllm/pull/22511)) by @nvpohanh
* Fix pre-commit ([#22487](https://github.com/vllm-project/vllm/pull/22487)) by @DarkLight1337
* [BugFix] Don't cancel asyncio tasks directly from destructors ([#22476](https://github.com/vllm-project/vllm/pull/22476)) by @njhill
* Fix loading of quantized BigCode models ([#22463](https://github.com/vllm-project/vllm/pull/22463)) by @eldarkurtic
* [Bugfix] Added more env vars to hash ([#22449](https://github.com/vllm-project/vllm/pull/22449)) by @nvjullin
* [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 ([#22426](https://github.com/vllm-project/vllm/pull/22426)) by @nvpohanh
* [BugFix] Skip the Q component for QKVParallelLinear in the case of QKVCrossParallelLinear since its width is 0 ([#22369](https://github.com/vllm-project/vllm/pull/22369)) by @sstamenk
* Fix TensorSchema validation test for symbolic dims ([#22366](https://github.com/vllm-project/vllm/pull/22366)) by @bbeckca
* [BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D ([#22317](https://github.com/vllm-project/vllm/pull/22317)) by @Pradyun92
* [BugFix] Fix port lookup in internal DP LB tests ([#22252](https://github.com/vllm-project/vllm/pull/22252)) by @njhill
* [Bugfix] Fix erroneous randomly generated cases in bad word testing ([#22170](https://github.com/vllm-project/vllm/pull/22170)) by @phantomlei3
* [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match ([#22065](https://github.com/vllm-project/vllm/pull/22065)) by @chaunceyjiang
* [BUGFIX] KeyError 'layers.14.mlp.gate.g_idx' for Qwen3-MoE with GPTQ on ROCm ([#22017](https://github.com/vllm-project/vllm/pull/22017)) by @JartX
* Fix Flashinfer CUTLASS MOE Allgather ([#21963](https://github.com/vllm-project/vllm/pull/21963)) by @wenscarl
* [Bugfix] Fix ModernBert cuda graph capturing in v1 ([#21901](https://github.com/vllm-project/vllm/pull/21901)) by @Isotr0py
* Fix: AWQ Marlin get_quant_method does not recognize "modules_to_not_convert" ([#21888](https://github.com/vllm-project/vllm/pull/21888)) by @Jun-Howie
* [Bugfix] Mamba2 SSD varlen bug fix initstates decay, improve test, assert chunk pwr 2 ([#21783](https://github.com/vllm-project/vllm/pull/21783)) by @RishiAstra
* [BugFix] Fix IMA FlashMLA full cuda-graph and DP + Update FlashMLA ([#21691](https://github.com/vllm-project/vllm/pull/21691)) by @LucasWilkinson
* fix: NIXL connector transfers partial block to pass full multi-modal context ([#21074](https://github.com/vllm-project/vllm/pull/21074)) by @GuanLuo

### ‚ö°Ô∏è Performance

* [Perf] Dont create unnecessary pooling params ([#22876](https://github.com/vllm-project/vllm/pull/22876)) by @LucasWilkinson
* [PERF] Use pybase64 to more quickly decode prompt embeddings ([#22469](https://github.com/vllm-project/vllm/pull/22469)) by @qthequartermasterman
* Optimize MiniCPMO mask creation with vectorized implementation ([#22464](https://github.com/vllm-project/vllm/pull/22464)) by @skyloevil
* [Perf] Support topk softmax fused kernel for broader num_experts ([#22211](https://github.com/vllm-project/vllm/pull/22211)) by @shixianc

### ü§ñ Model Support

* [Model] Granite-4 support loading quantized checkpoint ([#22925](https://github.com/vllm-project/vllm/pull/22925)) by @cyang49
* [Model] Modify the gate implementation of glm4_moe ([#22832](https://github.com/vllm-project/vllm/pull/22832)) by @jeejeelee
* [gpt-oss] upgrade gpt-oss to v0.0.3 and add version check ([#22768](https://github.com/vllm-project/vllm/pull/22768)) by @heheda12345
* [Model] Decouple glm4v ([#22751](https://github.com/vllm-project/vllm/pull/22751)) by @jeejeelee
* [Model] Add missing prefix to glm4_1v ([#22716](https://github.com/vllm-project/vllm/pull/22716)) by @zRzRzRzRzRzRzR
* [gpt-oss] Enable gpt-oss on ampere ([#22714](https://github.com/vllm-project/vllm/pull/22714)) by @zyongye
* [gpt-oss] Fix mxfp4 support ([#22700](https://github.com/vllm-project/vllm/pull/22700)) by @heheda12345
* [Model] Add option to run Step3VisionEncoder in DP ([#22697](https://github.com/vllm-project/vllm/pull/22697)) by @zzh142857
* [gpt-oss] Add test for response API + harmony (but skipped) ([#22554](https://github.com/vllm-project/vllm/pull/22554)) by @heheda12345
* [gpt-oss] guard import when triton kernel is not installed ([#22529](https://github.com/vllm-project/vllm/pull/22529)) by @zyongye
* GLM-4.5V with new class name at transformers ([#22520](https://github.com/vllm-project/vllm/pull/22520)) by @zRzRzRzRzRzRzR
* [gpt-oss] Small bug fixes for frontend ([#22512](https://github.com/vllm-project/vllm/pull/22512)) by @heheda12345
* [gpt-oss] Support streaming in response API ([#22431](https://github.com/vllm-project/vllm/pull/22431)) by @heheda12345
* [gpt-oss] Support tool call and implement MCP tool server ([#22427](https://github.com/vllm-project/vllm/pull/22427)) by @heheda12345
* [gpt-oss] triton kernel mxfp4 ([#22421](https://github.com/vllm-project/vllm/pull/22421)) by @zyongye
* [Model] NemotronH Support  ([#22349](https://github.com/vllm-project/vllm/pull/22349)) by @danielafrimi
* [Model] Pooling models default to using chunked prefill & prefix caching if supported. ([#20930](https://github.com/vllm-project/vllm/pull/20930)) by @noooop
* [Model] Gemma3n MM ([#20495](https://github.com/vllm-project/vllm/pull/20495)) by @NickLucche

### üîå Hardware & Backend

* [ROCm][Bugfix] Fix compilation error in topk softmax fused kernel ([#22819](https://github.com/vllm-project/vllm/pull/22819)) by @kliuae
* [ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module. ([#22521](https://github.com/vllm-project/vllm/pull/22521)) by @vllmellm
* [TPU] Add support for online w8a8 quantization ([#22425](https://github.com/vllm-project/vllm/pull/22425)) by @kyuyeunk
* [TPU] kv cache update kernel doesn't need to be padded slices to multiple of num_slices_per_block ([#22394](https://github.com/vllm-project/vllm/pull/22394)) by @yaochengji
* [XPU] upgrade torch 2.8 on for XPU ([#22300](https://github.com/vllm-project/vllm/pull/22300)) by @jikunshang
* [ROCm][Misc] Rename the context_len to seq_len in ROCm custom paged attention kernel ([#22097](https://github.com/vllm-project/vllm/pull/22097)) by @charlifu
* [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine ([#21496](https://github.com/vllm-project/vllm/pull/21496)) by @tjtanaa

### ‚öôÔ∏è Refactoring & Core

* [Core] direct indexing on self.block_table_np in compute_slot_mapping ([#22940](https://github.com/vllm-project/vllm/pull/22940)) by @linzebing
* [Frontend] Expose do_log_stats interval to env ([#22905](https://github.com/vllm-project/vllm/pull/22905)) by @Csrayz
* refactor: Change scaling factors calculation for flashinfer FusedMoE ([#22812](https://github.com/vllm-project/vllm/pull/22812)) by @amirkl94
* Remove unnecessary CUDA sync of qwen image and video preprocess ([#22792](https://github.com/vllm-project/vllm/pull/22792)) by @cyyever
* Remove unneeded ROCm platform import when using CUDA ([#22765](https://github.com/vllm-project/vllm/pull/22765)) by @mgoin
* Remove Phi 4 Flash configuration workaround ([#22723](https://github.com/vllm-project/vllm/pull/22723)) by @hmellor
* [Frontend] Multithreaded async multimodal load_bytes ([#22710](https://github.com/vllm-project/vllm/pull/22710)) by @milesial
* [Kernel][AMD] Avoid D2H copy and cumsum kernel ([#22683](https://github.com/vllm-project/vllm/pull/22683)) by @mxz297
* [Core] Use individual MM items in P0/P1 cache and model runner ([#22570](https://github.com/vllm-project/vllm/pull/22570)) by @DarkLight1337
* Remove mamba_ssm from vLLM requirements; install inside test container using `--no-build-isolation` ([#22541](https://github.com/vllm-project/vllm/pull/22541)) by @tdoublep
* [Kernel]  Add cuda kernel for gpt_oss activation ([#22538](https://github.com/vllm-project/vllm/pull/22538)) by @jeejeelee
* Remove redundant row_indices unsqueeze operation in MiniCPMO ([#22528](https://github.com/vllm-project/vllm/pull/22528)) by @skyloevil
* Remove exception for Python 3.8 typing from linter ([#22506](https://github.com/vllm-project/vllm/pull/22506)) by @hmellor
* [Core] [N-gram SD Optimization][1/n] Propose tokens with a single KMP ([#22437](https://github.com/vllm-project/vllm/pull/22437)) by @Jialin
* [Kernel] [Quantization] Add MXFP4 and bias support for marlin kernel ([#22428](https://github.com/vllm-project/vllm/pull/22428)) by @jinzhen-lin
* [Kernel] Add nvfp4 gemm flashinfer backends ([#22346](https://github.com/vllm-project/vllm/pull/22346)) by @nvjullin
* [Core] Return final response for aborted requests from `AsyncLLM.generate` ([#22283](https://github.com/vllm-project/vllm/pull/22283)) by @njhill
* [Frontend] Add chunked processing to handle long inputs in embedding models ([#22280](https://github.com/vllm-project/vllm/pull/22280)) by @x22x22
* [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) ([#22131](https://github.com/vllm-project/vllm/pull/22131)) by @0xjunhao
* Refactor sliding window configuration to Transformers best practice ([#21927](https://github.com/vllm-project/vllm/pull/21927)) by @hmellor
* [Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer ([#20059](https://github.com/vllm-project/vllm/pull/20059)) by @fhl2000
* [Frontend] Add unix domain socket support ([#18097](https://github.com/vllm-project/vllm/pull/18097)) by @yyweiss

### üîß Build, CI & Testing

* [CI] Temporarily disable flaky test  ([#22930](https://github.com/vllm-project/vllm/pull/22930)) by @LucasWilkinson
* [CI] Remove duplicated docs build from buildkite ([#22924](https://github.com/vllm-project/vllm/pull/22924)) by @hmellor
* [CI] [Hybrid]  Bump min transformers version for Bamba and Jamba ([#22908](https://github.com/vllm-project/vllm/pull/22908)) by @tdoublep
* [CI] Re-enable transcriptions `test_long_audio_request` ([#22890](https://github.com/vllm-project/vllm/pull/22890)) by @NickLucche
* [CI] Pooling models mteb test uses enforce_eager ([#22878](https://github.com/vllm-project/vllm/pull/22878)) by @noooop
* [CI] remove flaky v0 test ([#22864](https://github.com/vllm-project/vllm/pull/22864)) by @robertgshaw2-redhat
* [CI] Speed up Whisper tests by reusing server ([#22859](https://github.com/vllm-project/vllm/pull/22859)) by @mgoin
* [CI] Fix `tests/distributed/test_ca_buffer_sharing.py` ([#22849](https://github.com/vllm-project/vllm/pull/22849)) by @ilmarkov
* [CI][Entrypoints]: add filter to generation to filter out invalid tool calls ([#22826](https://github.com/vllm-project/vllm/pull/22826)) by @wseaton
* [CI] Fix `tests/v1/e2e/test_kv_sharing_fast_prefill.py` import on test ([#22815](https://github.com/vllm-project/vllm/pull/22815)) by @NickLucche
* [CI][Nixl] Check kv cache layout during handshake ([#22745](https://github.com/vllm-project/vllm/pull/22745)) by @NickLucche
* [CI] Increase timeout for test_completion_with_image_embeds ([#22670](https://github.com/vllm-project/vllm/pull/22670)) by @mgoin
* [CI] Skip Tree Attn Test in `test_max_len.py` to unblock CI ([#22664](https://github.com/vllm-project/vllm/pull/22664)) by @tjtanaa
* [CI] [Hybrid] Speed up hybrid models test by removing large models  ([#22563](https://github.com/vllm-project/vllm/pull/22563)) by @tdoublep

### üìö Documentation

* [Doc] fix dead link ([#22898](https://github.com/vllm-project/vllm/pull/22898)) by @dtrifiro
* docs: update fastsafetensors usage instructions ([#22891](https://github.com/vllm-project/vllm/pull/22891)) by @NirLevy98
* [Doc] Add max_lora_rank configuration guide ([#22782](https://github.com/vllm-project/vllm/pull/22782)) by @chi2liu
* [Docs] Hide the navigation and toc sidebars on home page ([#22749](https://github.com/vllm-project/vllm/pull/22749)) by @hmellor
* [Docs] Improve docs navigation ([#22720](https://github.com/vllm-project/vllm/pull/22720)) by @hmellor
* [doc] Update x86 CPU-inference installation doc to reflect optionality of AVX512f  ([#22707](https://github.com/vllm-project/vllm/pull/22707)) by @sooraj-satheesh
* [DOC] update v1_guide with INTEL HW ([#22679](https://github.com/vllm-project/vllm/pull/22679)) by @xuechendi
* [Docs] Add comprehensive CLI reference for all large `vllm` subcommands ([#22601](https://github.com/vllm-project/vllm/pull/22601)) by @hmellor
* [doc] add alibaba cloud as sponsor ([#22597](https://github.com/vllm-project/vllm/pull/22597)) by @youkaichao
* [doc] add beijing meetup links ([#22596](https://github.com/vllm-project/vllm/pull/22596)) by @youkaichao
* [Docs] Fix warnings in docs build ([#22588](https://github.com/vllm-project/vllm/pull/22588)) by @hmellor
* [Doc] Fix API doc link in side navigation ([#22585](https://github.com/vllm-project/vllm/pull/22585)) by @22quinn
* [Docs] Reduce noise in docs and `--help` from the JSON tip ([#22567](https://github.com/vllm-project/vllm/pull/22567)) by @hmellor
* [Doc] Add usage of implicit text-only mode  ([#22561](https://github.com/vllm-project/vllm/pull/22561)) by @ywang96
* [Docs] Rename ‚ÄúDistributed inference and serving‚Äù to ‚ÄúParallelism & Scaling‚Äù ([#22466](https://github.com/vllm-project/vllm/pull/22466)) by @crypdick
* [Docs] Improve API docs (+small tweaks) ([#22459](https://github.com/vllm-project/vllm/pull/22459)) by @hmellor
* [Docs] fix broken links in metrics.md ([#22315](https://github.com/vllm-project/vllm/pull/22315)) by @GuyStone
* [Doc] Sleep mode documentation ([#22310](https://github.com/vllm-project/vllm/pull/22310)) by @iAmir97
* [Doc] Added unmentioned required option "method" in the usage of EAGLE-3 based models ([#21737](https://github.com/vllm-project/vllm/pull/21737)) by @hsliuustc0106

### üì¶ Miscellaneous

* Use regex in convert-results-json-to-markdown.py ([#22989](https://github.com/vllm-project/vllm/pull/22989)) by @mgoin
* [V0 Deprecation] Remove advance_step ([#22969](https://github.com/vllm-project/vllm/pull/22969)) by @WoosukKwon
* Revert "[ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module." ([#22956](https://github.com/vllm-project/vllm/pull/22956)) by @tjtanaa
* [Benchmarks] Include image data when ShareGPT4V dataset is used. ([#22955](https://github.com/vllm-project/vllm/pull/22955)) by @huachenheli
* [MM] Allow skipping memory profiling for multimodal models. ([#22950](https://github.com/vllm-project/vllm/pull/22950)) by @ywang96
* Revert "[Kernel]  Add cuda kernel for gpt_oss activation" ([#22948](https://github.com/vllm-project/vllm/pull/22948)) by @simon-mo
* [CI Perf] Prune tests in `tests/kernels/quantization/` ([#22942](https://github.com/vllm-project/vllm/pull/22942)) by @mgoin
* [CI Perf] Prune tests in `tests/kernels/moe/` ([#22939](https://github.com/vllm-project/vllm/pull/22939)) by @mgoin
* [CI Perf] Prune tests in `tests/kernels/attention/` ([#22936](https://github.com/vllm-project/vllm/pull/22936)) by @mgoin
* [V1] [Hybrid] Support using float32 for state in Hybrid Models (Mamba2, Mamba1, Minimax) ([#22928](https://github.com/vllm-project/vllm/pull/22928)) by @tdoublep
* [FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches ([#22896](https://github.com/vllm-project/vllm/pull/22896)) by @JartX
* [Log] Debug Once for Randomizing dummy data for DP Rank ([#22860](https://github.com/vllm-project/vllm/pull/22860)) by @yewentao256
* Move checklist in PR template ([#22852](https://github.com/vllm-project/vllm/pull/22852)) by @ProExpertProg
* [CI/Build] Skip gpt_big model test because of broken HF model ([#22848](https://github.com/vllm-project/vllm/pull/22848)) by @Isotr0py
* [CI/Build] Fix param mismatch in `test_eagle_correctness` ([#22847](https://github.com/vllm-project/vllm/pull/22847)) by @DarkLight1337
* [CI/Build] Increase pooling tolerance to pass CI ([#22844](https://github.com/vllm-project/vllm/pull/22844)) by @DarkLight1337
* [CI/Build] Update VLM common tests ([#22841](https://github.com/vllm-project/vllm/pull/22841)) by @DarkLight1337
* Improve multimodal hasher performance for re-used Image prompts ([#22825](https://github.com/vllm-project/vllm/pull/22825)) by @p88h
* [Mamba] - refactor: Renamed mamba_attn to mamba2_attn ([#22818](https://github.com/vllm-project/vllm/pull/22818)) by @Josephasafg
* [Misc] Ignore ep_kernels_workspace ([#22807](https://github.com/vllm-project/vllm/pull/22807)) by @jeejeelee
* [Nixl][CI] Fix tests ([#22806](https://github.com/vllm-project/vllm/pull/22806)) by @NickLucche
* [Misc] clear and separate error messages for input too long and input + max-tokens too long ([#22803](https://github.com/vllm-project/vllm/pull/22803)) by @ywang96
* [V0 Deprecation] Remove args for multi-step scheduling ([#22779](https://github.com/vllm-project/vllm/pull/22779)) by @WoosukKwon
* [Misc] Remove tests/multi_step/__init__.py ([#22778](https://github.com/vllm-project/vllm/pull/22778)) by @WoosukKwon
* [Chore] Update CODEOWNERS to include @yewentao256 for CUDA kernels, attention backends, quantization, and related tests ([#22741](https://github.com/vllm-project/vllm/pull/22741)) by @yewentao256
* [Benchmark] Fix terminal colors in benchmark_serving_multi_turn (python 3.12) ([#22730](https://github.com/vllm-project/vllm/pull/22730)) by @pliops-daniels
* [Misc] remove GH discussions link ([#22722](https://github.com/vllm-project/vllm/pull/22722)) by @jeejeelee
* [CI Failure] fix tests/entrypoints/openai/test_skip_tokenizer.py ([#22708](https://github.com/vllm-project/vllm/pull/22708)) by @noooop
* [V1] Add tree drafting tests for eagle spec decoding ([#22705](https://github.com/vllm-project/vllm/pull/22705)) by @TheEpicDolphin
* [CI Failure] Use float32 for tests/entrypoints/openai/test_audio.py ([#22686](https://github.com/vllm-project/vllm/pull/22686)) by @mgoin
* Force TRTLLM attention for gpt-oss on SM100 ([#22678](https://github.com/vllm-project/vllm/pull/22678)) by @mgoin
* Re-enable Xet on TPU tests now that `hf_xet` has been updated ([#22666](https://github.com/vllm-project/vllm/pull/22666)) by @hmellor
* Officially support SmolLM3 using the Transformers backend ([#22665](https://github.com/vllm-project/vllm/pull/22665)) by @hmellor
* [New Model] Support Command-A-Vision ([#22660](https://github.com/vllm-project/vllm/pull/22660)) by @dongluw
* [CI/Build] Skip Mllama HF runner tests with Transformers v4.55.0 ([#22659](https://github.com/vllm-project/vllm/pull/22659)) by @Isotr0py
* [V1] - Split Prefill and Decode for Mamba1 models ([#22653](https://github.com/vllm-project/vllm/pull/22653)) by @amirai21
* [Misc] Further clean up some redundant config definitions ([#22649](https://github.com/vllm-project/vllm/pull/22649)) by @Isotr0py
* Document aarch64 CPU support works ([#22646](https://github.com/vllm-project/vllm/pull/22646)) by @ericcurtin
* [P/D]Provide bucket algorithm rate limiter  for proxy_server ([#22643](https://github.com/vllm-project/vllm/pull/22643)) by @frankie-ys
* Add: `SupportsEagle3` interface for explicit EAGLE3 support ([#22642](https://github.com/vllm-project/vllm/pull/22642)) by @rahul-tuli
* [Misc] parametrize 'dtype' in test_flash_mla ([#22641](https://github.com/vllm-project/vllm/pull/22641)) by @RUTHLESS-BOT
* [V0] Correct CUDA Graph capture for encoder-decoder models ([#22630](https://github.com/vllm-project/vllm/pull/22630)) by @Sugar-zsg
* Move `SchedulerConfig` from `config/__init__.py` to `config/scheduler.py` ([#22626](https://github.com/vllm-project/vllm/pull/22626)) by @hmellor
* [Misc] Move jsontree to utils ([#22622](https://github.com/vllm-project/vllm/pull/22622)) by @DarkLight1337
* Upgrade FlashInfer to v0.2.11 ([#22613](https://github.com/vllm-project/vllm/pull/22613)) by @nvpohanh
* [Misc] Move tensor schema tests ([#22612](https://github.com/vllm-project/vllm/pull/22612)) by @DarkLight1337
* minor: zero workspace buffer init for flashinfer trtllm-gen attn ([#22603](https://github.com/vllm-project/vllm/pull/22603)) by @yyihuang
* [Misc][gpt-oss] Add rules to label gpt-oss related PRs ([#22600](https://github.com/vllm-project/vllm/pull/22600)) by @draftbk
* Move `CacheConfig` from `config/__init__.py` to `config/cache.py` ([#22586](https://github.com/vllm-project/vllm/pull/22586)) by @hmellor
* [Misc][gpt-oss] guard import when triton kernel when not up to date  ([#22584](https://github.com/vllm-project/vllm/pull/22584)) by @zhewenl
* [CI/Build] Fix tensorizer test for load_format change ([#22583](https://github.com/vllm-project/vllm/pull/22583)) by @22quinn
* [Minor] Fix pre-commit error on main ([#22579](https://github.com/vllm-project/vllm/pull/22579)) by @Isotr0py
* [Misc] Replace flaky image urls in pixtral test ([#22574](https://github.com/vllm-project/vllm/pull/22574)) by @Isotr0py
* [Misc] code clean duplicate set_current_vllm_config in _set_vllm_config ([#22566](https://github.com/vllm-project/vllm/pull/22566)) by @andyxning
* Move `ParallelConfig` from `config/__init__.py` to `config/parallel.py` ([#22565](https://github.com/vllm-project/vllm/pull/22565)) by @hmellor
* Update docs for Minimax-Text support ([#22562](https://github.com/vllm-project/vllm/pull/22562)) by @tdoublep
* Drop flaky test_healthcheck_response_time ([#22539](https://github.com/vllm-project/vllm/pull/22539)) by @russellb
* Skip Qwen 1 in CI because remote code is no longer compatible with Transformers ([#22536](https://github.com/vllm-project/vllm/pull/22536)) by @hmellor
* Fix(benchmarks): allow multiple mm contents in OpenAI Chat Completion Benchmarks ([#22534](https://github.com/vllm-project/vllm/pull/22534)) by @h-brenoskuk
* Improve fast_topk function with type hints and documentation ([#22530](https://github.com/vllm-project/vllm/pull/22530)) by @skyloevil
* Extract `CompilationConfig` from `config.py` ([#22524](https://github.com/vllm-project/vllm/pull/22524)) by @hmellor
* [Platform] Custom ops support for FusedMoe ([#22509](https://github.com/vllm-project/vllm/pull/22509)) by @wangxiyuan
* [oss] Init gpt-oss bf16 support ([#22508](https://github.com/vllm-project/vllm/pull/22508)) by @jeejeelee
* [Misc] Further refine type annotations in parallel state ([#22499](https://github.com/vllm-project/vllm/pull/22499)) by @DarkLight1337
* [Misc] Begin deprecation of `get_tensor_model_*_group` ([#22494](https://github.com/vllm-project/vllm/pull/22494)) by @DarkLight1337
* [CI/Build] Fix multimodal tests ([#22491](https://github.com/vllm-project/vllm/pull/22491)) by @DarkLight1337
* [Misc] fix openai version ([#22485](https://github.com/vllm-project/vllm/pull/22485)) by @lengrongfu
* [Structured Output] Make the output of structured output example more complete ([#22481](https://github.com/vllm-project/vllm/pull/22481)) by @shen-shanshan
* [Attention] FA3 Attention Sinks Perf Boost ([#22478](https://github.com/vllm-project/vllm/pull/22478)) by @LucasWilkinson
* [Quantization]: Support compressed-tensors mixed-precision model loading ([#22468](https://github.com/vllm-project/vllm/pull/22468)) by @dsikka
* not tie_word_embeddings for glm-4.5 and glm-4.5v ([#22460](https://github.com/vllm-project/vllm/pull/22460)) by @zRzRzRzRzRzRzR
* [bench] Fix benchmark/serve.py to ignore unavailable results ([#22382](https://github.com/vllm-project/vllm/pull/22382)) by @lk-chen
* [Misc] normalize multiprocessing Queue usage ([#22371](https://github.com/vllm-project/vllm/pull/22371)) by @andyxning
* [Config] add "qwen" as a native eagle3 target supported model ([#22333](https://github.com/vllm-project/vllm/pull/22333)) by @lec77
* Implicit language-model-only mode via limit-mm-per-prompt ([#22299](https://github.com/vllm-project/vllm/pull/22299)) by @ywang96
* [Misc] benchmark_moe supports expert parallel ([#22251](https://github.com/vllm-project/vllm/pull/22251)) by @jeejeelee
* [Misc] DeepGEMM : Avoid JIT generation in the hot-path ([#22215](https://github.com/vllm-project/vllm/pull/22215)) by @varun-sundar-rabindranath
* [Log] Add Warning for Deprecation of DeepGEMM old version ([#22194](https://github.com/vllm-project/vllm/pull/22194)) by @yewentao256
* v1: Pass KVConnectorOutput to scheduler-side ([#22157](https://github.com/vllm-project/vllm/pull/22157)) by @orozery
* [V1] [Hybrid] Support Minimax-Text-01 in V1  ([#22151](https://github.com/vllm-project/vllm/pull/22151)) by @tdoublep
* [V0 Deprecation] Remove multi-step scheduling ([#22138](https://github.com/vllm-project/vllm/pull/22138)) by @WoosukKwon
*  vLLM Benchmark suite improvement ([#22119](https://github.com/vllm-project/vllm/pull/22119)) by @louie-tsai
* enable Docker-aware precompiled wheel setup ([#22106](https://github.com/vllm-project/vllm/pull/22106)) by @dougbtv
* [Kernels] Clean up FusedMoeMethodBase and modular kernel setup.  Remove extra arguments from modular kernel methods. ([#22035](https://github.com/vllm-project/vllm/pull/22035)) by @bnellnm
* Migrate MiniCPMVImageInputs to TensorSchema ([#21939](https://github.com/vllm-project/vllm/pull/21939)) by @bbeckca
* [Misc] Use config definitions from Transformers library ([#21913](https://github.com/vllm-project/vllm/pull/21913)) by @DarkLight1337
* Migrate LlavaNextVideoPixelInputs to TensorSchema ([#21843](https://github.com/vllm-project/vllm/pull/21843)) by @bbeckca
* Migrate LlavaNextImageInputs to TensorSchema ([#21774](https://github.com/vllm-project/vllm/pull/21774)) by @bbeckca
* Migrate LlavaImageInputs to TensorSchema ([#21770](https://github.com/vllm-project/vllm/pull/21770)) by @bbeckca
* Enable 4bit bnb prequant MOE ([#21548](https://github.com/vllm-project/vllm/pull/21548)) by @py-andy-c
* [V1] [Hybrid] Enable Full CUDA Graph (decode-only) for Mamba layers ([#21401](https://github.com/vllm-project/vllm/pull/21401)) by @tdoublep
* ci: Add CUDA + arm64 release builds ([#21201](https://github.com/vllm-project/vllm/pull/21201)) by @seemethere
* [LMCache][Example] Align the PYTHONHASHSEED for prefillers and decoders for KV chunks hashing ([#21161](https://github.com/vllm-project/vllm/pull/21161)) by @zejunchen-zejun
* [Benchmark] Add benchmark tool for multi turn conversations ([#20267](https://github.com/vllm-project/vllm/pull/20267)) by @pliops-daniels

## Contributors

@0xjunhao, @22quinn, @Csrayz, @DarkLight1337, @Edwardf0t1, @Gh0u1L5, @GuanLuo, @GuyStone, @Isotr0py, @JartX, @Jialin, @Josephasafg, @Jun-Howie, @LucasWilkinson, @NickLucche, @NirLevy98, @PicoCreator, @Pradyun92, @ProExpertProg, @RUTHLESS-BOT, @RishiAstra, @Sugar-zsg, @TheEpicDolphin, @WoosukKwon, @ZJY0516, @amirai21, @amirkl94, @andyxning, @bbeckca, @bnellnm, @charlifu, @chaunceyjiang, @chi2liu, @crypdick, @cyang49, @cyyever, @danielafrimi, @dongluw, @dougbtv, @draftbk, @dsikka, @dtrifiro, @ducviet00, @eicherseiji, @eldarkurtic, @ericcurtin, @fhl2000, @frankie-ys, @frankwang28, @h-brenoskuk, @heheda12345, @hmellor, @hsliuustc0106, @huachenheli, @iAmir97, @ilmarkov, @jeejeelee, @jikunshang, @jinzhen-lin, @jio-H, @kliuae, @kyuyeunk, @lec77, @lengrongfu, @linzebing, @lk-chen, @louie-tsai, @maxdebayser, @mgoin, @milesial, @mxz297, @njhill, @noooop, @nvjullin, @nvpohanh, @orozery, @p88h, @phantomlei3, @pliops-daniels, @py-andy-c, @qthequartermasterman, @rahul-tuli, @robertgshaw2-redhat, @russellb, @sayandipdutta, @seemethere, @shen-shanshan, @shixianc, @simon-mo, @skyloevil, @sooraj-satheesh, @sstamenk, @tdoublep, @teekenl, @tjtanaa, @varun-sundar-rabindranath, @vermouth1992, @vllmellm, @wangxiyuan, @wenscarl, @wseaton, @x22x22, @xuechendi, @yaochengji, @yewentao256, @youkaichao, @ywang96, @yyihuang, @yyweiss, @zRzRzRzRzRzRzR, @zejunchen-zejun, @zhewenl, @zifeitong, @zyongye, @zzh142857

