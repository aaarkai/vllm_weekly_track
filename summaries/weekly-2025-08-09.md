# Weekly Release Notes for vllm-project/vllm (2025-08-09)

## What's Changed

### ðŸš€ Features & Enhancements

* add the codes to check AMD Instinct GPU number (#22367) by @zhangnju
* feat: Add Flashinfer MoE Support for Compressed Tensor NVFP4 (#21639) by @yewentao256
* [Feature] Non-contiguous Support for FP8 Quantization (#21961) by @yewentao256

### ðŸ› Bug Fixes

* [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue (#22399) by @yewentao256
* [Bugfix] Make condition in triton kernel constexpr (#22370) by @gshtras
* [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks (#22368) by @LucasWilkinson
* [BugFix] Fix FA2 RuntimeError when sinks is provided (#22365) by @LucasWilkinson
* [BugFix] Improve internal DP load balancing (#21617) by @njhill
* [Bugfix] Fix: Fix multi loras with tp >=2 and LRU cache (#20873) by @charent
* [Bug] Update auto_tune.sh to separate benchmarking and profiling. (#21629) by @ericehanley

### âš¡ Performance Improvements

* [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036) by @yewentao256
* [Performance] Parallelize fill_bitmask to accelerate high-throughput guided decoding (#21862) by @benchislett

### ðŸ“š Documentation

* [Doc] Update pooling model docs (#22186) by @DarkLight1337
* [Docs] use `uv` in CPU installation docs (#22089) by @davidxia
* [Documentation] Add Voxtral to Supported Models page (#22059) by @DarkLight1337

### ðŸ“¦ Other Changes

* [gpt-oss] add demo tool server (#22393) by @heheda12345
* [gpt-oss] Add loop for built-in tool call (#22374) by @WoosukKwon
* [Minor] Fix type  (#22347) by @WoosukKwon
* [gpt-oss] Support chat completion api (#22342) by @WoosukKwon
* [gpt-oss] Add Tool/ConversationContext classes and harmony_utils (#22340) by @WoosukKwon
* [gpt-oss] flashinfer mxfp4 (#22339) by @zyongye
* [gpt-oss] add model to supported models doc (#22336) by @ywang96
* [gpt-oss] attention sink init fix gemini (#22335) by @zyongye
* [gpt-oss] Add openai-harmony as default dependency (#22332) by @WoosukKwon
* [gpt-oss] flashinfer attention sink init (#22330) by @zyongye
* [ROCm] Add attention sink to use_rocm_custom_paged_attention (#22329) by @WoosukKwon

## Contributors

@yewentao256, @heheda12345, @WoosukKwon, @gshtras, @LucasWilkinson, @zhangnju, @zyongye, @ywang96, @Isotr0py, @mgoin, @ruisearch42, @lsy323, @jeejeelee, @youkaichao, @NickLucche, @DarkLight1337, @tlrmchlsmth, @22quinn, @skyloevil, @andyxning, @davidxia, @hmellor, @tjtanaa, @vllmellm, @benchislett, @njhill, @dtrifiro, @sanchit-gandhi, @Abirdcfly, @lengrongfu, @elvischenv, @dsikka, @sarckk, @cyang49, @zhxchen17, @dougbtv, @varun-sundar-rabindranath, @zou3219, @wuhang2014, @eicherseiji, @sdavidbd, @kylesayrs, @vanbasten23, @yma11, @LopezCastroRoberto, @ericehanley, @morgendave, @noooop, @JartX, @alyosha-swamy, @yeqcharlotte, @ruisearch42, @fhl2000, @CLFutureX, @weixiao-huang, @SageMoore, @sidhpurwala-huzaifa, @ilmarkov, @n0gu-furiosa, @anijain2305, @TankNee, @Josephasafg, @vadiklyutiy, @mickaelseznec, @tdoublep, @MatthewBonanni, @amirkl94, @Oliver-ss, @chenxi-yang, @xiszishu, @bigPYJ1151, @tlipoca9, @zRzRzRzRzRzRzR, @nvpohan, @Alexei-V-Ivanov-AMD, @simon-mo, @ahengljh, @david6666666, @hsliuustc0106, @Abatom, @lk-chen, @linzebing, @bbeckca, @eric-haibin-lin, @charent, @TheEpicDolphin
