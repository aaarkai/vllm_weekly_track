# Weekly Release Notes for vllm-project/vllm (2025-08-09)

## What's Changed

### üöÄ Features & Enhancements

  * [Doc] Add usage of implicit text-only mode  ([#22561](https://github.com/vllm-project/vllm/pull/22561)) by @ywang96
  * [Docs] Add missing dependency for docs build ([#22435](https://github.com/vllm-project/vllm/pull/22435)) by @hmellor
  * Add H20-3e fused MoE kernel tuning configs for GLM-4.5 ([#22433](https://github.com/vllm-project/vllm/pull/22433)) by @JaceyShao
  * [gpt-oss] Support tool call and implement MCP tool server ([#22427](https://github.com/vllm-project/vllm/pull/22427)) by @heheda12345
  * [TPU] Add support for online w8a8 quantization ([#22425](https://github.com/vllm-project/vllm/pull/22425)) by @kyuyeunk
  * [gpt-oss] add demo tool server ([#22393](https://github.com/vllm-project/vllm/pull/22393)) by @heheda12345
  * Fix trtllm-gen attention env and add attention sink ([#22378](https://github.com/vllm-project/vllm/pull/22378)) by @IwakuraRein
  * [gpt-oss] Add loop for built-in tool call ([#22374](https://github.com/vllm-project/vllm/pull/22374)) by @WoosukKwon
  * add the codes to check AMD Instinct GPU number ([#22367](https://github.com/vllm-project/vllm/pull/22367)) by @zhangnju
  * [Bugfix] Add missing `packed_modules_mapping` to `DeepseekV2ForCausalLM` ([#22352](https://github.com/vllm-project/vllm/pull/22352)) by @fxmarty-amd
  * [gpt-oss] Add Tool/ConversationContext classes and harmony_utils ([#22340](https://github.com/vllm-project/vllm/pull/22340)) by @WoosukKwon
  * [gpt-oss] add model to supported models doc ([#22336](https://github.com/vllm-project/vllm/pull/22336)) by @ywang96
  * [gpt-oss] Add openai-harmony as default dependency ([#22332](https://github.com/vllm-project/vllm/pull/22332)) by @WoosukKwon
  * [ROCm] Add attention sink to use_rocm_custom_paged_attention ([#22329](https://github.com/vllm-project/vllm/pull/22329)) by @WoosukKwon
  * Add GPT-OSS model code and config [1/N] ([#22327](https://github.com/vllm-project/vllm/pull/22327)) by @WoosukKwon
  * [GptOss] Add GptOss reasoning parser to support structure output ([#22322](https://github.com/vllm-project/vllm/pull/22322)) by @heheda12345
  * Add attention sink in attention backends ([#22320](https://github.com/vllm-project/vllm/pull/22320)) by @WoosukKwon
  * [Bugfix] Add proper comparison for package versions ([#22314](https://github.com/vllm-project/vllm/pull/22314)) by @syedmba
  * [Log] Add Warning for Deprecation of DeepGEMM old version ([#22194](https://github.com/vllm-project/vllm/pull/22194)) by @yewentao256
  * [Docs] Update features/disagg_prefill, add v1 examples and development ([#22165](https://github.com/vllm-project/vllm/pull/22165)) by @david6666666
  * [Doc] add backend to doc string of initialize_model_parallel ([#22142](https://github.com/vllm-project/vllm/pull/22142)) by @andyxning
  * [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) ([#22131](https://github.com/vllm-project/vllm/pull/22131)) by @0xjunhao
  * [FEAT][ROCm] Enable running Flash Attention as ViT attn backend for Qwen-VL models on ROCm platform. ([#22069](https://github.com/vllm-project/vllm/pull/22069)) by @vllmellm
  * [PD] add test for chat completions endpoint ([#21925](https://github.com/vllm-project/vllm/pull/21925)) by @Abirdcfly
  * [Qwen3] Enable dual-chunk-attention support for Qwen3 models. ([#21924](https://github.com/vllm-project/vllm/pull/21924)) by @sighingnow
  * [Speculators][Speculative Decoding] Add Qwen Eagle3 Support ([#21835](https://github.com/vllm-project/vllm/pull/21835)) by @dsikka
  * [executor] feat: add supports_pp attr to executors ([#21786](https://github.com/vllm-project/vllm/pull/21786)) by @eric-haibin-lin
  * [Misc] Add tensor schema test coverage for multimodal models ([#21754](https://github.com/vllm-project/vllm/pull/21754)) by @Isotr0py
  * feat: Add Support GPTQ Quantization MOE on ROCM vllm serve ([#21733](https://github.com/vllm-project/vllm/pull/21733)) by @JartX
  * Fix Arcee model weight loading: Add custom load_weights ([#21725](https://github.com/vllm-project/vllm/pull/21725)) by @alyosha-swamy
  * [Docs] Factor out troubleshooting to its own guide; add section for Ray Observability ([#21578](https://github.com/vllm-project/vllm/pull/21578)) by @crypdick
  * [Test] Add Unit Test for Batched DeepGEMM ([#21559](https://github.com/vllm-project/vllm/pull/21559)) by @yewentao256
  * [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine ([#21496](https://github.com/vllm-project/vllm/pull/21496)) by @tjtanaa
  * Add chat doc in quick start ([#21213](https://github.com/vllm-project/vllm/pull/21213)) by @TankNee
  * [Attention][DBO] Add support for "splitting" the CommonAttentionMetadata ([#21153](https://github.com/vllm-project/vllm/pull/21153)) by @SageMoore
  * feat: Add --enable-log-outputs flag for logging model generations ([#20707](https://github.com/vllm-project/vllm/pull/20707)) by @mizadri
  * Add tree attention backend for v1 (part 1) ([#20401](https://github.com/vllm-project/vllm/pull/20401)) by @TheEpicDolphin
  * [Benchmark] Add benchmark tool for multi turn conversations ([#20267](https://github.com/vllm-project/vllm/pull/20267)) by @pliops-daniels
  * Add ModelOpt Qwen3 nvfp4 support ([#20101](https://github.com/vllm-project/vllm/pull/20101)) by @Edwardf0t1
  * [Frontend] Add unix domain socket support ([#18097](https://github.com/vllm-project/vllm/pull/18097)) by @yyweiss

### üêõ Bug Fixes

  * [Bugfix] Fix failing GPT-OSS initialization test ([#22557](https://github.com/vllm-project/vllm/pull/22557)) by @Isotr0py
  * [Bugfix] Fix CI moe kernel failure ([#22556](https://github.com/vllm-project/vllm/pull/22556)) by @jeejeelee
  * [CI/Build] Fix multimodal tests ([#22491](https://github.com/vllm-project/vllm/pull/22491)) by @DarkLight1337
  * Fix pre-commit ([#22487](https://github.com/vllm-project/vllm/pull/22487)) by @DarkLight1337
  * [Misc] fix openai version ([#22485](https://github.com/vllm-project/vllm/pull/22485)) by @lengrongfu
  * Fix loading of quantized BigCode models ([#22463](https://github.com/vllm-project/vllm/pull/22463)) by @eldarkurtic
  * Fix pre-commit error in main ([#22462](https://github.com/vllm-project/vllm/pull/22462)) by @WoosukKwon
  * [Tool] Fix auto tool call ([#22434](https://github.com/vllm-project/vllm/pull/22434)) by @heheda12345
  * [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 ([#22426](https://github.com/vllm-project/vllm/pull/22426)) by @nvpohanh
  * [Bugfix] Fix wrong method name in Intern-S1 image processor ([#22417](https://github.com/vllm-project/vllm/pull/22417)) by @DarkLight1337
  * [gpt-oss] fix model config with hf_config ([#22401](https://github.com/vllm-project/vllm/pull/22401)) by @zyongye
  * [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue ([#22399](https://github.com/vllm-project/vllm/pull/22399)) by @yewentao256
  * [Doc] Fix link to prefix caching design ([#22384](https://github.com/vllm-project/vllm/pull/22384)) by @sarckk
  * [bench] Fix benchmark/serve.py to ignore unavailable results ([#22382](https://github.com/vllm-project/vllm/pull/22382)) by @lk-chen
  * [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks ([#22368](https://github.com/vllm-project/vllm/pull/22368)) by @LucasWilkinson
  * [BugFix] Fix FA2 RuntimeError when sinks is provided ([#22365](https://github.com/vllm-project/vllm/pull/22365)) by @LucasWilkinson
  * [XPU]Fix `flash_attn_varlen_func` interface on xpu ([#22350](https://github.com/vllm-project/vllm/pull/22350)) by @jikunshang
  * [Minor] Fix type  ([#22347](https://github.com/vllm-project/vllm/pull/22347)) by @WoosukKwon
  * [gpt-oss] attention sink init fix gemini ([#22335](https://github.com/vllm-project/vllm/pull/22335)) by @zyongye
  * [Docs] fix broken links in metrics.md ([#22315](https://github.com/vllm-project/vllm/pull/22315)) by @GuyStone
  * [Bugfix] Fix 3D input passed into cutlass_scaled_mm ([#22278](https://github.com/vllm-project/vllm/pull/22278)) by @mgoin
  * [CI][TPU] Fix docker clean up ([#22271](https://github.com/vllm-project/vllm/pull/22271)) by @lsy323
  * [Bugfix] Fix MoE BNB version ([#22260](https://github.com/vllm-project/vllm/pull/22260)) by @jeejeelee
  * [bugfix] fix blackwell deepep installation ([#22255](https://github.com/vllm-project/vllm/pull/22255)) by @youkaichao
  * Revert "[Bugfix] V1 Fix the cursor leakage issue during request scheduling." ([#22223](https://github.com/vllm-project/vllm/pull/22223)) by @WoosukKwon
  * [V1] reduce block size for tree attention correctness test to fix 'ou‚Ä¶ ([#22207](https://github.com/vllm-project/vllm/pull/22207)) by @TheEpicDolphin
  * [ROCm][Bugfix] Compilation passes fix ([#22202](https://github.com/vllm-project/vllm/pull/22202)) by @gshtras
  * [Bugfix] Fix failing GGUF models test ([#22174](https://github.com/vllm-project/vllm/pull/22174)) by @Isotr0py
  * [CI Bugfix] Fix wNa16 kernel not found for test_shared_storage_connector_hashes ([#22163](https://github.com/vllm-project/vllm/pull/22163)) by @tlrmchlsmth
  * [RLHF] Fix torch.dtype not serializable in example ([#22158](https://github.com/vllm-project/vllm/pull/22158)) by @22quinn
  * [fix] fix correct assertion syntax error in attention utils. ([#22154](https://github.com/vllm-project/vllm/pull/22154)) by @skyloevil
  * [Bugfix] Fix failing multimodal standard test ([#22153](https://github.com/vllm-project/vllm/pull/22153)) by @Isotr0py
  * fix: kimi_k2 return empty tool call list ([#22149](https://github.com/vllm-project/vllm/pull/22149)) by @tlipoca9
  * [CI/Build][Bugfix] Fix Qwen2.5 tests in CPU CI via fallback silu_and_mul to torch native implementation ([#22145](https://github.com/vllm-project/vllm/pull/22145)) by @bigPYJ1151
  * [Fix] Fix llama4 modelopt weight loading error ([#22107](https://github.com/vllm-project/vllm/pull/22107)) by @jiahanc
  * [NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading ([#22073](https://github.com/vllm-project/vllm/pull/22073)) by @nvpohanh
  * [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match ([#22065](https://github.com/vllm-project/vllm/pull/22065)) by @chaunceyjiang
  * Fix test_kv_sharing_fast_prefill flakiness ([#22038](https://github.com/vllm-project/vllm/pull/22038)) by @sarckk
  * [Bugfix]: Fix the streaming output for function calls in the minimax ([#22015](https://github.com/vllm-project/vllm/pull/22015)) by @qscqesze
  * Fix Flashinfer CUTLASS MOE Allgather ([#21963](https://github.com/vllm-project/vllm/pull/21963)) by @wenscarl
  * [Bugfix] Fix ModernBert cuda graph capturing in v1 ([#21901](https://github.com/vllm-project/vllm/pull/21901)) by @Isotr0py
  * [Bugfix][V1][P/D]Fix the uneven polling issue in the toy proxy for P2pNcclConnector ([#21819](https://github.com/vllm-project/vllm/pull/21819)) by @Abatom
  * [BugFix] Fix IMA FlashMLA full cuda-graph and DP + Update FlashMLA ([#21691](https://github.com/vllm-project/vllm/pull/21691)) by @LucasWilkinson
  * [Attention] Support multiple attention metadata builders per kv_cache_spec  + proper local attention no hybrid kv cache fix ([#21588](https://github.com/vllm-project/vllm/pull/21588)) by @LucasWilkinson
  * [Bugfix] V1 Fix the cursor leakage issue during request scheduling. ([#21173](https://github.com/vllm-project/vllm/pull/21173)) by @CLFutureX

### ‚ö° Performance Improvements

  * [PERF] Use pybase64 to more quickly decode prompt embeddings ([#22469](https://github.com/vllm-project/vllm/pull/22469)) by @qthequartermasterman
  * Optimize logger init performance by using module-level constants ([#22373](https://github.com/vllm-project/vllm/pull/22373)) by @skyloevil
  * [Perf] Parallelize fill_bitmask to accelerate high-throughput guided decoding ([#21862](https://github.com/vllm-project/vllm/pull/21862)) by @benchislett
  * [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion ([#20000](https://github.com/vllm-project/vllm/pull/20000)) by @vadiklyutiy

### üìö Documentation

  * Update docs for Minimax-Text support ([#22562](https://github.com/vllm-project/vllm/pull/22562)) by @tdoublep
  * [Docs] Improve API docs (+small tweaks) ([#22459](https://github.com/vllm-project/vllm/pull/22459)) by @hmellor
  * [Doc] Update pooling model docs ([#22186](https://github.com/vllm-project/vllm/pull/22186)) by @DarkLight1337
  * docs: remove deprecated disable-log-requests flag ([#22113](https://github.com/vllm-project/vllm/pull/22113)) by @ywang96
  * [Misc] update doc comment for send ([#22026](https://github.com/vllm-project/vllm/pull/22026)) by @andyxning
  * [Doc] update docs for nightly benchmarks ([#12022](https://github.com/vllm-project/vllm/pull/12022)) by @andrewkchan

### üì¶ Other Changes

* [CI] [Hybrid] Speed up hybrid models test by removing large models  ([#22563](https://github.com/vllm-project/vllm/pull/22563)) by @tdoublep
  * [Bugfix] Update FA commit hash ([#22546](https://github.com/vllm-project/vllm/pull/22546)) by @tdoublep
  * Remove mamba_ssm from vLLM requirements; install inside test container using `--no-build-isolation` ([#22541](https://github.com/vllm-project/vllm/pull/22541)) by @tdoublep
  * Drop flaky test_healthcheck_response_time ([#22539](https://github.com/vllm-project/vllm/pull/22539)) by @russellb
  * Skip Qwen 1 in CI because remote code is no longer compatible with Transformers ([#22536](https://github.com/vllm-project/vllm/pull/22536)) by @hmellor
  * [gpt-oss] guard import when triton kernel is not installed ([#22529](https://github.com/vllm-project/vllm/pull/22529)) by @zyongye
  * Extract `CompilationConfig` from `config.py` ([#22524](https://github.com/vllm-project/vllm/pull/22524)) by @hmellor
  * GLM-4.5V with new class name at transformers ([#22520](https://github.com/vllm-project/vllm/pull/22520)) by @zRzRzRzRzRzRzR
  * Remove exception for Python 3.8 typing from linter ([#22506](https://github.com/vllm-project/vllm/pull/22506)) by @hmellor
  * [Misc] Begin deprecation of `get_tensor_model_*_group` ([#22494](https://github.com/vllm-project/vllm/pull/22494)) by @DarkLight1337
  * [BugFix] Don't cancel asyncio tasks directly from destructors ([#22476](https://github.com/vllm-project/vllm/pull/22476)) by @njhill
  * [Docs] Rename ‚ÄúDistributed inference and serving‚Äù to ‚ÄúParallelism & Scaling‚Äù ([#22466](https://github.com/vllm-project/vllm/pull/22466)) by @crypdick
  * Optimize MiniCPMO mask creation with vectorized implementation ([#22464](https://github.com/vllm-project/vllm/pull/22464)) by @skyloevil
  * not tie_word_embeddings for glm-4.5 and glm-4.5v ([#22460](https://github.com/vllm-project/vllm/pull/22460)) by @zRzRzRzRzRzRzR
  * [Core] Simplify mm processing cache ([#22457](https://github.com/vllm-project/vllm/pull/22457)) by @DarkLight1337
  * Remove `from_dict` from `SpeculativeConfig` ([#22451](https://github.com/vllm-project/vllm/pull/22451)) by @hmellor
  * [Frontend] Use engine argument to control MM cache size ([#22441](https://github.com/vllm-project/vllm/pull/22441)) by @DarkLight1337
  * [Misc] Enhance code formatting in mxfp4.py  ([#22423](https://github.com/vllm-project/vllm/pull/22423)) by @WoosukKwon
  * [gpt-oss] triton kernel mxfp4 ([#22421](https://github.com/vllm-project/vllm/pull/22421)) by @zyongye
  * [CI] Skip the pooling models that do not support transformers v4.55 ([#22411](https://github.com/vllm-project/vllm/pull/22411)) by @noooop
  * [gpt-oss] Generate ResponseOutputItem from Harmony Message ([#22410](https://github.com/vllm-project/vllm/pull/22410)) by @heheda12345
  * [Bench] Split serve.py:main into async/async versions ([#22405](https://github.com/vllm-project/vllm/pull/22405)) by @lk-chen
  * [gpt-oss] Convert user input to harmony format ([#22402](https://github.com/vllm-project/vllm/pull/22402)) by @heheda12345
  * Update `flashinfer-python==0.2.10` ([#22389](https://github.com/vllm-project/vllm/pull/22389)) by @mgoin
  * Use float32 for test_completion.py ([#22385](https://github.com/vllm-project/vllm/pull/22385)) by @mgoin
  * [Misc] normalize multiprocessing Queue usage ([#22371](https://github.com/vllm-project/vllm/pull/22371)) by @andyxning
  * [Bugfix] Make condition in triton kernel constexpr ([#22370](https://github.com/vllm-project/vllm/pull/22370)) by @gshtras
  * Update `hf_xet` pin to resolve hangs ([#22356](https://github.com/vllm-project/vllm/pull/22356)) by @hmellor
  * [gpt-oss] Support chat completion api ([#22342](https://github.com/vllm-project/vllm/pull/22342)) by @WoosukKwon
  * [gpt-oss] flashinfer mxfp4 ([#22339](https://github.com/vllm-project/vllm/pull/22339)) by @zyongye
  * [gpt-oss] flashinfer attention sink init ([#22330](https://github.com/vllm-project/vllm/pull/22330)) by @zyongye
  * [BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D ([#22317](https://github.com/vllm-project/vllm/pull/22317)) by @Pradyun92
  * Increase openai-python version ([#22316](https://github.com/vllm-project/vllm/pull/22316)) by @WoosukKwon
  * Upgrade FA3 for attention sink ([#22313](https://github.com/vllm-project/vllm/pull/22313)) by @WoosukKwon
  * [Misc] Clean up duplicated hf overrides ([#22311](https://github.com/vllm-project/vllm/pull/22311)) by @Isotr0py
  * [Doc] Sleep mode documentation ([#22310](https://github.com/vllm-project/vllm/pull/22310)) by @iAmir97
  * [XPU] upgrade torch 2.8 on for XPU ([#22300](https://github.com/vllm-project/vllm/pull/22300)) by @jikunshang
  * Implicit language-model-only mode via limit-mm-per-prompt ([#22299](https://github.com/vllm-project/vllm/pull/22299)) by @ywang96
  * [Bugfix] Remove faulty test for oot attention backend ([#22286](https://github.com/vllm-project/vllm/pull/22286)) by @mgoin
  * [Bugfix] Skip dead and non-GPU nodes for Ray DP engine allocation ([#22275](https://github.com/vllm-project/vllm/pull/22275)) by @ruisearch42
  * Support encoder_only attention for FlexAttention ([#22273](https://github.com/vllm-project/vllm/pull/22273)) by @maxdebayser
  * [Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm ([#22264](https://github.com/vllm-project/vllm/pull/22264)) by @gshtras
  * [V0 Deprecation][TPU] Remove V1 flag check from tests ([#22248](https://github.com/vllm-project/vllm/pull/22248)) by @NickLucche
  * [Docs][TPU] Highlight TPU Software version selection ([#22242](https://github.com/vllm-project/vllm/pull/22242)) by @NickLucche
  * [CI/Build] Update flashinfer to 0.2.9 ([#22233](https://github.com/vllm-project/vllm/pull/22233)) by @mgoin
  * [Core] Factor out common logic for MM budget calculation ([#22228](https://github.com/vllm-project/vllm/pull/22228)) by @DarkLight1337
  * [Bugfix] Misaligned params in TreeAttentionImpl ([#22226](https://github.com/vllm-project/vllm/pull/22226)) by @DarkLight1337
  * [UX] Fail if an invalid attention backend is specified ([#22217](https://github.com/vllm-project/vllm/pull/22217)) by @mgoin
  * [Misc] DeepGEMM : Avoid JIT generation in the hot-path ([#22215](https://github.com/vllm-project/vllm/pull/22215)) by @varun-sundar-rabindranath
  * preload heavy modules when mp method is forkserver ([#22214](https://github.com/vllm-project/vllm/pull/22214)) by @lionelvillard
  * [Log] DeepGEMM Update Log for Unaligned Problem Size ([#22208](https://github.com/vllm-project/vllm/pull/22208)) by @yewentao256
  * Optimize configuration access with LRU cache in custom ops ([#22204](https://github.com/vllm-project/vllm/pull/22204)) by @skyloevil
  * self.gate dtype update for GLM-4.5 ([#22203](https://github.com/vllm-project/vllm/pull/22203)) by @zRzRzRzRzRzRzR
  * [Refactor] Remove Unused Environment Variable `VLLM_NO_DEPRECATION_WARNING` ([#22199](https://github.com/vllm-project/vllm/pull/22199)) by @yewentao256
  * [Core] Store only the keys for multi-modal data in P0 ([#22198](https://github.com/vllm-project/vllm/pull/22198)) by @DarkLight1337
  * [FEAT] Refactor ROPE into module ([#22192](https://github.com/vllm-project/vllm/pull/22192)) by @tjtanaa
  * [Responses API] Ignore `store=True` and process the request by default ([#22185](https://github.com/vllm-project/vllm/pull/22185)) by @WoosukKwon
  * [Model] Switch to Fused RMS norm in Qwen2.5_VL model. ([#22184](https://github.com/vllm-project/vllm/pull/22184)) by @vllmellm
  * [Misc] Modify the organization of GLM series  ([#22171](https://github.com/vllm-project/vllm/pull/22171)) by @jeejeelee
  * [Bugfix] EPLB load statistics problem ([#22167](https://github.com/vllm-project/vllm/pull/22167)) by @david6666666
  * [model] Support MiniCPM-V 4.0 ([#22166](https://github.com/vllm-project/vllm/pull/22166)) by @tc-mb
  * v1: Pass KVConnectorOutput to scheduler-side ([#22157](https://github.com/vllm-project/vllm/pull/22157)) by @orozery
  * [refactor] improve ConstantList exception specificity ([#22156](https://github.com/vllm-project/vllm/pull/22156)) by @skyloevil
  * [V1] [Hybrid] Support Minimax-Text-01 in V1  ([#22151](https://github.com/vllm-project/vllm/pull/22151)) by @tdoublep
  * remove duplicate code within cleanup_dist_env_and_memory ([#22147](https://github.com/vllm-project/vllm/pull/22147)) by @andyxning
  * [Misc] log more detailed message for ensure_model_parallel_initialized ([#22144](https://github.com/vllm-project/vllm/pull/22144)) by @andyxning
  * fuse fp32 for GLM-4.5 e_score_correction_bias ([#22143](https://github.com/vllm-project/vllm/pull/22143)) by @zRzRzRzRzRzRzR
  * [Responses API] Disable response store by default ([#22137](https://github.com/vllm-project/vllm/pull/22137)) by @WoosukKwon
  * Use UV_LINK_MODE=copy in Dockerfile to avoid hardlink fail ([#22128](https://github.com/vllm-project/vllm/pull/22128)) by @mgoin
  * [Misc] Bump ray to 2.48.0 ([#22123](https://github.com/vllm-project/vllm/pull/22123)) by @ruisearch42
  * Revert "[compile][startup] Disable C++ compilation of symbolic shapes" ([#22122](https://github.com/vllm-project/vllm/pull/22122)) by @xiszishu
  * [Frontend] Improve error message for too many mm items ([#22114](https://github.com/vllm-project/vllm/pull/22114)) by @DarkLight1337
  * Remove index_put from MM embeddings merging ([#22105](https://github.com/vllm-project/vllm/pull/22105)) by @chenxi-yang
  * [Misc] `VLLM_TARGET_DEVICE.lower()` ([#22101](https://github.com/vllm-project/vllm/pull/22101)) by @NickLucche
  * [Frontend] Update OpenAI error response to upstream format ([#22099](https://github.com/vllm-project/vllm/pull/22099)) by @msanft
  * [ROCm][Misc] Rename the context_len to seq_len in ROCm custom paged attention kernel ([#22097](https://github.com/vllm-project/vllm/pull/22097)) by @charlifu
  * [NVIDIA] Support Flashinfer TRT-LLM Prefill Attention Kernel ([#22095](https://github.com/vllm-project/vllm/pull/22095)) by @elvischenv
  * [Model] Qwen2.5 VL SiLU-and-Mul ([#22066](https://github.com/vllm-project/vllm/pull/22066)) by @vllmellm
  * [Misc] Getting and passing ray runtime_env to workers ([#22040](https://github.com/vllm-project/vllm/pull/22040)) by @ruisearch42
  * [Bugfix] Mamba2 remove bugged initial state condition in chunk scan ([#22034](https://github.com/vllm-project/vllm/pull/22034)) by @cyang49
  * for glm-4.1V update ([#22000](https://github.com/vllm-project/vllm/pull/22000)) by @zRzRzRzRzRzRzR
  * [Misc] Support routing logic simulation ([#21990](https://github.com/vllm-project/vllm/pull/21990)) by @minosfuture
  * Use `aiohttp` connection pool for benchmarking ([#21981](https://github.com/vllm-project/vllm/pull/21981)) by @eicherseiji
  * [V1] [P/D] Refactor KV Connector Path ([#21980](https://github.com/vllm-project/vllm/pull/21980)) by @sdavidbd
  * [Feature] Non-contiguous Support for FP8 Quantization ([#21961](https://github.com/vllm-project/vllm/pull/21961)) by @yewentao256
  * [Misc] DeepGemmExperts : Avoid JIT generation in the hot-path ([#21955](https://github.com/vllm-project/vllm/pull/21955)) by @varun-sundar-rabindranath
  * [Misc] correct static type check for GroupCoordinator ([#21946](https://github.com/vllm-project/vllm/pull/21946)) by @andyxning
  * Update transformers to `v4.55` ([#21931](https://github.com/vllm-project/vllm/pull/21931)) by @hmellor
  * [Misc] Use config definitions from Transformers library ([#21913](https://github.com/vllm-project/vllm/pull/21913)) by @DarkLight1337
  * [Misc] Remove pass_config from CompilationConfig dump_json excluded ([#21911](https://github.com/vllm-project/vllm/pull/21911)) by @elvischenv
  * [Sampler] Support returning all logprobs or logits ([#21792](https://github.com/vllm-project/vllm/pull/21792)) by @22quinn
  * [V0 deprecation][P/D] Deprecate v0 `KVConnectorBase` code (1/2) ([#21785](https://github.com/vllm-project/vllm/pull/21785)) by @lk-chen
  * Migrate KimiVLImagePixelInputs to TensorSchema ([#21769](https://github.com/vllm-project/vllm/pull/21769)) by @bbeckca
  * [Benchmark] Support ready check timeout in `vllm bench serve` ([#21696](https://github.com/vllm-project/vllm/pull/21696)) by @yeqcharlotte
  * [xpu]support moe models on XPU platform ([#21643](https://github.com/vllm-project/vllm/pull/21643)) by @yma11
  * [Bug] Update auto_tune.sh to separate benchmarking and profiling. ([#21629](https://github.com/vllm-project/vllm/pull/21629)) by @ericehanley
  * [BugFix] Improve internal DP load balancing ([#21617](https://github.com/vllm-project/vllm/pull/21617)) by @njhill
  * [V1] [Hybrid] Validate compatibility of attention backend batch reordering at init time ([#21557](https://github.com/vllm-project/vllm/pull/21557)) by @tdoublep
  * [V1][CUDA] Full cudagraph support for FlashInfer ([#21367](https://github.com/vllm-project/vllm/pull/21367)) by @fhl2000
  * [V1] port xformers backend to v1 ([#21342](https://github.com/vllm-project/vllm/pull/21342)) by @TheEpicDolphin
  * Support Tensorrt-LLM MoE fp4 for low-latency ([#21331](https://github.com/vllm-project/vllm/pull/21331)) by @wenscarl
  * Support CUTLASS NVFP4 (w4a4) for Blackwell Geforce GPUs (SM120) ([#21309](https://github.com/vllm-project/vllm/pull/21309)) by @LopezCastroRoberto
  * [v1] - Mamba1 Attention Metadata ([#21249](https://github.com/vllm-project/vllm/pull/21249)) by @Josephasafg
  * [feat] move WEIGHT_SCALE_SUPPORTED into raise block to accelerate RLHF weight loading ([#21164](https://github.com/vllm-project/vllm/pull/21164)) by @weixiao-huang
  * [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead ([#21075](https://github.com/vllm-project/vllm/pull/21075)) by @cyang49
  * [Model] Pooling model activation supports per request control by PoolingParams ([#20538](https://github.com/vllm-project/vllm/pull/20538)) by @noooop

## Contributors

@0xjunhao, @22quinn, @Abatom, @Abirdcfly, @CLFutureX, @DarkLight1337, @Edwardf0t1, @GuyStone, @Isotr0py, @IwakuraRein, @JaceyShao, @JartX, @Josephasafg, @LopezCastroRoberto, @LucasWilkinson, @NickLucche, @Pradyun92, @SageMoore, @TankNee, @TheEpicDolphin, @WoosukKwon, @alyosha-swamy, @andrewkchan, @andyxning, @bbeckca, @benchislett, @bigPYJ1151, @charlifu, @chaunceyjiang, @chenxi-yang, @crypdick, @cyang49, @david6666666, @dsikka, @eicherseiji, @eldarkurtic, @elvischenv, @eric-haibin-lin, @ericehanley, @fhl2000, @fxmarty-amd, @gshtras, @heheda12345, @hmellor, @iAmir97, @jeejeelee, @jiahanc, @jikunshang, @kyuyeunk, @lengrongfu, @lionelvillard, @lk-chen, @lsy323, @maxdebayser, @mgoin, @minosfuture, @mizadri, @msanft, @njhill, @noooop, @nvpohanh, @orozery, @pliops-daniels, @qscqesze, @qthequartermasterman, @ruisearch42, @russellb, @sarckk, @sdavidbd, @sighingnow, @skyloevil, @syedmba, @tc-mb, @tdoublep, @tjtanaa, @tlipoca9, @tlrmchlsmth, @vadiklyutiy, @varun-sundar-rabindranath, @vllmellm, @weixiao-huang, @wenscarl, @xiszishu, @yeqcharlotte, @yewentao256, @yma11, @youkaichao, @ywang96, @yyweiss, @zRzRzRzRzRzRzR, @zhangnju, @zyongye
