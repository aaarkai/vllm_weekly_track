# Weekly Release Notes for vllm-project/vllm (2025-09-19)

## What's Changed

### ‚ú® Features & Enhancements

* Add 'path' option to ImagePrompt data_format ([#25081](https://github.com/vllm-project/vllm/pull/25081)) by @gfinol
* Add a batched auto tune script ([#25076](https://github.com/vllm-project/vllm/pull/25076)) by @karan
* Add pytest-cov and .coveragerc ([#24778](https://github.com/vllm-project/vllm/pull/24778)) by @rzabarazesh
* Add FLASHINFER_MLA to backend selector test ([#24753](https://github.com/vllm-project/vllm/pull/24753)) by @MatthewBonanni
* Add RADIO Vision Encoder Support to vLLM ([#24595](https://github.com/vllm-project/vllm/pull/24595)) by @danielafrimi
* [feat]: Create interface for model-specific M-RoPE ([#24194](https://github.com/vllm-project/vllm/pull/24194)) by @AzizCode92
* [Feat][EPLB] A novel static EPLB placement strategy for MoE models. ([#23745](https://github.com/vllm-project/vllm/pull/23745)) by @cboss6
* feat: Add Grafana and Perces monitoring dashboards for vLLM ([#23498](https://github.com/vllm-project/vllm/pull/23498)) by @liangwen12year
* Add more documentation and improve usability of lognormal dist (benchmark_serving_multi_turn) ([#23255](https://github.com/vllm-project/vllm/pull/23255)) by @pliops-daniels

### üêõ Bug Fixes

* [BugFix] Fix DeepGEMM warmup, no m.weight_scale_inv ([#25206](https://github.com/vllm-project/vllm/pull/25206)) by @LucasWilkinson
* Fix `validate-config` pre-commit check ([#25157](https://github.com/vllm-project/vllm/pull/25157)) by @hmellor
* Fix forward reference warning in documentation ([#25150](https://github.com/vllm-project/vllm/pull/25150)) by @hmellor
* [Bug] Fix `returned_lse` not Defined issue ([#25106](https://github.com/vllm-project/vllm/pull/25106)) by @yewentao256
* [Bug] Fix torch Compilation Cache Hit Error ([#25093](https://github.com/vllm-project/vllm/pull/25093)) by @yewentao256
* [BUG] Exclude .pth files when pulling remote files  ([#25092](https://github.com/vllm-project/vllm/pull/25092)) by @ahao-anyscale
* [Bugfix] Fix Stream usage in CPU model runner and OneDNN kernel check ([#25046](https://github.com/vllm-project/vllm/pull/25046)) by @bigPYJ1151
* [Bugfix][B200] Fix `cutlass_mla` hang ([#24966](https://github.com/vllm-project/vllm/pull/24966)) by @alexm-redhat
* [Bugfix][Qwen3-Next] add prefixes to shared_expert in qwen3-next and mlp in qwen2moe to successfully load ignored params in quantized models ([#24960](https://github.com/vllm-project/vllm/pull/24960)) by @toncao
* [Bugfix][Qwen3-Next] fixes the varlen issue in qwen3-next's MTP implementation. ([#24957](https://github.com/vllm-project/vllm/pull/24957)) by @sighingnow
* Fix: Add explicit #include <omp.h> for OpenMP compatibility on certain toolchains  ([#24951](https://github.com/vllm-project/vllm/pull/24951)) by @ihb2032
* [Bug] Fix Cutlass Scaled MM Compilation Error ([#24887](https://github.com/vllm-project/vllm/pull/24887)) by @yewentao256
* [Bugfix][Mamba] - Fix Conv State Kernel FP32 Support ([#24883](https://github.com/vllm-project/vllm/pull/24883)) by @Josephasafg
* [Bugfix] Fix accuracy issue for silu_mul + nvfp4 quant fusion kernel ([#24833](https://github.com/vllm-project/vllm/pull/24833)) by @elvischenv
* fix type of sampling rate for encode_base64 ([#24826](https://github.com/vllm-project/vllm/pull/24826)) by @co63oc
* [Bugfix] Fix GLM4.1V multimodal processor with compatability for Transformers v4.56 ([#24822](https://github.com/vllm-project/vllm/pull/24822)) by @Isotr0py
* [Bug] Fix `is_flashmla_supported` Check Error ([#24774](https://github.com/vllm-project/vllm/pull/24774)) by @yewentao256
* [Bugfix] Update import path for bc_linter_include ([#24766](https://github.com/vllm-project/vllm/pull/24766)) by @mmangkad
* [Bugfix] Fix GPUModelRunner has no attribute lora_manager ([#24762](https://github.com/vllm-project/vllm/pull/24762)) by @jeejeelee
* [Bugfix] Fix incompatibility between #20452 and #24548 ([#24754](https://github.com/vllm-project/vllm/pull/24754)) by @DarkLight1337
* [Bugfix] MiDashengLM model contact error under concurrent testing ([#24738](https://github.com/vllm-project/vllm/pull/24738)) by @bingchen-mi
* [Bugfix] Fix BNB name match ([#24735](https://github.com/vllm-project/vllm/pull/24735)) by @jeejeelee
* [Bugfix] Fix MRoPE dispatch on XPU ([#24724](https://github.com/vllm-project/vllm/pull/24724)) by @yma11
* [Bugfix] Fix MRoPE dispatch on CPU ([#24712](https://github.com/vllm-project/vllm/pull/24712)) by @bigPYJ1151
* [BugFix] Fix Qwen3-Next PP ([#24709](https://github.com/vllm-project/vllm/pull/24709)) by @njhill
* Fix implementation divergence for BLOOM models between vLLM and HuggingFace when using prompt embeds ([#24686](https://github.com/vllm-project/vllm/pull/24686)) by @qthequartermasterman
* [Bugfix] fixes the causal_conv1d_update kernel update non-speculative decoding cases ([#24680](https://github.com/vllm-project/vllm/pull/24680)) by @sighingnow
* [BugFix] enable DOTALL to match multi-line tool_call parameters in extract_tool_call_required_streaming ([#24668](https://github.com/vllm-project/vllm/pull/24668)) by @shijun-yin
* [Bugfix][Frontend] Fix `--enable-log-outputs` does not match the documentation ([#24626](https://github.com/vllm-project/vllm/pull/24626)) by @kebe7jun
* [Bugfix] Refactor Flashinfer TRTLLM attention kernel selection logic ([#24600](https://github.com/vllm-project/vllm/pull/24600)) by @elvischenv
* [Bugfix] Fix unable to run encoder model when disable_hybrid_kv_cache_manager is true ([#24571](https://github.com/vllm-project/vllm/pull/24571)) by @lianyiibo
* [Bug] [Spec Dec]: Fix kv_cache dtype mismatch for Eagle3 drafter on FP8 target ([#24505](https://github.com/vllm-project/vllm/pull/24505)) by @vllmellm
* [Bugfix] when use s3 model cannot use default load_format ([#24435](https://github.com/vllm-project/vllm/pull/24435)) by @lengrongfu
* [Bugfix] Fix sequence parallelism bug when enable pipeline parallelism ([#24021](https://github.com/vllm-project/vllm/pull/24021)) by @cascade812
* [fix]: remove data type hardcoding from gptoss model implementation ([#23807](https://github.com/vllm-project/vllm/pull/23807)) by @nikhil-arm
* [fix] lora benchmarks pass no_lora_flag_cpu ([#23774](https://github.com/vllm-project/vllm/pull/23774)) by @dolpm
* [Bugfix] remove duplicate tokens streamed in required tool choice streaming ([#23312](https://github.com/vllm-project/vllm/pull/23312)) by @Jason-CKY

### ‚ö°Ô∏è Performance

* [PERF] Add `conv1d` metadata to GDN attn ([#25105](https://github.com/vllm-project/vllm/pull/25105)) by @vadiklyutiy
* [Performance] Remove redundant clone() calls in cutlass_mla ([#24891](https://github.com/vllm-project/vllm/pull/24891)) by @alexm-redhat
* [Perf] Fix DeepGEMM Contiguous Layout Issue, 5.5% Throughput Improvement ([#24783](https://github.com/vllm-project/vllm/pull/24783)) by @yewentao256
* [Perf] Use NVIDIA hardware-accelerated instruction for float to fp8_e4m3 quantization ([#24757](https://github.com/vllm-project/vllm/pull/24757)) by @elvischenv
* [Perf] Reuse workspace for FP8+FP4 Marlin MoE ([#20500](https://github.com/vllm-project/vllm/pull/20500)) by @mgoin

### ü§ñ Model Support

* [Model] Improve Pooling Model ([#25149](https://github.com/vllm-project/vllm/pull/25149)) by @jeejeelee
* [gpt-oss] Add ResponseReasoningPartAddedEvent, ResponseReasoningPartDoneEvent for streaming ([#24938](https://github.com/vllm-project/vllm/pull/24938)) by @qandrew
* [Model] Pass param prefix to LLMHead ([#24862](https://github.com/vllm-project/vllm/pull/24862)) by @whx-sjtu
* [Model] Apply SharedFusedMoE to glm4_moe. ([#24849](https://github.com/vllm-project/vllm/pull/24849)) by @whx-sjtu
* [gpt-oss][1b] streaming add item id, content id ([#24788](https://github.com/vllm-project/vllm/pull/24788)) by @qandrew
* [gpt-oss][1a] create_responses stream outputs BaseModel type, api server is SSE still ([#24759](https://github.com/vllm-project/vllm/pull/24759)) by @qandrew
* [Model] Switch to Fused RMSNorm in GLM-4.1V model ([#24733](https://github.com/vllm-project/vllm/pull/24733)) by @SamitHuang
* [Model] Support Qwen3-VL Model Series ([#24727](https://github.com/vllm-project/vllm/pull/24727)) by @ywang96
* [Model]: support Ling2.0 ([#24627](https://github.com/vllm-project/vllm/pull/24627)) by @ant-yy
* [gpt-oss] Add IncompleteDetails to ResponsesRepsonse ([#24561](https://github.com/vllm-project/vllm/pull/24561)) by @qandrew
* [gpt-oss][2] fix types for streaming ([#24556](https://github.com/vllm-project/vllm/pull/24556)) by @qandrew
* [Model] Add Olmo3 model implementation ([#24534](https://github.com/vllm-project/vllm/pull/24534)) by @2015aroras
* [gpt-oss][1][bugfix] fix streaming final output ([#24466](https://github.com/vllm-project/vllm/pull/24466)) by @qandrew
* [Model] Clean up and simplify Mamba2 Metadata Usage in both V0 and V1 ([#24331](https://github.com/vllm-project/vllm/pull/24331)) by @cyang49
* [Model] enable data parallel for InternVL vision encoder ([#23909](https://github.com/vllm-project/vllm/pull/23909)) by @666even666

### üîå Hardware & Backend

* [ROCm][CI/Build] Use ROCm7.0 as the base ([#25178](https://github.com/vllm-project/vllm/pull/25178)) by @gshtras
* [XPU] Whisper model support on XPU Platform ([#25123](https://github.com/vllm-project/vllm/pull/25123)) by @chaojun-zhang
* [ROCm][AITER][Bugfix] Switch AITER to use PIECEWISE_AND_FULL compilation ([#25104](https://github.com/vllm-project/vllm/pull/25104)) by @Rohan138
* [XPU] Fix xpu model runner call torch.cuda APIs ([#25011](https://github.com/vllm-project/vllm/pull/25011)) by @jikunshang
* [ROCm][Bugfix] Aiter mha fp8 fix ([#24991](https://github.com/vllm-project/vllm/pull/24991)) by @dllehr-amd
* [XPU] Fix circular import error.  ([#24927](https://github.com/vllm-project/vllm/pull/24927)) by @jikunshang
* [ROCm] Add dependencies for ROCm ([#24900](https://github.com/vllm-project/vllm/pull/24900)) by @Concurrensee
* [ROCm][Bugfix] Fix the case where there's bias ([#24895](https://github.com/vllm-project/vllm/pull/24895)) by @gshtras
* [XPU] Set consistent default KV cache layout ([#24745](https://github.com/vllm-project/vllm/pull/24745)) by @NickLucche
* [Rocm] [quantization] Fix quark ptpc moe and add test case ([#24649](https://github.com/vllm-project/vllm/pull/24649)) by @haoyangli-amd

### ‚öôÔ∏è Refactoring & Core

* Remove unused find_cuda_init helper script ([#25044](https://github.com/vllm-project/vllm/pull/25044)) by @simon-mo
* [Frontend] Support setting logprobs to -1 ([#25031](https://github.com/vllm-project/vllm/pull/25031)) by @chaunceyjiang
* [Core][MM] Cleanup `MultiModalCache` ([#25006](https://github.com/vllm-project/vllm/pull/25006)) by @lgeiger
* [Core] Get num_encoder_tokens from scheduler config ([#24989](https://github.com/vllm-project/vllm/pull/24989)) by @russellb
* [Core][MultiModalHasher] Hash images without converting image mode ([#24969](https://github.com/vllm-project/vllm/pull/24969)) by @lgeiger
* [Frontend] Support returning all prompt logprobs ([#24956](https://github.com/vllm-project/vllm/pull/24956)) by @chaunceyjiang
* [Core][MultiModalHasher] Don't convert memoryviews to bytes during hashing ([#24925](https://github.com/vllm-project/vllm/pull/24925)) by @lgeiger
* Remove V0 Encoder-Decoder Support ([#24907](https://github.com/vllm-project/vllm/pull/24907)) by @WoosukKwon
* [Kernel] Better inf handling for grouped topk cu ([#24886](https://github.com/vllm-project/vllm/pull/24886)) by @lumina37
* [Core] Use `CpuGpuBuffer` for block table tensors ([#24795](https://github.com/vllm-project/vllm/pull/24795)) by @njhill
* [Core][Multimodal] Cache `supports_kw` ([#24773](https://github.com/vllm-project/vllm/pull/24773)) by @lgeiger
* Remove redundant assignment in xfer_buffers, This is a little fix ([#24732](https://github.com/vllm-project/vllm/pull/24732)) by @ChenTaoyu-SJTU
* [Kernel] [CPU] refactor `cpu_attn.py:_run_sdpa_forward` for better memory access ([#24701](https://github.com/vllm-project/vllm/pull/24701)) by @ignaciosica
* [CORE] Prompt Embeddings Support for v1 Engine ([#24278](https://github.com/vllm-project/vllm/pull/24278)) by @qthequartermasterman
* [Core] Support async scheduling with uniproc executor  ([#24219](https://github.com/vllm-project/vllm/pull/24219)) by @njhill
* [Core] Remove tokenizer group in vLLM ([#24078](https://github.com/vllm-project/vllm/pull/24078)) by @zhuohan123
* [Kernel] Faster pre-processing time for W4A8 ([#23972](https://github.com/vllm-project/vllm/pull/23972)) by @czhu-cohere
* Remove old cutlass mla ([#23961](https://github.com/vllm-project/vllm/pull/23961)) by @MatthewBonanni
* [Frontend][Multimodal] Allow skipping media data when UUIDs are provided.  ([#23950](https://github.com/vllm-project/vllm/pull/23950)) by @huachenheli
* [Kernel] Delegate construction of FusedMoEQuantConfig to FusedMoEMethodBase subclasses ([#22537](https://github.com/vllm-project/vllm/pull/22537)) by @bnellnm
* Refactor dense FP8 tensor/channel/block utils and add CT FP8 block ([#21404](https://github.com/vllm-project/vllm/pull/21404)) by @mgoin
* [Kernel] Enable Hybrid Model Support in Triton Unified Attention Kernel ([#21197](https://github.com/vllm-project/vllm/pull/21197)) by @jvlunteren
* [Core] Shared memory based object store for Multimodal data caching and IPC ([#20452](https://github.com/vllm-project/vllm/pull/20452)) by @dongluw
* [Frontend] Skip `stop` in reasoning content ([#14550](https://github.com/vllm-project/vllm/pull/14550)) by @gaocegege

### üîß Build, CI & Testing

* [CI] Revert back prepare_prompts and check_answers ([#25087](https://github.com/vllm-project/vllm/pull/25087)) by @WoosukKwon
* [CI][Bugfix] Fix failing Blackwell test ([#24993](https://github.com/vllm-project/vllm/pull/24993)) by @MatthewBonanni
* [CI] GPT-OSS GPQA eval test for Blackwell ([#24920](https://github.com/vllm-project/vllm/pull/24920)) by @mgoin
* [ci] fix wheel names for arm wheels ([#24898](https://github.com/vllm-project/vllm/pull/24898)) by @simon-mo
* [CI][Spec Decode] Adjust threshold for flaky ngram spec decoding test again ([#24771](https://github.com/vllm-project/vllm/pull/24771)) by @wwl2755
* [CI] Trigger BC Linter when labels are added/removed ([#24767](https://github.com/vllm-project/vllm/pull/24767)) by @zhewenl
* [CI] Fix flaky test  v1/worker/test_gpu_model_runner.py::test_kv_cache_stride_order          ([#24640](https://github.com/vllm-project/vllm/pull/24640)) by @heheda12345
* [CI] Add ci_envs for convenient local testing ([#24630](https://github.com/vllm-project/vllm/pull/24630)) by @noooop
* [CI] Add Decode Context Parallelism (DCP) test to CI ([#24487](https://github.com/vllm-project/vllm/pull/24487)) by @minosfuture
* [CI] Small Accuracy Eval Test for Deepseek Model ([#24259](https://github.com/vllm-project/vllm/pull/24259)) by @yewentao256
* [CI] Speed up model unit tests in CI ([#24253](https://github.com/vllm-project/vllm/pull/24253)) by @afeldman-nm

### üìö Documentation

* [Docs] Fix warnings in mkdocs build (continued) ([#25163](https://github.com/vllm-project/vllm/pull/25163)) by @Zerohertz
* [Docs] Fix API Reference ([#25140](https://github.com/vllm-project/vllm/pull/25140)) by @hmellor
* [Docs] Clean up the contributing README ([#25099](https://github.com/vllm-project/vllm/pull/25099)) by @hmellor
* [Doc] Fix cross-reference warnings ([#25058](https://github.com/vllm-project/vllm/pull/25058)) by @punitvara
* [Docs] Fix griffe warning in base_static_graph.py ([#25018](https://github.com/vllm-project/vllm/pull/25018)) by @windsonsea
* [Docs] fix invalid doc link ([#25017](https://github.com/vllm-project/vllm/pull/25017)) by @yyzxw
* [Docs] improve code formatting and comments for eliminate griffe build warning. ([#25010](https://github.com/vllm-project/vllm/pull/25010)) by @samzong
* [Docs] vllm/benchmarks/datasets.py fix docstring param format. ([#24970](https://github.com/vllm-project/vllm/pull/24970)) by @samzong
* [Docs] Fix pooling-params doc references in openai_compatible_server.md ([#24939](https://github.com/vllm-project/vllm/pull/24939)) by @yankay
* [Docs] Update instructions for how to using existing torch binary ([#24892](https://github.com/vllm-project/vllm/pull/24892)) by @zou3519
* [Docs] Have a try to improve frameworks/streamlit.md ([#24841](https://github.com/vllm-project/vllm/pull/24841)) by @windsonsea
* [Doc]: fix typos in various files ([#24821](https://github.com/vllm-project/vllm/pull/24821)) by @didier-durand
* [Docs] move benchmarks README to contributing guides ([#24820](https://github.com/vllm-project/vllm/pull/24820)) by @yeqcharlotte
* [Doc]: fix typos in various files ([#24798](https://github.com/vllm-project/vllm/pull/24798)) by @didier-durand
* [Docs] Fix warnings in mkdocs build (continued) ([#24791](https://github.com/vllm-project/vllm/pull/24791)) by @Zerohertz
* [Doc]: Remove 404 hyperlinks ([#24785](https://github.com/vllm-project/vllm/pull/24785)) by @rozeappletree
* [Docs] Fix warnings in mkdocs build (continued) ([#24740](https://github.com/vllm-project/vllm/pull/24740)) by @Zerohertz
* [Doc]: fix typos in various files ([#24726](https://github.com/vllm-project/vllm/pull/24726)) by @didier-durand
* [DOCs] Update ROCm installation docs section ([#24691](https://github.com/vllm-project/vllm/pull/24691)) by @gshtras
* [Docs] Remove Neuron install doc as backend no longer exists ([#24396](https://github.com/vllm-project/vllm/pull/24396)) by @hmellor
* [Doc] Add --force-overwrite option to generate_cmake_presets.py ([#24375](https://github.com/vllm-project/vllm/pull/24375)) by @elvischenv
* [Docs] add the parallel sampling usage in LLMEngine and AsyncLLM ([#24222](https://github.com/vllm-project/vllm/pull/24222)) by @gigit0000

### üì¶ Miscellaneous

* [KV offload][1b/N] rename offloading to kv_offload ([#25191](https://github.com/vllm-project/vllm/pull/25191)) by @orozery
* [V0 Deprecation] Remove unused async_timeout.py ([#25190](https://github.com/vllm-project/vllm/pull/25190)) by @WoosukKwon
* [Misc] Add codeowner for Transformers backend ([#25180](https://github.com/vllm-project/vllm/pull/25180)) by @hmellor
* [Misc] Add kv-connector label ([#25156](https://github.com/vllm-project/vllm/pull/25156)) by @NickLucche
* Move `StructuredOutputsConfig` from `config/__init__.py` to `config/structured_outputs.py` ([#25153](https://github.com/vllm-project/vllm/pull/25153)) by @hmellor
* [Misc] Clean up flags in `vllm bench serve` ([#25138](https://github.com/vllm-project/vllm/pull/25138)) by @ywang96
* [spec decode] Fix MTP inference path for MiMo-7B model ([#25136](https://github.com/vllm-project/vllm/pull/25136)) by @zixi-qi
* [V0 Deprecation] Skip PP test ([#25128](https://github.com/vllm-project/vllm/pull/25128)) by @WoosukKwon
* [V0 Deprecation] Remove misc V0 tests ([#25118](https://github.com/vllm-project/vllm/pull/25118)) by @WoosukKwon
* [V0 Deprecation] Remove more V0 tests ([#25117](https://github.com/vllm-project/vllm/pull/25117)) by @WoosukKwon
* [V0 Deprecation] Remove V0 Tracing & Metrics tests ([#25115](https://github.com/vllm-project/vllm/pull/25115)) by @WoosukKwon
* [V0 Deprecation] Remove V0 Engine tests ([#25114](https://github.com/vllm-project/vllm/pull/25114)) by @WoosukKwon
* Disable failing GPT-OSS Eval (Blackwell) for now ([#25107](https://github.com/vllm-project/vllm/pull/25107)) by @mgoin
* [V0 Deprecation] Remove V0 tests in test_sequence.py ([#25088](https://github.com/vllm-project/vllm/pull/25088)) by @WoosukKwon
* [CI Bugfix] Fix failing test_model_load_with_params tests due to tokenizer refactor ([#25086](https://github.com/vllm-project/vllm/pull/25086)) by @mgoin
* Retrieve `sliding_window` from text config in Gemma3 MM ([#25085](https://github.com/vllm-project/vllm/pull/25085)) by @hmellor
* [V0 Deprecation] Remove V0 Core tests ([#25082](https://github.com/vllm-project/vllm/pull/25082)) by @WoosukKwon
* [Qwen] Add fp8 checkpoint support for qwen3-next. ([#25079](https://github.com/vllm-project/vllm/pull/25079)) by @sighingnow
* [CI Bugfix] Fix failing test_invalid_env ([#25078](https://github.com/vllm-project/vllm/pull/25078)) by @mgoin
* Mark prompt logprobs as incompatible with prompt embeds at API level ([#25077](https://github.com/vllm-project/vllm/pull/25077)) by @qthequartermasterman
* silu-v1: Fix EPS not being used during max-reduction ([#25069](https://github.com/vllm-project/vllm/pull/25069)) by @elvircrn
* [Misc] Avoid use of deprecated `AutoModelForVision2Seq` ([#25065](https://github.com/vllm-project/vllm/pull/25065)) by @DarkLight1337
* cleanup: remove adapter commons  ([#25045](https://github.com/vllm-project/vllm/pull/25045)) by @simon-mo
* [Misc] Update owners for KV connector and V1 offloading ([#25041](https://github.com/vllm-project/vllm/pull/25041)) by @ApostaC
* [DP] Create placement groups by ray_device_key ([#25026](https://github.com/vllm-project/vllm/pull/25026)) by @xinyu-intel
* [V0 Deprecation] Remove AsyncLLMEngine ([#25025](https://github.com/vllm-project/vllm/pull/25025)) by @WoosukKwon
* [V0 Deprecation] Remove unused output processor util ([#25023](https://github.com/vllm-project/vllm/pull/25023)) by @WoosukKwon
* [V0 Deprecation] Remove MQLLMEngine ([#25019](https://github.com/vllm-project/vllm/pull/25019)) by @WoosukKwon
* [UX] Remove "quantization is not fully optimized yet" log ([#25012](https://github.com/vllm-project/vllm/pull/25012)) by @mgoin
* Change log level from info to debug for IOProcessor ([#24999](https://github.com/vllm-project/vllm/pull/24999)) by @mgoin
* [misc] fix typo in value error ([#24995](https://github.com/vllm-project/vllm/pull/24995)) by @prashantgupta24
* Use kwargs for long lists of `EngineCoreRequest` arguments in tests and fix extra kwargs ([#24987](https://github.com/vllm-project/vllm/pull/24987)) by @qthequartermasterman
* [Misc] Add removed encoder-decoder models to previously supported models list ([#24961](https://github.com/vllm-project/vllm/pull/24961)) by @Isotr0py
* [MM Encoder] Apply DP ViT for Qwen3-VL model series ([#24955](https://github.com/vllm-project/vllm/pull/24955)) by @ywang96
* [MISC] Add code owners of vllm/v1 to vllm/v1/core ([#24928](https://github.com/vllm-project/vllm/pull/24928)) by @heheda12345
* [QWEN NEXT] Fused MoE kernels Optimization configs ([#24924](https://github.com/vllm-project/vllm/pull/24924)) by @samanamp
* Updated CODEOWNERS for flashinfer, mla, fused_moe ([#24906](https://github.com/vllm-project/vllm/pull/24906)) by @mgoin
* Move `SpeculativeConfig` from `config/__init__.py` to `config/speculative.py` ([#24904](https://github.com/vllm-project/vllm/pull/24904)) by @hmellor
* [Deprecation] Remove DeepGEMM Old Symbol Wrapper ([#24902](https://github.com/vllm-project/vllm/pull/24902)) by @yewentao256
* feat(api): Return 503 on /health when engine is dead ([#24897](https://github.com/vllm-project/vllm/pull/24897)) by @dongbo910220
* `HuggingFace` -> `Hugging Face` in `Integration with Hugging Face` docs ([#24889](https://github.com/vllm-project/vllm/pull/24889)) by @sergiopaniego
* [Compile] Fix noop_elimination pass and add tests for noop_elimination ([#24880](https://github.com/vllm-project/vllm/pull/24880)) by @ZJY0516
* Removes source compilation of nixl dependency ([#24874](https://github.com/vllm-project/vllm/pull/24874)) by @bbartels
* [New Model] Support BertForTokenClassification / Named Entity Recognition (NER) task ([#24872](https://github.com/vllm-project/vllm/pull/24872)) by @noooop
* Bump Flashinfer to 0.3.1 ([#24868](https://github.com/vllm-project/vllm/pull/24868)) by @bbartels
* [Misc] Own KVConnectors installation ([#24867](https://github.com/vllm-project/vllm/pull/24867)) by @NickLucche
* Directly get max encoder len from VLLM config in V1 ([#24866](https://github.com/vllm-project/vllm/pull/24866)) by @Sugar-zsg
* [Misc] Fix examples openai_pooling_client.py  ([#24853](https://github.com/vllm-project/vllm/pull/24853)) by @noooop
* [Chore] Remove ipex_ops warning ([#24835](https://github.com/vllm-project/vllm/pull/24835)) by @robertgshaw2-redhat
* [Misc] Improve `s3_utils` type hints with `BaseClient` ([#24825](https://github.com/vllm-project/vllm/pull/24825)) by @Zerohertz
* Force use C++17 globally to avoid compilation error ([#24823](https://github.com/vllm-project/vllm/pull/24823)) by @chenfengjin
* [Benchmarks] Throw usage error when using dataset-name random and dataset-path together ([#24819](https://github.com/vllm-project/vllm/pull/24819)) by @yeqcharlotte
* [Chore] Minor simplification for non-PP path ([#24810](https://github.com/vllm-project/vllm/pull/24810)) by @WoosukKwon
* [Minor] Simplify duplicative device check for cuda ([#24793](https://github.com/vllm-project/vllm/pull/24793)) by @ziliangpeng
* [Chore] Remove unused batched RoPE op & kernel ([#24789](https://github.com/vllm-project/vllm/pull/24789)) by @WoosukKwon
* Invert pattern order to make sure that out_proj layers are identified ([#24781](https://github.com/vllm-project/vllm/pull/24781)) by @anmarques
* [Compilation Bug] Fix Inductor Graph Output with Shape Issue ([#24772](https://github.com/vllm-project/vllm/pull/24772)) by @yewentao256
* [benchmark] Add triton version in the moe tuned config ([#24769](https://github.com/vllm-project/vllm/pull/24769)) by @jeejeelee
* [Misc] Correct an outdated comment. ([#24765](https://github.com/vllm-project/vllm/pull/24765)) by @russellb
* [UX] Enforce valid choices for envs like VLLM_ATTENTION_BACKEND, etc ([#24761](https://github.com/vllm-project/vllm/pull/24761)) by @mgoin
* [CI Failure] Fix test_flashinfer_cutlass_mxfp4_mxfp8_fused_moe ([#24750](https://github.com/vllm-project/vllm/pull/24750)) by @mgoin
* [Models] Optimise and simplify `_validate_and_reshape_mm_tensor` ([#24742](https://github.com/vllm-project/vllm/pull/24742)) by @lgeiger
* [Models] Prevent CUDA sync in Qwen2.5-VL ([#24741](https://github.com/vllm-project/vllm/pull/24741)) by @lgeiger
* [Qwen3-Next] MoE configs for H100 TP=1,2 and TP2/EP ([#24739](https://github.com/vllm-project/vllm/pull/24739)) by @elvircrn
* [sleep mode] save memory for on-the-fly quantization ([#24731](https://github.com/vllm-project/vllm/pull/24731)) by @youkaichao
* Reinstate existing torch script ([#24729](https://github.com/vllm-project/vllm/pull/24729)) by @hmellor
* [CI/Build] Skip prompt embeddings tests on V1-only CPU backend ([#24721](https://github.com/vllm-project/vllm/pull/24721)) by @bigPYJ1151
* [Benchmarks] Add MMVU video dataset support and clean up deprecated datasets ([#24719](https://github.com/vllm-project/vllm/pull/24719)) by @Isotr0py
* [Misc][gpt-oss] Add gpt-oss label to PRs that mention harmony or related to builtin tool call ([#24717](https://github.com/vllm-project/vllm/pull/24717)) by @heheda12345
* [Qwen3-Next] MoE configs for H20 TP=1,2,4,8 ([#24707](https://github.com/vllm-project/vllm/pull/24707)) by @jeejeelee
* [Attention][FlashInfer] Enable FP8 FlashInfer (TRTLLM) MLA decode ([#24705](https://github.com/vllm-project/vllm/pull/24705)) by @MatthewBonanni
* [Startup] Make DeepGEMM warmup scale with max-num-batched-tokens ([#24693](https://github.com/vllm-project/vllm/pull/24693)) by @LucasWilkinson
* [Qwen3Next] Fixes the cuda graph capture conditions under large batch sizes (#24660) ([#24667](https://github.com/vllm-project/vllm/pull/24667)) by @sighingnow
* Move `MultiModalConfig` from `config/__init__.py` to `config/multimodal.py` ([#24659](https://github.com/vllm-project/vllm/pull/24659)) by @hmellor
* [UX] Remove AsyncLLM torch profiler disabled log ([#24609](https://github.com/vllm-project/vllm/pull/24609)) by @mgoin
* Apply fixes for CUDA 13 ([#24599](https://github.com/vllm-project/vllm/pull/24599)) by @Aidyn-A
* [Mamba] Support TP>1 with quantization for mamba2 mixer in case `n_groups % tp_size == 0` ([#24593](https://github.com/vllm-project/vllm/pull/24593)) by @tomeras91
* [Multimodal] Remove legacy multimodal fields in favor of MultiModalFeatureSpec  ([#24548](https://github.com/vllm-project/vllm/pull/24548)) by @sfeng33
* [Spec Decode] Efficient padded speculation ([#24539](https://github.com/vllm-project/vllm/pull/24539)) by @benchislett
* [Multi Modal][Performance] Fused Q,K's apply_rope into one ([#24511](https://github.com/vllm-project/vllm/pull/24511)) by @wwl2755
* Upgrade flashinfer to 0.3.1 ([#24470](https://github.com/vllm-project/vllm/pull/24470)) by @houseroad
* [Kernels] Enable DeepGEMM by default ([#24462](https://github.com/vllm-project/vllm/pull/24462)) by @bnellnm
* Enable conversion of multimodal models to pooling tasks ([#24451](https://github.com/vllm-project/vllm/pull/24451)) by @maxdebayser
* [Multi Modal] Add FA3 in VIT ([#24347](https://github.com/vllm-project/vllm/pull/24347)) by @wwl2755
* [FP8] Extend per-token-group quantization support to QuantFP8 ([#24342](https://github.com/vllm-project/vllm/pull/24342)) by @tahsintunan
* [Tests] fix initialization of kv hash in tests ([#24273](https://github.com/vllm-project/vllm/pull/24273)) by @mickaelseznec
* [Kernels] Overlap shared experts with combine instead of dispatch ([#24254](https://github.com/vllm-project/vllm/pull/24254)) by @bnellnm
* [Metrics] Hide deprecated metrics with gpu_ prefix ([#24245](https://github.com/vllm-project/vllm/pull/24245)) by @markmc
* [Misc] rename interval to max_recent_requests ([#24229](https://github.com/vllm-project/vllm/pull/24229)) by @andyxning
* [kv cache] update num_free_blocks in the end ([#24228](https://github.com/vllm-project/vllm/pull/24228)) by @andyxning
* [UT] enhance free kv cache block queue popleft_n ([#24220](https://github.com/vllm-project/vllm/pull/24220)) by @andyxning
* [Transform] Deterministic Hadacore Transforms ([#24106](https://github.com/vllm-project/vllm/pull/24106)) by @kylesayrs
* Update num_tokens_across_dp to use nccl instead of gloo ([#24105](https://github.com/vllm-project/vllm/pull/24105)) by @SageMoore
* [Kernels][DP/EP] Optimize Silu Kernel for R1 ([#24054](https://github.com/vllm-project/vllm/pull/24054)) by @elvircrn
* [Spec Decoding]Support Spec Decoding Metrics in DP Mode ([#24049](https://github.com/vllm-project/vllm/pull/24049)) by @wuhang2014
* [Hybrid Allocator] Support Pipeline Parallel ([#23974](https://github.com/vllm-project/vllm/pull/23974)) by @heheda12345
* Enable Allgather/ReduceScatter backend for NaiveAllToAll ([#23964](https://github.com/vllm-project/vllm/pull/23964)) by @wenscarl
* [Benchmark] Allow arbitrary headers to be passed to benchmarked endpoints ([#23937](https://github.com/vllm-project/vllm/pull/23937)) by @smarterclayton
* [P/D]`kv_output_aggregator` support heterogeneous ([#23917](https://github.com/vllm-project/vllm/pull/23917)) by @LCAIZJ
* [benchmark] add peak throughput metrics and plot ([#23867](https://github.com/vllm-project/vllm/pull/23867)) by @simon-mo
* [CLI] Use streaming in CLI chat and completion commands ([#23769](https://github.com/vllm-project/vllm/pull/23769)) by @simon-mo
* [Core/DBO][1/N] Add Dual-Batch Overlap mechanism to VLLM ([#23693](https://github.com/vllm-project/vllm/pull/23693)) by @SageMoore
* (doc): set cmake c++ compatible standard when building on MacOS CPU. ([#23483](https://github.com/vllm-project/vllm/pull/23483)) by @teekenl
* [EPLB] Add EPLB support for hunyuan_v1 ([#23078](https://github.com/vllm-project/vllm/pull/23078)) by @666even666
* [V1] Logits processor docs ([#22919](https://github.com/vllm-project/vllm/pull/22919)) by @afeldman-nm
* [EPLB] Support EPLB for Mixtral Model ([#22842](https://github.com/vllm-project/vllm/pull/22842)) by @rouchenzi
* [Chore] Cleanup guided namespace, move to structured outputs config ([#22772](https://github.com/vllm-project/vllm/pull/22772)) by @aarnphm
* fp8 kv cache support fix for torch.compile ([#22758](https://github.com/vllm-project/vllm/pull/22758)) by @maleksan85
* Fp8 paged attention update ([#22222](https://github.com/vllm-project/vllm/pull/22222)) by @xiao-llm
* [Structured Output][Refactor] Move `apply_grammar_bitmask()` method from `ModelRunner` to structured output utils ([#21999](https://github.com/vllm-project/vllm/pull/21999)) by @shen-shanshan
* [V1] feat:add engine v1 tracing ([#20372](https://github.com/vllm-project/vllm/pull/20372)) by @RichardoMrMu
* [USAGE] Improve error handling for weight initialization in Unquantized‚Ä¶ ([#20321](https://github.com/vllm-project/vllm/pull/20321)) by @koiker
* [KV offload][2/N] Introduce LRU-based CPU offloading management ([#20075](https://github.com/vllm-project/vllm/pull/20075)) by @orozery
* [KV offload][1/N] Introduce an offloading component ([#19848](https://github.com/vllm-project/vllm/pull/19848)) by @orozery

## Contributors

@2015aroras, @666even666, @Aidyn-A, @ApostaC, @AzizCode92, @ChenTaoyu-SJTU, @Concurrensee, @DarkLight1337, @Isotr0py, @Jason-CKY, @Josephasafg, @LCAIZJ, @LucasWilkinson, @MatthewBonanni, @NickLucche, @RichardoMrMu, @Rohan138, @SageMoore, @SamitHuang, @Sugar-zsg, @WoosukKwon, @ZJY0516, @Zerohertz, @aarnphm, @afeldman-nm, @ahao-anyscale, @alexm-redhat, @andyxning, @anmarques, @ant-yy, @bbartels, @benchislett, @bigPYJ1151, @bingchen-mi, @bnellnm, @cascade812, @cboss6, @chaojun-zhang, @chaunceyjiang, @chenfengjin, @co63oc, @cyang49, @czhu-cohere, @danielafrimi, @didier-durand, @dllehr-amd, @dolpm, @dongbo910220, @dongluw, @elvircrn, @elvischenv, @gaocegege, @gfinol, @gigit0000, @gshtras, @haoyangli-amd, @heheda12345, @hmellor, @houseroad, @huachenheli, @ignaciosica, @ihb2032, @jeejeelee, @jikunshang, @jvlunteren, @karan, @kebe7jun, @koiker, @kylesayrs, @lengrongfu, @lgeiger, @liangwen12year, @lianyiibo, @lumina37, @maleksan85, @markmc, @maxdebayser, @mgoin, @mickaelseznec, @minosfuture, @mmangkad, @nikhil-arm, @njhill, @noooop, @orozery, @pliops-daniels, @prashantgupta24, @punitvara, @qandrew, @qthequartermasterman, @robertgshaw2-redhat, @rouchenzi, @rozeappletree, @russellb, @rzabarazesh, @samanamp, @samzong, @sergiopaniego, @sfeng33, @shen-shanshan, @shijun-yin, @sighingnow, @simon-mo, @smarterclayton, @tahsintunan, @teekenl, @tomeras91, @toncao, @vadiklyutiy, @vllmellm, @wenscarl, @whx-sjtu, @windsonsea, @wuhang2014, @wwl2755, @xiao-llm, @xinyu-intel, @yankay, @yeqcharlotte, @yewentao256, @yma11, @youkaichao, @ywang96, @yyzxw, @zhewenl, @zhuohan123, @ziliangpeng, @zixi-qi, @zou3519

