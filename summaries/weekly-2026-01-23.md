# Weekly Release Notes for vllm-project/vllm (2026-01-23)

## What's Changed

### âœ¨ Features & Enhancements

* Add missing import of fused_topk to benchmark_moe ([#32784](https://github.com/vllm-project/vllm/pull/32784)) by @danisereb
* Support nccl fp8 communication ([#32760](https://github.com/vllm-project/vllm/pull/32760)) by @amirkl94
* Support custom URI schemes and trace handlers for profiler ([#32393](https://github.com/vllm-project/vllm/pull/32393)) by @diviramon
* [Feature] Add FIPS 140-3 compliant hash algorithm option for multimodal hashing ([#32386](https://github.com/vllm-project/vllm/pull/32386)) by @karanb192
* Add thread_n=64 support to Marlin MoE ([#32360](https://github.com/vllm-project/vllm/pull/32360)) by @mgoin
* [Feat] Support non-gated MoE with Marlin, NVFP4 CUTLASS, FP8, INT8, compressed-tensors ([#32257](https://github.com/vllm-project/vllm/pull/32257)) by @TomerBN-Nvidia
* support dynamic resolution image encoding for Nemotron Nano VL ([#32121](https://github.com/vllm-project/vllm/pull/32121)) by @netanel-haber
* [Feat] allow inplace loading lora ([#31326](https://github.com/vllm-project/vllm/pull/31326)) by @Jackmin801
* [Feature] Add --ssl-ciphers CLI argument for TLS cipher control ([#30937](https://github.com/vllm-project/vllm/pull/30937)) by @ricky-chaoju
* Add support for LoRA adapters in Nemotron-H models ([#30802](https://github.com/vllm-project/vllm/pull/30802)) by @danisereb
* Add llmcompressor fp8 kv-cache quant (per-tensor and per-attn_head) ([#30141](https://github.com/vllm-project/vllm/pull/30141)) by @eldarkurtic
* feat: spec decode with draft models ([#24322](https://github.com/vllm-project/vllm/pull/24322)) by @tomasruizt
* Support bge-m3 sparse embeddings and colbert embeddings ([#14526](https://github.com/vllm-project/vllm/pull/14526)) by @maxdebayser

### ðŸ› Bug Fixes

* [BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 op registration ([#32855](https://github.com/vllm-project/vllm/pull/32855)) by @fadara01
* [Bugfix] ModelScope is supported when downloading LORA models. ([#32844](https://github.com/vllm-project/vllm/pull/32844)) by @AuYang261
* [Bugfix] Fix potential EAGLE spec decode segfault during graph capture ([#32818](https://github.com/vllm-project/vllm/pull/32818)) by @mawong-amd
* [Bugfix][Attention] Explicitly report support for kv_cache_dtype bfloat16 ([#32795](https://github.com/vllm-project/vllm/pull/32795)) by @MatthewBonanni
* [Bugfix] Fix Whisper/encoder-decoder GPU memory leak ([#32789](https://github.com/vllm-project/vllm/pull/32789)) by @NickLucche
* [Bugfix] Force using spawn multiprocess method when it's the WSL platform ([#32749](https://github.com/vllm-project/vllm/pull/32749)) by @jasonyanwenl
* [bugfix] Aria model ([#32727](https://github.com/vllm-project/vllm/pull/32727)) by @divakar-amd
* [Bugfix] Suppress log on non-ROCm platform ([#32703](https://github.com/vllm-project/vllm/pull/32703)) by @tjtanaa
* [Bugfix] fix the ima issue of qwen-vit ([#32687](https://github.com/vllm-project/vllm/pull/32687)) by @JJJYmmm
* [Bugfix] Fix Nemotron-Nano-v2-vlm static resolution ([#32682](https://github.com/vllm-project/vllm/pull/32682)) by @netanel-haber
* [Bugfix] Support HF sharded weights for Mistral3/Pixtral models ([#32673](https://github.com/vllm-project/vllm/pull/32673)) by @ricky-chaoju
* [Bugfix] Fix the  fp8_mqa_logits dim mismatch ([#32652](https://github.com/vllm-project/vllm/pull/32652)) by @chaunceyjiang
* [Bugfix] Fix Off-by-one error in _num_tokens_to_min_blocks calculation ([#32603](https://github.com/vllm-project/vllm/pull/32603)) by @lingebeng
* [Bugfix] Fix GLM-ASR audio encoder RoPE dim ([#32540](https://github.com/vllm-project/vllm/pull/32540)) by @Isotr0py
* [BUGFIX] Fix `test_mla_backends.py`. Scale MLA projection weights to prevent numerical instability  ([#32529](https://github.com/vllm-project/vllm/pull/32529)) by @vadiklyutiy
* [Bugfix] Add OOT backend option ([#32471](https://github.com/vllm-project/vllm/pull/32471)) by @iboiko-habana
* [BugFix] Fix embed_input_ids argument error of QwenVLForConditionalGeneration ([#32462](https://github.com/vllm-project/vllm/pull/32462)) by @honglyua-il
* [Bugfix] Fix ROCm dockerfiles ([#32447](https://github.com/vllm-project/vllm/pull/32447)) by @tjtanaa
* [Bug] Add TPU backend option ([#32438](https://github.com/vllm-project/vllm/pull/32438)) by @vanbasten23
* [BUGFIX]  Fix degenerate strides in TRTLLM query tensors for FlashInfer backend. Fixes issue #32353 ([#32417](https://github.com/vllm-project/vllm/pull/32417)) by @vadiklyutiy
* [BugFix] Fix TRT-LLM NVFP4 DP/EP ([#32349](https://github.com/vllm-project/vllm/pull/32349)) by @jiahanc
* [Bugfix] Refactor to support DP parallel in R3 ([#32306](https://github.com/vllm-project/vllm/pull/32306)) by @xhx1022
* [Bugfix] Fix Granite Vision / Don't use Siglip Pooling Head Nested Models by Default  ([#32299](https://github.com/vllm-project/vllm/pull/32299)) by @alex-jw-brooks
* [Bugfix] [DeepSeek-V3.2] fix sparse_attn_indexer padding ([#32175](https://github.com/vllm-project/vllm/pull/32175)) by @kebe7jun
* [Bugfix] Fix byte fallback handling when using outlines  ([#31391](https://github.com/vllm-project/vllm/pull/31391)) by @Alnusjaponica
* [bugfix] Fix online serving crash when text type response_format is received ([#26822](https://github.com/vllm-project/vllm/pull/26822)) by @cjackal

### âš¡ï¸ Performance

* [Perf] Create TMA-aligned input scale tensor for DeepGemm on Hopper ([#32619](https://github.com/vllm-project/vllm/pull/32619)) by @xyang16
* [Performance] Improve Triton prefill attention kernel's performance  ([#32403](https://github.com/vllm-project/vllm/pull/32403)) by @Isotr0py
* [Perf] Only clone when needed for `moe_permute` ([#32273](https://github.com/vllm-project/vllm/pull/32273)) by @yewentao256

### ðŸ¤– Model Support

* [Model] Extend `collect_children` and `no_init_weights` contexts ([#32757](https://github.com/vllm-project/vllm/pull/32757)) by @DarkLight1337
* [Model] Use context managers for encoder- and LM-only mode ([#32605](https://github.com/vllm-project/vllm/pull/32605)) by @DarkLight1337
* [Model] Remove the unnecessary dtype conversion in MiniCPM ([#32523](https://github.com/vllm-project/vllm/pull/32523)) by @gcanlin
* [Model] Support Step1 Model ([#32511](https://github.com/vllm-project/vllm/pull/32511)) by @randzero
* [Model] Add Eagle2.5-8B Vision-Language Model support   ([#32456](https://github.com/vllm-project/vllm/pull/32456)) by @George-Polya
* [Model] Molmo2: Enable quantized weight mapping for vision backbone ([#32385](https://github.com/vllm-project/vllm/pull/32385)) by @George-Polya
* [Model] Add Step3vl 10b ([#32329](https://github.com/vllm-project/vllm/pull/32329)) by @ltd0924

### ðŸ”Œ Hardware & Backend

* [ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallback for ROCm ([#32835](https://github.com/vllm-project/vllm/pull/32835)) by @AndreasKaratzas
* [ROCm][CI] fix get_valid_backends ([#32787](https://github.com/vllm-project/vllm/pull/32787)) by @divakar-amd
* [ROCm] fix import for on_gfx9 ([#32783](https://github.com/vllm-project/vllm/pull/32783)) by @divakar-amd
* [ROCm][CI] Lower Acceptance Len Threshold For test_draft_model_quantization ([#32731](https://github.com/vllm-project/vllm/pull/32731)) by @micah-wil
* [ROCm][CI] Remove DS async eplb accuracy test from AMD CI ([#32717](https://github.com/vllm-project/vllm/pull/32717)) by @micah-wil
* [XPU]Support AgRsAll2AllManager on XPU device ([#32654](https://github.com/vllm-project/vllm/pull/32654)) by @ys950902
* [ROCm][CI] Skip Qwen3-30B-A3B-MXFP4A16 Eval Test On Non-CUDA Platforms ([#32460](https://github.com/vllm-project/vllm/pull/32460)) by @micah-wil
* [ROCm][CI] Enable AITER Unified Attention On ROCm For gpt-oss Test ([#32431](https://github.com/vllm-project/vllm/pull/32431)) by @micah-wil
* [ROCm][CI] Add ROCm attention backend support for EAGLE DP tests ([#32363](https://github.com/vllm-project/vllm/pull/32363)) by @AndreasKaratzas
* [ROCm][CI] Fix AITER test flakiness by using explicit attention backend ([#32346](https://github.com/vllm-project/vllm/pull/32346)) by @AndreasKaratzas
* [ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp ([#29287](https://github.com/vllm-project/vllm/pull/29287)) by @ganyi1996ppo
* [TPU][Core] Enable Pipeline Parallelism on TPU backend ([#28506](https://github.com/vllm-project/vllm/pull/28506)) by @Chenyaaang

### âš™ï¸ Refactoring & Core

* [Frontend] add prompt_cache_key for openresponses ([#32824](https://github.com/vllm-project/vllm/pull/32824)) by @chaunceyjiang
* [Refactor] Remove unused tpu files ([#32610](https://github.com/vllm-project/vllm/pull/32610)) by @yewentao256
* [Frontend] Score entrypoint support data_1 & data_2 and queries & documents as inputs ([#32577](https://github.com/vllm-project/vllm/pull/32577)) by @noooop
* [Frontend][2/n] Make pooling entrypoints request schema consensus | ChatRequest ([#32574](https://github.com/vllm-project/vllm/pull/32574)) by @noooop
* [Frontend] Add render endpoints for prompt preprocessing ([#32473](https://github.com/vllm-project/vllm/pull/32473)) by @hyeongyun0916
* [Refactor] Remove unused file `pallas_kv_cache_update.py` ([#32433](https://github.com/vllm-project/vllm/pull/32433)) by @yewentao256
* [Core] Cleanup shm based object store on engine shutdown ([#32429](https://github.com/vllm-project/vllm/pull/32429)) by @walterbm
* [Frontend][1/n] Make pooling entrypoints request schema consensus | CompletionRequest  ([#32395](https://github.com/vllm-project/vllm/pull/32395)) by @noooop
* [Refactor] Remove unused cutlass moe problem size function ([#32047](https://github.com/vllm-project/vllm/pull/32047)) by @yewentao256
* [Kernel] Add topk_sigmoid kernel ([#31246](https://github.com/vllm-project/vllm/pull/31246)) by @xyang16
* [Core] Whisper support `torch.compile` ([#30385](https://github.com/vllm-project/vllm/pull/30385)) by @NickLucche
* [Frontend] Introduce Renderer for processing chat messages (using `ModelConfig`) ([#30200](https://github.com/vllm-project/vllm/pull/30200)) by @DarkLight1337
* [Refactor] Make FP8 Linear Ops use kernel abstraction ([#27814](https://github.com/vllm-project/vllm/pull/27814)) by @vllmellm

### ðŸ”§ Build, CI & Testing

* [CI] refactor release pipeline config into groups ([#32833](https://github.com/vllm-project/vllm/pull/32833)) by @Harry-Chen
* [CI][amd] Revert NIXL connector change to avoid crash ([#32570](https://github.com/vllm-project/vllm/pull/32570)) by @qli88
* [CI] Move Distributed Tests from H200 -> H100 ([#32555](https://github.com/vllm-project/vllm/pull/32555)) by @robertgshaw2-redhat
* [build] fix cu130 related release pipeline steps and publish as nightly image ([#32522](https://github.com/vllm-project/vllm/pull/32522)) by @Harry-Chen
* [CI] Fix OOM in Hopper Fusion E2E Tests (H100) ([#32489](https://github.com/vllm-project/vllm/pull/32489)) by @LucasWilkinson
* [CI][Attention] Add more CI dependencies for attention tests ([#32487](https://github.com/vllm-project/vllm/pull/32487)) by @MatthewBonanni
* [CI] Add Helion as an optional dependency ([#32482](https://github.com/vllm-project/vllm/pull/32482)) by @gmagogsfm
* [CI] Update deepgemm to newer version ([#32479](https://github.com/vllm-project/vllm/pull/32479)) by @yewentao256
* [CI][AMD] Skip test_permute_cols since the kernel is not used and not built for ROCm ([#32444](https://github.com/vllm-project/vllm/pull/32444)) by @rasmith
* [CI] Fix LM Eval Large Models (H100) ([#32423](https://github.com/vllm-project/vllm/pull/32423)) by @MatthewBonanni
* [CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_fused ([#32408](https://github.com/vllm-project/vllm/pull/32408)) by @mawong-amd
* [CI] Implement uploading to PyPI and GitHub in the release pipeline, enable release image building for CUDA 13.0 ([#31032](https://github.com/vllm-project/vllm/pull/31032)) by @Harry-Chen
* [CI] Breakup h200 tests ([#30499](https://github.com/vllm-project/vllm/pull/30499)) by @LucasWilkinson

### ðŸ“š Documentation

* [Docs] Remove outdated async_scheduling limitation with speculative decoding ([#32775](https://github.com/vllm-project/vllm/pull/32775)) by @ikaadil
* [Doc] Update docs for MM model development with context usage ([#32691](https://github.com/vllm-project/vllm/pull/32691)) by @DarkLight1337
* [Docs] Fix GitHub handle in governance process ([#32582](https://github.com/vllm-project/vllm/pull/32582)) by @pacoxu
* [Doc] [ROCm] Update ROCm getting started doc ([#32580](https://github.com/vllm-project/vllm/pull/32580)) by @tjtanaa
* [Doc] Correct comment for _jobs dict in OffloadingConnectorWorker ([#32556](https://github.com/vllm-project/vllm/pull/32556)) by @DemingCheng
* [Docs][Governance] Add @robertshaw2-redhat to lead maintainers group ([#32498](https://github.com/vllm-project/vllm/pull/32498)) by @simon-mo
* docs: prefix caching seems quite outdated ([#28784](https://github.com/vllm-project/vllm/pull/28784)) by @longregen

### ðŸ“¦ Miscellaneous

* [MISC] Add .cursor to .gitignore ([#32868](https://github.com/vllm-project/vllm/pull/32868)) by @vadiklyutiy
* [Hardware][AMD][CI][Bugfix] Fix regressions from deprecated env vars ([#32837](https://github.com/vllm-project/vllm/pull/32837)) by @mawong-amd
* [Model Runner V2] Do not error on attention backends ([#32820](https://github.com/vllm-project/vllm/pull/32820)) by @WoosukKwon
* [Deprecation] Remove deprecated environment variables ([#32812](https://github.com/vllm-project/vllm/pull/32812)) by @yewentao256
* [Model Runner V2] Refactor Prompt Logprobs ([#32811](https://github.com/vllm-project/vllm/pull/32811)) by @WoosukKwon
* [FlashMLA] Update FlashMLA to expose new arguments ([#32810](https://github.com/vllm-project/vllm/pull/32810)) by @LucasWilkinson
* [torch.compile] Improve Cold Start for MoEs ([#32805](https://github.com/vllm-project/vllm/pull/32805)) by @zou3519
* [ModelRunner V2] Don't pin reused flashinfer tensors ([#32799](https://github.com/vllm-project/vllm/pull/32799)) by @njhill
* [Misc] Add Helion version check to collect_env ([#32797](https://github.com/vllm-project/vllm/pull/32797)) by @gmagogsfm
* [Misc] Log vLLM logo when starting server ([#32796](https://github.com/vllm-project/vllm/pull/32796)) by @njhill
* [Model Runner V2] Minor refactor for `compute_slot_mappings` ([#32794](https://github.com/vllm-project/vllm/pull/32794)) by @WoosukKwon
* [CPU Backend] [Perf] Accelerate tensor-parallel/data-parallel inference across NUMA domains on Arm ([#32792](https://github.com/vllm-project/vllm/pull/32792)) by @fadara01
* Cleanup some huggingface_hub-related stuff ([#32788](https://github.com/vllm-project/vllm/pull/32788)) by @Wauplin
* [Llama.py -> mistral.py] Extract mistral-only relevant code into separate file ([#32780](https://github.com/vllm-project/vllm/pull/32780)) by @patrickvonplaten
* [Misc] Replace urllib's `urlparse` with urllib3's `parse_url` ([#32746](https://github.com/vllm-project/vllm/pull/32746)) by @Isotr0py
* [PluggableLayer][1/N] Define PluggableLayer (Fix ci) ([#32744](https://github.com/vllm-project/vllm/pull/32744)) by @whx-sjtu
* [Documentation] Fix typo in `docs/design/torch_compile_multimodal.md` ([#32741](https://github.com/vllm-project/vllm/pull/32741)) by @Lucaskabela
* Revert "[PluggableLayer][1/N] Define PluggableLayer" ([#32725](https://github.com/vllm-project/vllm/pull/32725)) by @robertgshaw2-redhat
* [Benchmark] Don't default to `temperature==0` in `vllm bench serve` ([#32723](https://github.com/vllm-project/vllm/pull/32723)) by @njhill
* [Model Runner V2] Support FLASHINFER_MLA backend ([#32709](https://github.com/vllm-project/vllm/pull/32709)) by @WoosukKwon
* [Misc] Omit "disable NCCL for DP sync" startup log when not applicable ([#32707](https://github.com/vllm-project/vllm/pull/32707)) by @njhill
* [Cleanup] Move scheduler `get_routed_experts` logic to separate method ([#32706](https://github.com/vllm-project/vllm/pull/32706)) by @njhill
* [Quantization][Deprecation] Remove RTN ([#32697](https://github.com/vllm-project/vllm/pull/32697)) by @robertgshaw2-redhat
* [5/N] Initialize MM components in context managers (Q-Z) ([#32695](https://github.com/vllm-project/vllm/pull/32695)) by @DarkLight1337
* [Quantization][Deprecation] Deprecate HQQ ([#32681](https://github.com/vllm-project/vllm/pull/32681)) by @robertgshaw2-redhat
* [Quantization][Deprecation] Remove `DeepSpeedFp8` ([#32679](https://github.com/vllm-project/vllm/pull/32679)) by @robertgshaw2-redhat
* [Misc] Bump opencv-python dependecy version to 4.13 ([#32668](https://github.com/vllm-project/vllm/pull/32668)) by @Isotr0py
* [bench] add start_times field to vllm bench serve json result ([#32667](https://github.com/vllm-project/vllm/pull/32667)) by @kebe7jun
* [4/N] Initialize MM components in context managers (M-P) ([#32663](https://github.com/vllm-project/vllm/pull/32663)) by @DarkLight1337
* [Metrics] Complete removal of deprecated vllm:time_per_output_token_seconds metric ([#32661](https://github.com/vllm-project/vllm/pull/32661)) by @carlory
* [3/N] Initialize MM components in context managers (I-L) ([#32650](https://github.com/vllm-project/vllm/pull/32650)) by @DarkLight1337
* [2/N] Initialize MM components in context managers (E-H) ([#32641](https://github.com/vllm-project/vllm/pull/32641)) by @DarkLight1337
* [Model Runner V2] Skip kernel launch for penalties & logit_bias ([#32634](https://github.com/vllm-project/vllm/pull/32634)) by @WoosukKwon
* [1/N] Initialize MM components in context managers (A-D) ([#32632](https://github.com/vllm-project/vllm/pull/32632)) by @DarkLight1337
* [Model Runner V2] Decouple temperature from penalties ([#32629](https://github.com/vllm-project/vllm/pull/32629)) by @WoosukKwon
* [Model Runner V2] Refactor get_cudagraph_and_dp_padding ([#32625](https://github.com/vllm-project/vllm/pull/32625)) by @WoosukKwon
* [Model Runner V2] Initialized communication buffer for DP ([#32624](https://github.com/vllm-project/vllm/pull/32624)) by @WoosukKwon
* [Attention][MLA] Make FLASHINFER_MLA the default MLA backend on Blackwell, and TRTLLM the default prefill ([#32615](https://github.com/vllm-project/vllm/pull/32615)) by @MatthewBonanni
* [Misc] Remove unused ModelKeys ([#32608](https://github.com/vllm-project/vllm/pull/32608)) by @jeejeelee
* [EC Connector] Optimize remote cache check in scheduler ([#32585](https://github.com/vllm-project/vllm/pull/32585)) by @knlnguyen1802
* [Model Runner V2] Refactor `update_states` ([#32562](https://github.com/vllm-project/vllm/pull/32562)) by @WoosukKwon
* [CI/Build] Fix dependency conflict between model-hosting-container-standards and starlette ([#32560](https://github.com/vllm-project/vllm/pull/32560)) by @DanielMe
* [Model Runner V2] Support VLM ([#32546](https://github.com/vllm-project/vllm/pull/32546)) by @WoosukKwon
* Enable Eagle3 speculative decoding for Pixtral (LlavaForConditionalGeneration) ([#32542](https://github.com/vllm-project/vllm/pull/32542)) by @gopalsarda
* [Model Runner V2] Minor optimization for eagle input processing ([#32535](https://github.com/vllm-project/vllm/pull/32535)) by @WoosukKwon
* [Model Runner V2] Refactor `dummy_run` ([#32533](https://github.com/vllm-project/vllm/pull/32533)) by @WoosukKwon
* [Model Runner V2] Move mrope_positions buffer to MRopeState ([#32532](https://github.com/vllm-project/vllm/pull/32532)) by @WoosukKwon
* [CI/Build] Use Common Event Map Fixture in Harmony / MCP Server Tests ([#32531](https://github.com/vllm-project/vllm/pull/32531)) by @alex-jw-brooks
* [UX] Default api_server_count to dp_size if not specified ([#32525](https://github.com/vllm-project/vllm/pull/32525)) by @tlrmchlsmth
* [FlashMLA] Update FlashMLA ([#32491](https://github.com/vllm-project/vllm/pull/32491)) by @LucasWilkinson
* "refactor: refactor_repeated_interfaces" ([#32486](https://github.com/vllm-project/vllm/pull/32486)) by @tom-zju
* Revert "[Attention][MLA] Make `FLASHINFER_MLA` the default MLA backenâ€¦ ([#32484](https://github.com/vllm-project/vllm/pull/32484)) by @MatthewBonanni
* [Chore] Replace swish with silu ([#32459](https://github.com/vllm-project/vllm/pull/32459)) by @DarkLight1337
* apply _validate_input to MistralTokenizer token-id chat prompts ([#32448](https://github.com/vllm-project/vllm/pull/32448)) by @vanshilshah97
* [LoRA] Update LoRA expand kernel heuristic ([#32425](https://github.com/vllm-project/vllm/pull/32425)) by @xyang16
* [EPLB][BugFix]Possible deadlock fix ([#32418](https://github.com/vllm-project/vllm/pull/32418)) by @ilmarkov
* [MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priority ([#32414](https://github.com/vllm-project/vllm/pull/32414)) by @robertgshaw2-redhat
* [Misc] Fix typo: seperator -> separator in flashmla_sparse.py ([#32411](https://github.com/vllm-project/vllm/pull/32411)) by @T1mn
* [responsesAPI] allow tuning include_stop_str_in_output ([#32383](https://github.com/vllm-project/vllm/pull/32383)) by @qandrew
* [Nixl][Bugfix] Track `nixl_num_kv_expired_reqs` metric in Prometheus ([#32340](https://github.com/vllm-project/vllm/pull/32340)) by @NickLucche
* [PluggableLayer][1/N] Define PluggableLayer ([#32331](https://github.com/vllm-project/vllm/pull/32331)) by @whx-sjtu
* Upgrade transformers-4.57.5 ([#32287](https://github.com/vllm-project/vllm/pull/32287)) by @huydhn
* fix(rocm): Enable non-gated MoE (is_act_and_mul=False) support on ROCm ([#32244](https://github.com/vllm-project/vllm/pull/32244)) by @rabi
* fp8 online quant: split out Fp8OnlineLinearMethod ([#32189](https://github.com/vllm-project/vllm/pull/32189)) by @vkuzo
* [MoE Refactor] Move Test Impl into Test Dirs ([#32129](https://github.com/vllm-project/vllm/pull/32129)) by @robertgshaw2-redhat
* [Cleanup] Remove unused `KVConnectorModelRunnerMixin` methods ([#32077](https://github.com/vllm-project/vllm/pull/32077)) by @njhill
* Added qwen3 vision language moe support for speculative decoding ([#32048](https://github.com/vllm-project/vllm/pull/32048)) by @shanjiaz
* Test: added acceptance length tests ([#32030](https://github.com/vllm-project/vllm/pull/32030)) by @rahul-tuli
* [MoE Refactor] Move `select_experts` from `FusedMoEQuantMethod` -> `FusedMoE` ([#31996](https://github.com/vllm-project/vllm/pull/31996)) by @bnellnm
* [Misc][BE] Turn on strict type coverage for vllm/compilation ([#31756](https://github.com/vllm-project/vllm/pull/31756)) by @Lucaskabela
* Use the same memory for workspace13 and fused_output. ([#31531](https://github.com/vllm-project/vllm/pull/31531)) by @halyavin
* [CI/Build][Docker] Add centralized version manifest for Docker builds ([#31492](https://github.com/vllm-project/vllm/pull/31492)) by @mritunjaysharma394
* [GLM-4.7] GLM Model support for GLM-Lite ([#31386](https://github.com/vllm-project/vllm/pull/31386)) by @zRzRzRzRzRzRzR
* Bump Flashinfer to v0.6.1 ([#30993](https://github.com/vllm-project/vllm/pull/30993)) by @elvischenv
* OffloadingConnector: Support kernel_block_size != block_size ([#30692](https://github.com/vllm-project/vllm/pull/30692)) by @orozery
* [MoE Refactor] Separate Router into OO Classes ([#30623](https://github.com/vllm-project/vllm/pull/30623)) by @bnellnm
* Enable Cross layers KV cache layout at NIXL Connector ([#30207](https://github.com/vllm-project/vllm/pull/30207)) by @liranschour
* [Misc] Remove pad_for_cudagraphs from config ([#30143](https://github.com/vllm-project/vllm/pull/30143)) by @LucasWilkinson
* Atomics Reduce Counting Optimization for SplitK Skinny GEMMs. ([#29843](https://github.com/vllm-project/vllm/pull/29843)) by @amd-hhashemi
* OffloadingConnector: Prevent redundant loads ([#29087](https://github.com/vllm-project/vllm/pull/29087)) by @orozery
* [Models] Lfm2Moe: minor name changes for resolving lora conflicts ([#29063](https://github.com/vllm-project/vllm/pull/29063)) by @paulpak58
* [AMD][ROCm] MoRI EP: a high-performance all2all backend ([#28664](https://github.com/vllm-project/vllm/pull/28664)) by @alexsun07
* [AOT compilation] support torch.compile inductor artifacts in VllmCompiledFunction ([#25205](https://github.com/vllm-project/vllm/pull/25205)) by @dolpm

## Contributors

@Alnusjaponica, @AndreasKaratzas, @AuYang261, @Chenyaaang, @DanielMe, @DarkLight1337, @DemingCheng, @George-Polya, @Harry-Chen, @Isotr0py, @JJJYmmm, @Jackmin801, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @NickLucche, @T1mn, @TomerBN-Nvidia, @Wauplin, @WoosukKwon, @alex-jw-brooks, @alexsun07, @amd-hhashemi, @amirkl94, @bnellnm, @carlory, @chaunceyjiang, @cjackal, @danisereb, @divakar-amd, @diviramon, @dolpm, @eldarkurtic, @elvischenv, @fadara01, @ganyi1996ppo, @gcanlin, @gmagogsfm, @gopalsarda, @halyavin, @honglyua-il, @huydhn, @hyeongyun0916, @iboiko-habana, @ikaadil, @ilmarkov, @jasonyanwenl, @jeejeelee, @jiahanc, @karanb192, @kebe7jun, @knlnguyen1802, @lingebeng, @liranschour, @longregen, @ltd0924, @mawong-amd, @maxdebayser, @mgoin, @micah-wil, @mritunjaysharma394, @netanel-haber, @njhill, @noooop, @orozery, @pacoxu, @patrickvonplaten, @paulpak58, @qandrew, @qli88, @rabi, @rahul-tuli, @randzero, @rasmith, @ricky-chaoju, @robertgshaw2-redhat, @shanjiaz, @simon-mo, @tjtanaa, @tlrmchlsmth, @tom-zju, @tomasruizt, @vadiklyutiy, @vanbasten23, @vanshilshah97, @vkuzo, @vllmellm, @walterbm, @whx-sjtu, @xhx1022, @xyang16, @yewentao256, @ys950902, @zRzRzRzRzRzRzR, @zou3519

