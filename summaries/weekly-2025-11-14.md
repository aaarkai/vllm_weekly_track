# Weekly Release Notes for vllm-project/vllm (2025-11-14)

## What's Changed

### ‚ú® Features & Enhancements

* Support DeepEP for Kimi-k2-thinking through enabling gemm selection for compressed-tensor marlin wna16 ([#28574](https://github.com/vllm-project/vllm/pull/28574)) by @luccafong
* Add NUMA node validation for CPU thread binding ([#28555](https://github.com/vllm-project/vllm/pull/28555)) by @usberkeley
* Add Zurich vLLM Meetup ([#28488](https://github.com/vllm-project/vllm/pull/28488)) by @mgoin
* Support all interleaved layer types ([#28485](https://github.com/vllm-project/vllm/pull/28485)) by @sarckk
* Add @markmc to CODEOWNERS for Observability ([#28457](https://github.com/vllm-project/vllm/pull/28457)) by @markmc
* [Feature] Add env var `VLLM_MOE_USE_DEEP_GEMM` ([#28422](https://github.com/vllm-project/vllm/pull/28422)) by @yewentao256
* Add request timeout override for multi-turn benchmarks ([#28386](https://github.com/vllm-project/vllm/pull/28386)) by @segevido
* Add @tjtanaa to codeowner for ROCm and multi-modal ([#28360](https://github.com/vllm-project/vllm/pull/28360)) by @tjtanaa
* add cpu option for p/d in nixl_connector ([#28356](https://github.com/vllm-project/vllm/pull/28356)) by @ZhengHongming888
* [Feature] Allow configuring FlashInfer workspace size ([#28269](https://github.com/vllm-project/vllm/pull/28269)) by @maxyanghu
* [Feature] Default `ignore_eos` True for `random` dataset ([#28227](https://github.com/vllm-project/vllm/pull/28227)) by @yewentao256
* Add runai model streamer e2e test for GCS ([#28079](https://github.com/vllm-project/vllm/pull/28079)) by @amacaskill
* [Feat] Drop-in Torch CUDA Profiler ([#27841](https://github.com/vllm-project/vllm/pull/27841)) by @benchislett
* [Feature] Refactor batch invariant fp8 DeepGEMM ([#27606](https://github.com/vllm-project/vllm/pull/27606)) by @yewentao256

### üêõ Bug Fixes

* Fix `get_num_experts` when config sets it explicitly to `None` ([#28652](https://github.com/vllm-project/vllm/pull/28652)) by @hmellor
* [Bugfix] Fix FPS value type for Qwen2.5-Omni video processing ([#28630](https://github.com/vllm-project/vllm/pull/28630)) by @faaany
* [BugFix] DeepSeek-OCR: apply NoRepeatNGramLogitsProcessor to greedy path ([#28617](https://github.com/vllm-project/vllm/pull/28617)) by @YuanpingSong
* Fix: Correctly filter special tokens in benchmark_prefix_caching ([#28615](https://github.com/vllm-project/vllm/pull/28615)) by @dw2761
* [BugFix][ROCm] Fix `get_cu_count` missing variable error ([#28608](https://github.com/vllm-project/vllm/pull/28608)) by @ganyi1996ppo
* [BugFix] Fix type error when assign a trition kernel tensor to a torch.nn.Parameter ([#28603](https://github.com/vllm-project/vllm/pull/28603)) by @liuzijing2014
* [BugFix] Fix `mm_encoder_attn_backend` arg type checking ([#28599](https://github.com/vllm-project/vllm/pull/28599)) by @njhill
* [Bugfix] Fix SM100 gpt-oss regression due to faulty attn sink support ([#28561](https://github.com/vllm-project/vllm/pull/28561)) by @mgoin
* [BugFix] Priority scheduling and spec tokens preemption ([#28558](https://github.com/vllm-project/vllm/pull/28558)) by @andylolu2
* Fix pre-commit (and XPU) on `main` ([#28556](https://github.com/vllm-project/vllm/pull/28556)) by @hmellor
* [Bugfix] Fix gpt_oss packed_modules_mapping ([#28536](https://github.com/vllm-project/vllm/pull/28536)) by @jeejeelee
* [Bugfix] Eliminate tuple inputs to submodules in graph partitioning ([#28533](https://github.com/vllm-project/vllm/pull/28533)) by @gmagogsfm
* [bugfix] correct local_chunk_len for DCP in reorg_kvcache with long context ([#28526](https://github.com/vllm-project/vllm/pull/28526)) by @pisceskkk
* [BugFix] Ensure `EngineArgs.create_engine_config` is idempotent ([#28515](https://github.com/vllm-project/vllm/pull/28515)) by @njhill
* Fix io processor pooling  #28273 ([#28484](https://github.com/vllm-project/vllm/pull/28484)) by @baonudesifeizhai
* [BugFix] Fix Failing Ruff Check ([#28469](https://github.com/vllm-project/vllm/pull/28469)) by @jvlunteren
* [BugFix] Add test_outputs.py to CI pipeline ([#28466](https://github.com/vllm-project/vllm/pull/28466)) by @usberkeley
* Fix Fused MoE LoRA Triton kernel bug ([#28450](https://github.com/vllm-project/vllm/pull/28450)) by @chaojun-zhang
* [BugFix] Fix Siglip2Attention on XPU ([#28448](https://github.com/vllm-project/vllm/pull/28448)) by @faaany
* [BugFix] Add fallback path in `apply_rotary_pos_emb_flashattn` for non-cuda platforms ([#28447](https://github.com/vllm-project/vllm/pull/28447)) by @faaany
* [Bugfix] fix kimi-linear crash ([#28445](https://github.com/vllm-project/vllm/pull/28445)) by @ZJY0516
* [BugFix] Fix RuntimeError in PixtralHFAttention on CPU/XPU ([#28444](https://github.com/vllm-project/vllm/pull/28444)) by @faaany
* [Bugfix] Fix max image size for PaddleOCR-VL ([#28442](https://github.com/vllm-project/vllm/pull/28442)) by @ywang96
* [Bugfix] Fix Stream Sync for Shared Expert Overlap ([#28430](https://github.com/vllm-project/vllm/pull/28430)) by @robertgshaw2-redhat
* [Bugfix] Disable shared expert overlap if Marlin MoE is used ([#28410](https://github.com/vllm-project/vllm/pull/28410)) by @mgoin
* [BugFix] 'DeepseekV2Config' object has no attribute 'use_mla'`  ([#28387](https://github.com/vllm-project/vllm/pull/28387)) by @faaany
* [Bugfix][EPLB] Disabled shared expert overlap when EPLB is enabled ([#28377](https://github.com/vllm-project/vllm/pull/28377)) by @SageMoore
* [Fix] optimize visual token mask with caching and multi-token support ([#28374](https://github.com/vllm-project/vllm/pull/28374)) by @bo-ke
* [Bugfix] Fix persistent_masked_m_silu_mul_quant tests ([#28366](https://github.com/vllm-project/vllm/pull/28366)) by @varun-sundar-rabindranath
* [bugfix] fix siglip batch text output error ([#28365](https://github.com/vllm-project/vllm/pull/28365)) by @piood
* [Bugfix] Update device name for H200 detection ([#28349](https://github.com/vllm-project/vllm/pull/28349)) by @robertgshaw2-redhat
* fix: close issue 28338 by fixed python version ([#28339](https://github.com/vllm-project/vllm/pull/28339)) by @yihong0618
* Fix rotary embedding benchmark script ([#28323](https://github.com/vllm-project/vllm/pull/28323)) by @xyang16
* [bugfix] support eagle with lora cudagraph specialization ([#28318](https://github.com/vllm-project/vllm/pull/28318)) by @gnovack
* [Bugfix] Spec decode + structured output + spec model max len edge case ([#28298](https://github.com/vllm-project/vllm/pull/28298)) by @andylolu2
* [Bugfix] Adjust Marlin CUDA arch selection to 8.0+PTX;9.0+PTX ([#28294](https://github.com/vllm-project/vllm/pull/28294)) by @mgoin
* [fix] Revert "fixing mm placeholder replacement issue with gemma3" ([#28285](https://github.com/vllm-project/vllm/pull/28285)) by @khluu
* Fix issues from #28242 ([#28257](https://github.com/vllm-project/vllm/pull/28257)) by @hmellor
* [BugFix] Fix DeepGEMM over-allocating workspace ([#28254](https://github.com/vllm-project/vllm/pull/28254)) by @LucasWilkinson
* [BugFix] Avoid calling KV connector layer APIs when metadata is unset ([#28253](https://github.com/vllm-project/vllm/pull/28253)) by @sdavidbd
* [BugFix] Fix cu_num_generated_tokens slicing logic in LogprobsLists.slice() method ([#28214](https://github.com/vllm-project/vllm/pull/28214)) by @usberkeley
* [Bugfix] Prevent crash on empty grammar string ([#28210](https://github.com/vllm-project/vllm/pull/28210)) by @tjandy98
* [Bugfix] fix qwen3-next crash ([#28202](https://github.com/vllm-project/vllm/pull/28202)) by @ZJY0516
* [Bugfix] Fix and add tests for GptOss reasoning parser ([#28000](https://github.com/vllm-project/vllm/pull/28000)) by @benchislett
* [Bugfix] Fix test fused quant layernorm tests ([#27865](https://github.com/vllm-project/vllm/pull/27865)) by @ElizaWszola
* [Bugfix] [CPU] bump torch to 2.9.0 for Darwin to fix segmentation fault ([#27791](https://github.com/vllm-project/vllm/pull/27791)) by @kebe7jun
* [BugFix]: --enable-lora with model granite-4.0-micro crash ([#27733](https://github.com/vllm-project/vllm/pull/27733)) by @yyzxw
* [BugFix] Graceful handling of torch symm mem errors. ([#27671](https://github.com/vllm-project/vllm/pull/27671)) by @ilmarkov
* [Bugfix] Use latency MOE backend as default for Flashinfer and other misc fixes ([#27439](https://github.com/vllm-project/vllm/pull/27439)) by @pavanimajety
* [Bugfix] Ensure calculated KV scales are applied in attention. ([#27232](https://github.com/vllm-project/vllm/pull/27232)) by @adabeyta
* [Bugfix] Fix validate model input for decoder models ([#27099](https://github.com/vllm-project/vllm/pull/27099)) by @yannicks1
* [Bugfix] Fix llguidance backend, rollback when EOS was encountered ([#25905](https://github.com/vllm-project/vllm/pull/25905)) by @Flechman
* [Bugfix][LoRA][Spec Decode] Support LoRA with speculative decoding ([#21068](https://github.com/vllm-project/vllm/pull/21068)) by @xiaohongchen1991

### ‚ö°Ô∏è Performance

* [Performance][Hopper] Avoid M dim padding to 4x for most cases (due to cuda graphs paddings) ([#28492](https://github.com/vllm-project/vllm/pull/28492)) by @alexm-redhat
* [Perf] Refactor cudagraph_support to enable full CUDA graphs for spec decoding with FlashInfer ([#28479](https://github.com/vllm-project/vllm/pull/28479)) by @benchislett
* [Performance] Cache loaded custom logitsprocs to avoid overheads ([#28462](https://github.com/vllm-project/vllm/pull/28462)) by @Isotr0py
* [Performance][B200] silu_mul_quant: pack scales in int32 ([#28358](https://github.com/vllm-project/vllm/pull/28358)) by @varun-sundar-rabindranath
* [Performance][gpt-oss] Revert gpt-oss max cudagraph size to 1024 ([#28345](https://github.com/vllm-project/vllm/pull/28345)) by @mmangkad
* [Perf] Use np.ndarray instead of list[list[int]] to reduce GC overhead ([#28245](https://github.com/vllm-project/vllm/pull/28245)) by @Jialin
* [Perf] Introduce FlattenLogprobs to store logprobs results to reduce GC overhead ([#28171](https://github.com/vllm-project/vllm/pull/28171)) by @Jialin
* [Perf][DeepSeek] Add sigmoid+bias fusion to fused_grouped_topk from TRTLLM ([#28124](https://github.com/vllm-project/vllm/pull/28124)) by @mgoin
* [Performance][B200] Fix deepgemm prologue ([#27897](https://github.com/vllm-project/vllm/pull/27897)) by @varun-sundar-rabindranath
* [Perf] Move gc.freeze logic from EngineCoreProc to EngineCore for better coverage ([#27896](https://github.com/vllm-project/vllm/pull/27896)) by @Jialin
* [Perf] Support stream interval for reducing host overhead ([#27869](https://github.com/vllm-project/vllm/pull/27869)) by @elvischenv
* [Performance] Support FP8 flashinfer TRTLLM MOE on Qwen3 and Qwen-3next ([#27492](https://github.com/vllm-project/vllm/pull/27492)) by @jiahanc
* [PERF] Allreduce fusion. Support torch native matching. Tuning of the thresholds ([#24248](https://github.com/vllm-project/vllm/pull/24248)) by @ilmarkov

### ü§ñ Model Support

* [Model] [Config] Correctly identify granite-4.0-micro as non-hybrid model ([#28563](https://github.com/vllm-project/vllm/pull/28563)) by @tdoublep
* [Model][Qwen3VL] Slighly speedup `fast_pos_embed_interpolate` ([#28434](https://github.com/vllm-project/vllm/pull/28434)) by @lgeiger
* [Model] Pass `mm_features` directly into `get_mrope_input_positions` ([#28399](https://github.com/vllm-project/vllm/pull/28399)) by @DarkLight1337
* [Model][Qwen3VL] Simplify `get_mrope_input_positions` using numpy ([#28302](https://github.com/vllm-project/vllm/pull/28302)) by @lgeiger
* [Model] Consolidate Deepseek-MoE implementation with DeepSeek-v2 ([#28101](https://github.com/vllm-project/vllm/pull/28101)) by @Isotr0py
* [Model] fix glm4_moe_mtp load weights with GLM-4.6 checkpoint. ([#27597](https://github.com/vllm-project/vllm/pull/27597)) by @wuyaoxuehun

### üîå Hardware & Backend

* [ROCm] Bump up the version of amd-smi to 6.4.3 ([#28680](https://github.com/vllm-project/vllm/pull/28680)) by @SageMoore
* [ROCm][BugFix]Fix `get_cu_count` in rocm_aiter_fa.py ([#28618](https://github.com/vllm-project/vllm/pull/28618)) by @ganyi1996ppo
* [XPU] add sym params to IPEXConfig ([#28611](https://github.com/vllm-project/vllm/pull/28611)) by @zufangzhu
* [ROCm][Bugfix] Revert removing setuptools version restriction ([#28592](https://github.com/vllm-project/vllm/pull/28592)) by @gshtras
* [XPU]Fix crash due to removed VLLM_USE_V1 attribute ([#28520](https://github.com/vllm-project/vllm/pull/28520)) by @chaojun-zhang
* [XPU] Support Triton path for LoRA operations on XPU   ([#28511](https://github.com/vllm-project/vllm/pull/28511)) by @faaany
* [ROCm] [Bugfix] Fix `fused_qknorm_rope_kernel` rocm compatibility ([#28500](https://github.com/vllm-project/vllm/pull/28500)) by @tjtanaa
* [TPU] Support GCS path in VLLM_TORCH_PROFILER_DIR ([#28487](https://github.com/vllm-project/vllm/pull/28487)) by @QiliangCui
* [ROCM] Fix ROCm warnings, environment flag access, and GEMM kernel naming for consistency in `_aiter_ops.py` ([#28464](https://github.com/vllm-project/vllm/pull/28464)) by @vllmellm
* [TPU] Rename path to tpu platform ([#28452](https://github.com/vllm-project/vllm/pull/28452)) by @kyuyeunk
* [ROCm][BugFix] Remove the usage of `device_info` from aiter ([#28383](https://github.com/vllm-project/vllm/pull/28383)) by @ganyi1996ppo
* [ROCm] Add missing gemm_a8w8_blockscale import ([#28378](https://github.com/vllm-project/vllm/pull/28378)) by @sarckk
* [ROCm] Add env to enable/disable aiter triton gemm ([#28321](https://github.com/vllm-project/vllm/pull/28321)) by @sarckk
* [ROCm][Platform] Add RX7900XTX device id in _ROCM_DEVICE_ID_NAME_MAP ([#28279](https://github.com/vllm-project/vllm/pull/28279)) by @JartX
* [XPU] Enable Expert parallel for MoE models ([#28263](https://github.com/vllm-project/vllm/pull/28263)) by @jikunshang
* [Rocm][fused_moe][fp4] view weight to torch.float4_e2m1fn_x2 when running aiter fused moe for fp4 model ([#27474](https://github.com/vllm-project/vllm/pull/27474)) by @zejunchen-zejun
* [TPU] patch TPU wheel build script to resolve metadata issue ([#27279](https://github.com/vllm-project/vllm/pull/27279)) by @jcyang43
* [ROCm][Quantization] extend AMD Quark to support mixed-precision quantized model ([#24239](https://github.com/vllm-project/vllm/pull/24239)) by @xuebwang-amd

### ‚öôÔ∏è Refactoring & Core

* [Frontend] supports interleaved thinking ([#28531](https://github.com/vllm-project/vllm/pull/28531)) by @chaunceyjiang
* Remove weight_scale.T special case for SM90 Block FP8 CUTLASS kernel ([#28431](https://github.com/vllm-project/vllm/pull/28431)) by @mgoin
* [Kernel] Fix fused_gdn_gating ([#28343](https://github.com/vllm-project/vllm/pull/28343)) by @ZJY0516
* Remove setuptools upper bound constraint (<80) ([#28337](https://github.com/vllm-project/vllm/pull/28337)) by @ColeMurray
* [Frontend] split append tool output ([#28333](https://github.com/vllm-project/vllm/pull/28333)) by @qandrew
* [Frontend][2/n] remove empty content from _parse_tool_calls_from_content ([#28331](https://github.com/vllm-project/vllm/pull/28331)) by @qandrew
* [Core] Simplify async KV output aggregation ([#28327](https://github.com/vllm-project/vllm/pull/28327)) by @njhill
* [Core] Cache `vllm_is_batch_invariant` ([#28304](https://github.com/vllm-project/vllm/pull/28304)) by @lgeiger
* [Kernel] Optimization of the mm_k operator. ([#28280](https://github.com/vllm-project/vllm/pull/28280)) by @caozuoba
* [Refactor] Remove redundant TP gather/split in split_qkv in QwenVL ([#28271](https://github.com/vllm-project/vllm/pull/28271)) by @gcanlin
* [Core] Rework handling of async scheduling config ([#28250](https://github.com/vllm-project/vllm/pull/28250)) by @njhill
* [Frontend][responsesAPI][1/n] convert responses API tool input to chat completions tool format ([#28231](https://github.com/vllm-project/vllm/pull/28231)) by @qandrew
* [Core][MM] Add mechanism to configure multimodal fields which should stay on CPU ([#28168](https://github.com/vllm-project/vllm/pull/28168)) by @lgeiger
* [Frontend] Change CompilationMode to a proper Enum ([#28165](https://github.com/vllm-project/vllm/pull/28165)) by @gmagogsfm
* remove resolve_op_overloads and use splitting_ops directly ([#28081](https://github.com/vllm-project/vllm/pull/28081)) by @BoyuanFeng
* Refactor CPU/GPU extension targets for CMake build ([#28026](https://github.com/vllm-project/vllm/pull/28026)) by @ashahba
* [Kernel] Optimize rms_norm kernel ([#27931](https://github.com/vllm-project/vllm/pull/27931)) by @xyang16
* [Frontend] Add sagemaker_standards dynamic lora adapter and stateful session management decorators to vLLM OpenAI API server ([#27892](https://github.com/vllm-project/vllm/pull/27892)) by @zhaozuy
* Remove deprecated fields from `CompilationConfig` ([#27593](https://github.com/vllm-project/vllm/pull/27593)) by @hmellor
* [Kernel] LoRA triton kernels support PDL ([#27402](https://github.com/vllm-project/vllm/pull/27402)) by @jeejeelee
* [Kernel][Perf] fuse QK Norm and RoPE into one cuda kernel for Qwen Model ([#27165](https://github.com/vllm-project/vllm/pull/27165)) by @izhuhaoran
* [Core] Separate out attention metadata building logic from prepare inputs ([#26764](https://github.com/vllm-project/vllm/pull/26764)) by @LucasWilkinson
* [Core] Encoder separation for Encode-Prefill-Decode Disaggregation ([#25233](https://github.com/vllm-project/vllm/pull/25233)) by @fake0fan
* [Core][AMD] Migrate fully transparent sleep mode to ROCm platform ([#12695](https://github.com/vllm-project/vllm/pull/12695)) by @HollowMan6

### üîß Build, CI & Testing

* [CI] Bug: Fix ci entrypoint pooling ([#28684](https://github.com/vllm-project/vllm/pull/28684)) by @yewentao256
* [ci][amd] fix basic models extra init test ([#28676](https://github.com/vllm-project/vllm/pull/28676)) by @bradleyhd
* [CI] Skip "Multi-Modal Models Test (Extended) 3" test that's broken in current Transformers ([#28559](https://github.com/vllm-project/vllm/pull/28559)) by @hmellor
* [Test] Remove old non-varlen FA2 test ([#28420](https://github.com/vllm-project/vllm/pull/28420)) by @MatthewBonanni
* [CI] Add mergify rules for `nvidia` label ([#28417](https://github.com/vllm-project/vllm/pull/28417)) by @mgoin
* [CI] Fix Plugin Tests Tests ([#28413](https://github.com/vllm-project/vllm/pull/28413)) by @robertgshaw2-redhat
* [CI] Fix flaky `test_eagle_correctness` test ([#28364](https://github.com/vllm-project/vllm/pull/28364)) by @NickLucche
* [CI] lora/test_mixtral.py : Add additional expected outputs due to flakiness ([#28322](https://github.com/vllm-project/vllm/pull/28322)) by @varun-sundar-rabindranath
* [Build] Fix release pipeline failing annotation ([#28272](https://github.com/vllm-project/vllm/pull/28272)) by @simon-mo
* [CI] Reduce Blackwell Fusion test runtime by filtering tests and only run all tests in nightly ([#28074](https://github.com/vllm-project/vllm/pull/28074)) by @app/copilot-swe-agent
* [build][cmake]: Bundle static ACL and torch libgomp for CPU extension builds ([#28059](https://github.com/vllm-project/vllm/pull/28059)) by @Radu2k
* [CI] Introduce autorun_on_main feature ([#27836](https://github.com/vllm-project/vllm/pull/27836)) by @hl475

### üìö Documentation

* [Docs] Add some details about what the MoE block needs for the Transformers backend ([#28588](https://github.com/vllm-project/vllm/pull/28588)) by @hmellor
* [Docs] Update meetups.md description ([#28583](https://github.com/vllm-project/vllm/pull/28583)) by @mgoin
* [Doc] Fix typo in serving docs ([#28474](https://github.com/vllm-project/vllm/pull/28474)) by @the-codeboy
* [Docs] Fix grammar in CPU installation guide ([#28461](https://github.com/vllm-project/vllm/pull/28461)) by @maryamtahhan
* [Doc] Sleep mode documentation  ([#28357](https://github.com/vllm-project/vllm/pull/28357)) by @iAmir97
* [doc] add guide about the provided PTX was compiled with an unsupported toolchain ([#28305](https://github.com/vllm-project/vllm/pull/28305)) by @youkaichao

### üì¶ Miscellaneous

* [CPU][Bugfix] Fix Apple Silicon M1 compilation failure ([#28681](https://github.com/vllm-project/vllm/pull/28681)) by @mgoin
* [Misc] Update CODEOWNERS for simon-mo and comaniac ([#28675](https://github.com/vllm-project/vllm/pull/28675)) by @simon-mo
* [Attention][Bugfix] Fix FA sink support ([#28660](https://github.com/vllm-project/vllm/pull/28660)) by @MatthewBonanni
* [Misc] Turn off encoder torch compile by default ([#28634](https://github.com/vllm-project/vllm/pull/28634)) by @ywang96
* [Misc] Remove `warn_for_unimplemented_methods` ([#28613](https://github.com/vllm-project/vllm/pull/28613)) by @DarkLight1337
* Use official xformers-0.0.33 built for PT 2.9 ([#28600](https://github.com/vllm-project/vllm/pull/28600)) by @huydhn
* Rewrite C++ meta funcs to Python ([#28595](https://github.com/vllm-project/vllm/pull/28595)) by @janeyx99
* [n-gen] DO NOT repeatedly return finished child requests ([#28591](https://github.com/vllm-project/vllm/pull/28591)) by @Jialin
* [Misc]Fix typo in llm_engine.py ([#28584](https://github.com/vllm-project/vllm/pull/28584)) by @frank-wei
* Mirrored test group definitions for AMD (2025-11-11) ([#28573](https://github.com/vllm-project/vllm/pull/28573)) by @Alexei-V-Ivanov-AMD
* [KV Connector] Test async mode in scheduler tests ([#28550](https://github.com/vllm-project/vllm/pull/28550)) by @markmc
* [Hardware][PowerPC] Fix fp16 compilation error for Power in cpu attention backend and bump oneDNN version ([#28535](https://github.com/vllm-project/vllm/pull/28535)) by @Akashcodes732
* [CI Failure] Fix backend selection for encoder-only models ([#28534](https://github.com/vllm-project/vllm/pull/28534)) by @hl475
* [CI/Build] Fix crash due to removed VLLM_USE_V1 attribute in EPD ([#28521](https://github.com/vllm-project/vllm/pull/28521)) by @fake0fan
* [quantization][config] enable override existing quant_config ([#28510](https://github.com/vllm-project/vllm/pull/28510)) by @ILikeIneine
* [Benchmark] Add retry support to fix workload bias in multi-turn benchmark ([#28493](https://github.com/vllm-project/vllm/pull/28493)) by @ai-jz
* Use FLASHINFER MLA backend when testing fp8_kv_scale_compile ([#28491](https://github.com/vllm-project/vllm/pull/28491)) by @adabeyta
* Skip models that cannot currently init on Transformers v5 ([#28471](https://github.com/vllm-project/vllm/pull/28471)) by @hmellor
* [Misc] Cleanup Executor interface ([#28441](https://github.com/vllm-project/vllm/pull/28441)) by @wangxiyuan
* Only register rocm_aiter_ops if aiter is found ([#28428](https://github.com/vllm-project/vllm/pull/28428)) by @mgoin
* [CI/Build] Refactor Attention backend for test_prefix_prefill from xformers to SDPA ([#28424](https://github.com/vllm-project/vllm/pull/28424)) by @zhewenl
* [MoE][Kernel][Perf] Improve Shared Expert Stream Overlap ([#28406](https://github.com/vllm-project/vllm/pull/28406)) by @alexm-redhat
* [CI/Test Fix] Fix CP tests on Blackwell ([#28404](https://github.com/vllm-project/vllm/pull/28404)) by @LucasWilkinson
* [V0 Deprecation] Remove unused `context_len` and `seq_len` from M-RoPE ([#28395](https://github.com/vllm-project/vllm/pull/28395)) by @DarkLight1337
* Multi turn benchmark progress bar for synthetic conversation generation ([#28394](https://github.com/vllm-project/vllm/pull/28394)) by @segevido
* [Misc] fix typo in DCP comment ([#28389](https://github.com/vllm-project/vllm/pull/28389)) by @Livinfly
* [LoRA][1/N]Remove LoRA extra vocab ([#28382](https://github.com/vllm-project/vllm/pull/28382)) by @jeejeelee
* [Hardware][AMD][Model] Add Triton MoE tuning support and optimized configs for Qwen3 omni for MI308X ([#28373](https://github.com/vllm-project/vllm/pull/28373)) by @sammysun0711
* [V0 deprecation] Remove no longer used `get_metadata_cls` ([#28370](https://github.com/vllm-project/vllm/pull/28370)) by @LucasWilkinson
* [EPLB] Refactor balance_packing to use numpy and optimize GPU-CPU transfers in EPLB ([#28369](https://github.com/vllm-project/vllm/pull/28369)) by @SageMoore
* [chore] Move some wikimedia images to S3 ([#28351](https://github.com/vllm-project/vllm/pull/28351)) by @khluu
* [Misc] FlattenLogprobs -> FlatLogprobs ([#28335](https://github.com/vllm-project/vllm/pull/28335)) by @zhuohan123
* [Misc] Add more scoping for improved trace ([#28329](https://github.com/vllm-project/vllm/pull/28329)) by @frank-wei
* Enhance run_cluster.sh for multi-NIC support ([#28328](https://github.com/vllm-project/vllm/pull/28328)) by @evberrypi
* [CI/Build] Temporary fix to LM Eval Small Models ([#28324](https://github.com/vllm-project/vllm/pull/28324)) by @zhewenl
* [PerfFix] Avoid separate thread for MP executor shm spin (take 2) ([#28319](https://github.com/vllm-project/vllm/pull/28319)) by @njhill
* Update gpu.rocm.inc.md to add support for AMD Ryzen AI MAX / AI 300 Series (gfx1151, gfx1150) ([#28308](https://github.com/vllm-project/vllm/pull/28308)) by @hammmmy
* [README] Add Arm CPUs to the list of supported targets ([#28290](https://github.com/vllm-project/vllm/pull/28290)) by @fadara01
* Revert "[PerfFix] Avoid separate thread for MP executor shm spin (#28012)" ([#28289](https://github.com/vllm-project/vllm/pull/28289)) by @NickLucche
* [NIXL] Generalize block-first backend layouts (FlashInfer-like) ([#28282](https://github.com/vllm-project/vllm/pull/28282)) by @NickLucche
* [Misc] Add some comments in qwen3-next ([#28267](https://github.com/vllm-project/vllm/pull/28267)) by @ZJY0516
* [FixBug]Aeala/ShareGPT_Vicuna_unfiltered marked as multimodal benchmark ([#28265](https://github.com/vllm-project/vllm/pull/28265)) by @princepride
* [CPU]Avoid repeated random sample compile ([#28260](https://github.com/vllm-project/vllm/pull/28260)) by @xiangze-arm
* [Misc][Model][Refactor] Pass the prefix into Linear layers ([#28259](https://github.com/vllm-project/vllm/pull/28259)) by @MengqingCao
*   [Bug] Fix missing token_ids for reasoning parser models in chat completions   #28246 ([#28256](https://github.com/vllm-project/vllm/pull/28256)) by @baonudesifeizhai
* [Log] update shm wait time msg ([#28255](https://github.com/vllm-project/vllm/pull/28255)) by @BoyuanFeng
* [CI/Build] Loosen STT LoRA Translate Check (Flaky Test) ([#28247](https://github.com/vllm-project/vllm/pull/28247)) by @alex-jw-brooks
* [Multimodal][torch.compile] Add compilation config field for turning off ViT/MM compile ([#28242](https://github.com/vllm-project/vllm/pull/28242)) by @Lucaskabela
* [[V0 deprecation]]Remove VLLM_USE_V1 env ([#28204](https://github.com/vllm-project/vllm/pull/28204)) by @wangxiyuan
* [V0 deprecation] Clean up num_prefill_tokens logic for V0 ([#28203](https://github.com/vllm-project/vllm/pull/28203)) by @gcanlin
* [Misc] fix typo and add detailed log ([#28178](https://github.com/vllm-project/vllm/pull/28178)) by @andyxning
* Bump arctic-inference requirement ([#28174](https://github.com/vllm-project/vllm/pull/28174)) by @aurickq
* [CI/Build] Install uv for AMD MI300: Language Models Tests (Hybrid) %N ([#28142](https://github.com/vllm-project/vllm/pull/28142)) by @amdfaa
* [V0 deprecation] Deprecate use_v1 parameter ([#28112](https://github.com/vllm-project/vllm/pull/28112)) by @wangxiyuan
* [CLI] add --max-tokens to `vllm complete` ([#28109](https://github.com/vllm-project/vllm/pull/28109)) by @Iceber
* [Kernels] Split up fused_moe/layer.py, isolate more modular kernel code ([#28064](https://github.com/vllm-project/vllm/pull/28064)) by @bnellnm
* [amd][gptoss] Perf gain because of block alignment ([#28024](https://github.com/vllm-project/vllm/pull/28024)) by @smitkadvani
* Restore PlaMo2 unit test as `pfnet/plamo-2-1b` now supports `transformers >=4.56` ([#28019](https://github.com/vllm-project/vllm/pull/28019)) by @Alnusjaponica
* [flashinfer][fix] do not check nvcc availability when using pre-downloaded cubins ([#27990](https://github.com/vllm-project/vllm/pull/27990)) by @mxz297
* [KVConnector] Enable get_block_ids_with_load_errors() in LMCache connector  ([#27978](https://github.com/vllm-project/vllm/pull/27978)) by @ziruiliu
* [CPU] Refactor CPU attention backend ([#27954](https://github.com/vllm-project/vllm/pull/27954)) by @bigPYJ1151
* Update Flashinfer from `v0.4.1` to `v0.5.2` ([#27952](https://github.com/vllm-project/vllm/pull/27952)) by @hmellor
* [KV connector][WIP] KV cache proxy based on LMCache multi-process mode ([#27902](https://github.com/vllm-project/vllm/pull/27902)) by @ApostaC
* [FA/Chore] Bump FA version for FP8 two-level accumulation  ([#27889](https://github.com/vllm-project/vllm/pull/27889)) by @jmkuebler
* [Attention] Remove max cudagraph size limit of 992 ([#27840](https://github.com/vllm-project/vllm/pull/27840)) by @22quinn
* [Misc] Refactor Attention kv transfer methods into decorator ([#27816](https://github.com/vllm-project/vllm/pull/27816)) by @NickLucche
* `reasoning_content` -> `reasoning` ([#27752](https://github.com/vllm-project/vllm/pull/27752)) by @hmellor
* [EPLB][ROCm]: support EPBL for ROCm backend ([#27731](https://github.com/vllm-project/vllm/pull/27731)) by @PerryZhang01
* `VLLM_USE_TRITON_FLASH_ATTN` V0 variable deprecation ([#27611](https://github.com/vllm-project/vllm/pull/27611)) by @AndreasKaratzas
* Rename clashing method names for vLLM model protocol ([#27583](https://github.com/vllm-project/vllm/pull/27583)) by @hmellor
* Prefer FlashAttention MLA as default over FlashMLA ([#27363](https://github.com/vllm-project/vllm/pull/27363)) by @MatthewBonanni
* [Quantization] fix attention quantization of gpt_oss model ([#27334](https://github.com/vllm-project/vllm/pull/27334)) by @xuebwang-amd
* Implement ARC KV cache eviction policy ([#27039](https://github.com/vllm-project/vllm/pull/27039)) by @albertoperdomo2
* [platform] Move get_cu_count to utils ([#27005](https://github.com/vllm-project/vllm/pull/27005)) by @wangxiyuan
* [Misc] Remove unused attention prefix prefill ops functions ([#26971](https://github.com/vllm-project/vllm/pull/26971)) by @lgeiger
* [Metrics] Refactor LoRA state tracking ([#26801](https://github.com/vllm-project/vllm/pull/26801)) by @markmc
* [DCP] Support dcp kv_cache interleave size > 1 ([#26696](https://github.com/vllm-project/vllm/pull/26696)) by @zhangsicheng5
* [Attention] Refactor CUDA attention backend selection logic ([#24794](https://github.com/vllm-project/vllm/pull/24794)) by @MatthewBonanni
* [RFC][ROCm][AITER] Keep all AITER kernels in `_aiter_ops` class like `_custom_ops` and `_ipex_ops` ([#24490](https://github.com/vllm-project/vllm/pull/24490)) by @vllmellm

## Contributors

@22quinn, @Akashcodes732, @Alexei-V-Ivanov-AMD, @Alnusjaponica, @AndreasKaratzas, @ApostaC, @BoyuanFeng, @ColeMurray, @DarkLight1337, @ElizaWszola, @Flechman, @HollowMan6, @ILikeIneine, @Iceber, @Isotr0py, @JartX, @Jialin, @Livinfly, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @MengqingCao, @NickLucche, @PerryZhang01, @QiliangCui, @Radu2k, @SageMoore, @YuanpingSong, @ZJY0516, @ZhengHongming888, @adabeyta, @ai-jz, @albertoperdomo2, @alex-jw-brooks, @alexm-redhat, @amacaskill, @amdfaa, @andylolu2, @andyxning, @app/copilot-swe-agent, @ashahba, @aurickq, @baonudesifeizhai, @benchislett, @bigPYJ1151, @bnellnm, @bo-ke, @bradleyhd, @caozuoba, @chaojun-zhang, @chaunceyjiang, @dw2761, @elvischenv, @evberrypi, @faaany, @fadara01, @fake0fan, @frank-wei, @ganyi1996ppo, @gcanlin, @gmagogsfm, @gnovack, @gshtras, @hammmmy, @hl475, @hmellor, @huydhn, @iAmir97, @ilmarkov, @izhuhaoran, @janeyx99, @jcyang43, @jeejeelee, @jiahanc, @jikunshang, @jmkuebler, @jvlunteren, @kebe7jun, @khluu, @kyuyeunk, @lgeiger, @liuzijing2014, @luccafong, @markmc, @maryamtahhan, @maxyanghu, @mgoin, @mmangkad, @mxz297, @njhill, @pavanimajety, @piood, @pisceskkk, @princepride, @qandrew, @robertgshaw2-redhat, @sammysun0711, @sarckk, @sdavidbd, @segevido, @simon-mo, @smitkadvani, @tdoublep, @the-codeboy, @tjandy98, @tjtanaa, @usberkeley, @varun-sundar-rabindranath, @vllmellm, @wangxiyuan, @wuyaoxuehun, @xiangze-arm, @xiaohongchen1991, @xuebwang-amd, @xyang16, @yannicks1, @yewentao256, @yihong0618, @youkaichao, @ywang96, @yyzxw, @zejunchen-zejun, @zhangsicheng5, @zhaozuy, @zhewenl, @zhuohan123, @ziruiliu, @zufangzhu

