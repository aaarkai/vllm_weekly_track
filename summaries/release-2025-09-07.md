# Weekly Release Report for vllm-project/vllm (2025-09-07)

This week merged 147 PRs from 90 contributors. Key areas: features 3, fixes 4, performance 16.

## Highlights

* QWEN3 Coder Fused MoE kernels Optimization configs ([#24266](https://github.com/vllm-project/vllm/pull/24266)) by @samanamp
* [Feature] Support Decode Context Parallel (DCP) for MLA ([#23734](https://github.com/vllm-project/vllm/pull/23734)) by @youzhedian
* [V0 deprecation] Deprecate V0 Neuron backend ([#21159](https://github.com/vllm-project/vllm/pull/21159)) by @WoosukKwon
* [Bugfix][Misc] Fix silu_and_mul_nvfp4_quant issue and extract common utils for nvfp4 kernel source files ([#23727](https://github.com/vllm-project/vllm/pull/23727)) by @elvischenv
*  Adding int4 and int8 models for CPU benchmarking ([#23709](https://github.com/vllm-project/vllm/pull/23709)) by @louie-tsai

## Features & Enhancements

* Add @benchislett to codeowner for spec decode and structured outputs ([#24362](https://github.com/vllm-project/vllm/pull/24362)) by @benchislett
* Add @22quinn as code reviewer for RL related components ([#24346](https://github.com/vllm-project/vllm/pull/24346)) by @22quinn
* Support add_generation_prompt in embeddings endpoint with chat request ([#23931](https://github.com/vllm-project/vllm/pull/23931)) by @biba10

## Bug Fixes

* Fix weights loading for Apertus ([#24100](https://github.com/vllm-project/vllm/pull/24100)) by @nathanrchn
* fix some typos ([#24071](https://github.com/vllm-project/vllm/pull/24071)) by @co63oc
* Fix the bug related to loading GPTP INT3 weights. ([#23328](https://github.com/vllm-project/vllm/pull/23328)) by @Jun-Howie
* FIX: Add libnuma-dev to Dockerfile for dev stage ([#20388](https://github.com/vllm-project/vllm/pull/20388)) by @dongbo910220

## Performance

* QWEN3 Thinking Fused MoE kernels Optimization configs ([#24330](https://github.com/vllm-project/vllm/pull/24330)) by @samanamp
* QWEN3 Coder Fused MoE kernels Optimization configs ([#24266](https://github.com/vllm-project/vllm/pull/24266)) by @samanamp
* [Misc] Enhance output readability of helper script ([#24214](https://github.com/vllm-project/vllm/pull/24214)) by @wdhongtw
* [Doc]: fix typos in Python comments ([#24173](https://github.com/vllm-project/vllm/pull/24173)) by @didier-durand
* [Doc]: fix typos in Python comments ([#24115](https://github.com/vllm-project/vllm/pull/24115)) by @didier-durand
* [Bugfix] Fix Qwen3-coder moe tuned config ([#24072](https://github.com/vllm-project/vllm/pull/24072)) by @jeejeelee
* [Perf] Freeze core engine proc heap after init ([#24008](https://github.com/vllm-project/vllm/pull/24008)) by @njhill
* [Benchmark] Add support for local hf dataset path in benchmark ([#23999](https://github.com/vllm-project/vllm/pull/23999)) by @ZJY0516
* [V1] v1 engine + full CUDA graph support for PLaMo2 ([#23998](https://github.com/vllm-project/vllm/pull/23998)) by @nopperl
* [Benchmark] add benchmark for custom activation op ([#23908](https://github.com/vllm-project/vllm/pull/23908)) by @ZJY0516
* [Feature][P/D]: Optimize NIXL Connector xfer Launch ([#23887](https://github.com/vllm-project/vllm/pull/23887)) by @david6666666
* Improve flexibility of auto_tune.sh execution. ([#23766](https://github.com/vllm-project/vllm/pull/23766)) by @anthonsu
*  Adding int4 and int8 models for CPU benchmarking ([#23709](https://github.com/vllm-project/vllm/pull/23709)) by @louie-tsai
* [Perf][V1] Fully overlap model execution ([#23569](https://github.com/vllm-project/vllm/pull/23569)) by @benchislett
* Migrate Qwen2 inputs to TensorSchema ([#23475](https://github.com/vllm-project/vllm/pull/23475)) by @bbeckca
* [CI/Build] Improve Tensor Schema tests speed by avoid engine core initialization ([#23357](https://github.com/vllm-project/vllm/pull/23357)) by @Isotr0py

## Model Support

* [Bugfix] Fix test_mixtral_moe ([#24371](https://github.com/vllm-project/vllm/pull/24371)) by @jeejeelee
* refactor: Turn GPUModelRunner.inputs_embeds to a CpuGpuBuffer ([#24345](https://github.com/vllm-project/vllm/pull/24345)) by @qthequartermasterman
* [Fix] [gpt-oss] fix non-tool calling path for chat completion ([#24324](https://github.com/vllm-project/vllm/pull/24324)) by @aarnphm
* [New Model]: google/embeddinggemma-300m ([#24318](https://github.com/vllm-project/vllm/pull/24318)) by @noooop
* [gpt-oss][Bugfix]Fix streamableparser for missing handling of certain token_ids ([#24306](https://github.com/vllm-project/vllm/pull/24306)) by @chaunceyjiang
* break execute_model in gpu_model_runner into sub-functions for custom scopes ([#24265](https://github.com/vllm-project/vllm/pull/24265)) by @bangshengtang
* [LoRA]: Add lora support to qwen-2.5-omni ([#24231](https://github.com/vllm-project/vllm/pull/24231)) by @pratapyash
* [Model] Add pp support for hunyuan ([#24212](https://github.com/vllm-project/vllm/pull/24212)) by @ZJY0516
* [BugFix] Fix routed_scaling_factor double mul for dots1 and glm4 MoE models ([#24132](https://github.com/vllm-project/vllm/pull/24132)) by @sarckk
* [Model] Classification models support logit_bias / sigmoid_normalize ([#24031](https://github.com/vllm-project/vllm/pull/24031)) by @noooop
* [Misc] Avoid redundant copy for encoder-only models ([#24012](https://github.com/vllm-project/vllm/pull/24012)) by @WoosukKwon
* [BUGFIX] GPTQ quantization compatibility for Qwen3 MOE models (AutoGPTQ and AutoRound-GPTQ) ([#23994](https://github.com/vllm-project/vllm/pull/23994)) by @JartX
* [Model]: support KeyeVL-1_5-8B ([#23838](https://github.com/vllm-project/vllm/pull/23838)) by @Kwai-Keye
* [Model] Support DP for ViT on Kimi-VL-A3B-Thinking-2506 ([#23817](https://github.com/vllm-project/vllm/pull/23817)) by @david6666666
* [Model] Add MiDashengLM model support ([#23652](https://github.com/vllm-project/vllm/pull/23652)) by @bingchen-mi
* [Core][Model] Terratorch backend integration ([#23513](https://github.com/vllm-project/vllm/pull/23513)) by @mgazz
* [V1][Mamba1] - FP32 SSM Kernel Support ([#23506](https://github.com/vllm-project/vllm/pull/23506)) by @Josephasafg
* [Feature][gpt-oss] Add support for num_cached_tokens and num_reasoning_tokens tracking ([#23460](https://github.com/vllm-project/vllm/pull/23460)) by @NagyGeorge
* [Model] Support dp on ViT on GLM-4.5V ([#23168](https://github.com/vllm-project/vllm/pull/23168)) by @david6666666
* [Misc] IO Processor plugins for pooling models ([#22820](https://github.com/vllm-project/vllm/pull/22820)) by @christian-pinto
* [gpt-oss] tool parser supports for /chat/completions [1/n] ([#22386](https://github.com/vllm-project/vllm/pull/22386)) by @aarnphm

## Hardware & Backend

* [CI][Fix] deterministic seed for flaky CI runs on structured outputs ([#24380](https://github.com/vllm-project/vllm/pull/24380)) by @aarnphm
* [CI] Disable flaky structured output test from CI ([#24366](https://github.com/vllm-project/vllm/pull/24366)) by @ywang96
* [Frontend][Responses API] Support reporting tool output tokens and fix reasoning token count ([#24285](https://github.com/vllm-project/vllm/pull/24285)) by @yeqcharlotte
* [Hardware][Apple-CPU] Disable OneDNN build for Apple Silicon ([#24200](https://github.com/vllm-project/vllm/pull/24200)) by @ignaciosica
* [CPU] Refactor CPU unquantized linear ([#24150](https://github.com/vllm-project/vllm/pull/24150)) by @bigPYJ1151
* [XPU] support Triton Attention backend on Intel GPU ([#24149](https://github.com/vllm-project/vllm/pull/24149)) by @jikunshang
* [Kernel][Bugfix] Fix grouped topk cu ([#24146](https://github.com/vllm-project/vllm/pull/24146)) by @mayuyuace
* [Doc]: fix typos in Python comments ([#24093](https://github.com/vllm-project/vllm/pull/24093)) by @didier-durand
* [XPU] Fix the bug of LoRA logits on the XPU platform ([#24081](https://github.com/vllm-project/vllm/pull/24081)) by @chaojun-zhang
* [Doc]: Fix CPU install docs: force torch-backend=cpu to avoid GPU torchvision errors ([#24033](https://github.com/vllm-project/vllm/pull/24033)) by @yankay
* [Doc]: fix typos in Python comments ([#24026](https://github.com/vllm-project/vllm/pull/24026)) by @didier-durand
* [Kernel] Update DeepGEMM to latest commit ([#23915](https://github.com/vllm-project/vllm/pull/23915)) by @jeejeelee
* [CI/Build] Serve images used by multimodal tests through local HTTP Server ([#23907](https://github.com/vllm-project/vllm/pull/23907)) by @divyanshsinghvi
* [Feature] Support Decode Context Parallel (DCP) for MLA ([#23734](https://github.com/vllm-project/vllm/pull/23734)) by @youzhedian
* [AMD][Kernel][Bugfix] Cast offsets tensor bn to tl.int64 to avoid GPU segfault ([#23692](https://github.com/vllm-project/vllm/pull/23692)) by @rasmith
* [Kernels] Overlap shared experts with send/recv ([#23273](https://github.com/vllm-project/vllm/pull/23273)) by @bnellnm
* [XPU][Feature] fp8 online quantization support for XPU ([#23148](https://github.com/vllm-project/vllm/pull/23148)) by @yma11
* [XPU][P/D] Add XPU support in NixlConnector ([#22436](https://github.com/vllm-project/vllm/pull/22436)) by @zhenwei-intel
* [Attention] FlashAttn MLA ([#14258](https://github.com/vllm-project/vllm/pull/14258)) by @LucasWilkinson

## Refactoring & Core

* [Refactor] Introduce basic Renderer for completion-style request ([#24010](https://github.com/vllm-project/vllm/pull/24010)) by @sfeng33
* [Feature][Responses API]Support MCP tools with streaming mode + background mode ([#23927](https://github.com/vllm-project/vllm/pull/23927)) by @wuhang2014
* [Feature][Response API] Add streaming support for non-harmony ([#23741](https://github.com/vllm-project/vllm/pull/23741)) by @kebe7jun
* [Attention][Platform] Refactor MLA to support Custom Op ([#23332](https://github.com/vllm-project/vllm/pull/23332)) by @whx-sjtu

## Build, CI & Testing

* [Bugfix] Fix unstable silu_mul+nvfp4 quant fusion test ([#24370](https://github.com/vllm-project/vllm/pull/24370)) by @elvischenv
* [Bugfix] Fix silu_mul+quant fusion test ([#24341](https://github.com/vllm-project/vllm/pull/24341)) by @elvischenv
* [CI/Build] Reduce the number of redundant cases to test for LoRA ([#24276](https://github.com/vllm-project/vllm/pull/24276)) by @zhuohan123
* [CI] Add timeouts to tests ([#24260](https://github.com/vllm-project/vllm/pull/24260)) by @rafvasq
* [CI/Build] Disable SiluMul NVFP4 quant fusion tests ([#24121](https://github.com/vllm-project/vllm/pull/24121)) by @MatthewBonanni
* [CI] Accelerate mteb test by setting SentenceTransformers mteb score to a constant ([#24088](https://github.com/vllm-project/vllm/pull/24088)) by @noooop
* [CI Failure] Skip failing nvfp4 silu test ([#23959](https://github.com/vllm-project/vllm/pull/23959)) by @mgoin
* [CI]: reduce HTTP calls inside entrypoints openai tests ([#23646](https://github.com/vllm-project/vllm/pull/23646)) by @AzizCode92

## Documentation

* [docs] add shenzhen meetup ([#24326](https://github.com/vllm-project/vllm/pull/24326)) by @youkaichao
* [Doc]: fix typos in Python comments ([#24294](https://github.com/vllm-project/vllm/pull/24294)) by @didier-durand
* [Doc] Update vLLM Singapore Meetup info ([#24234](https://github.com/vllm-project/vllm/pull/24234)) by @tjtanaa
* [Doc]: fix typos in Python comments ([#24077](https://github.com/vllm-project/vllm/pull/24077)) by @didier-durand
* [Gemma3n] Fix audio batching ([#24052](https://github.com/vllm-project/vllm/pull/24052)) by @NickLucche
* [docs][misc] IOProcessor plugins fixes ([#24046](https://github.com/vllm-project/vllm/pull/24046)) by @christian-pinto
* [Doc]: fix typos in Python comments ([#24042](https://github.com/vllm-project/vllm/pull/24042)) by @didier-durand
* [docs] add SYS_NICE cap & `security-opt` for docker/k8s ([#24017](https://github.com/vllm-project/vllm/pull/24017)) by @panpan0000
* [Misc] add hash_function doc string ([#24014](https://github.com/vllm-project/vllm/pull/24014)) by @andyxning
* [Doc]: fix typos in Python comments ([#24001](https://github.com/vllm-project/vllm/pull/24001)) by @didier-durand
* Document multi-proc method selection for profiling ([#23802](https://github.com/vllm-project/vllm/pull/23802)) by @hypdeb
* [V1] Wrapper which plumbs request-level logits processors into vLLM batch-level logits processing ([#23656](https://github.com/vllm-project/vllm/pull/23656)) by @afeldman-nm
* correct LWS deployment yaml ([#23104](https://github.com/vllm-project/vllm/pull/23104)) by @cberge908
* v1: Support KV events from connectors ([#19737](https://github.com/vllm-project/vllm/pull/19737)) by @orozery

## Miscellaneous

* [Misc] collect flashinfer version in collect_env.py ([#24378](https://github.com/vllm-project/vllm/pull/24378)) by @yeqcharlotte
* [attention][DCP] use AttentionImpl.need_to_return_lse_for_decode ([#24372](https://github.com/vllm-project/vllm/pull/24372)) by @youkaichao
* [Bugfix] Fix broken deepseek fp8 TP weights loading ([#24367](https://github.com/vllm-project/vllm/pull/24367)) by @Isotr0py
* [Bugfix] Catch and log invalid token ids in detokenizer ([#24351](https://github.com/vllm-project/vllm/pull/24351)) by @njhill
* [KV Sharing] Raise error if using eagle with fast prefill ([#24350](https://github.com/vllm-project/vllm/pull/24350)) by @sarckk
* [Bugfix] Avoid uninitialized usage of azp_val when AZP is false. ([#24335](https://github.com/vllm-project/vllm/pull/24335)) by @mohankku
* [Multimodal] Improve max video embedding length estimation in V1 ([#24312](https://github.com/vllm-project/vllm/pull/24312)) by @ywang96
* [Frontend] Skip unnecessary detokenization when token_id is requested ([#24236](https://github.com/vllm-project/vllm/pull/24236)) by @NickLucche
* Use hidden_size_per_head as head_size fallback ([#24221](https://github.com/vllm-project/vllm/pull/24221)) by @nopperl
* [Bugfix] Fix Incremental Detokenization with `tokenizers == 0.22.0` ([#24159](https://github.com/vllm-project/vllm/pull/24159)) by @faaany
* [Misc] Clean up deadcode for legacy processing pipeline ([#24153](https://github.com/vllm-project/vllm/pull/24153)) by @Isotr0py
* [Bug] R1 Accuracy: Fix `routed_scaling_factor` Double Mul Issue ([#24119](https://github.com/vllm-project/vllm/pull/24119)) by @yewentao256
* Upgrade FlashInfer to v0.3.0 ([#24086](https://github.com/vllm-project/vllm/pull/24086)) by @nvpohanh
* [Misc] Slight improve deepgemm print ([#24085](https://github.com/vllm-project/vllm/pull/24085)) by @jeejeelee
* Run ruff format on a few files. ([#24075](https://github.com/vllm-project/vllm/pull/24075)) by @huachenheli
* Update release pipeline post PyTorch 2.8.0 update ([#24073](https://github.com/vllm-project/vllm/pull/24073)) by @youkaichao
* [Misc] Add check for dual_chunk_attention ([#24070](https://github.com/vllm-project/vllm/pull/24070)) by @ZJY0516
* [bugfix]fix MTP hidden states ([#24056](https://github.com/vllm-project/vllm/pull/24056)) by @luccafong
* [Misc] Minor code simplification for spec decode ([#24053](https://github.com/vllm-project/vllm/pull/24053)) by @WoosukKwon
* [Bugfix] Fix the issue that Blip2ForConditionalGeneration' object has… ([#24028](https://github.com/vllm-project/vllm/pull/24028)) by @DamonJiang777
* [Misc] Move fast prefill logic to separate method ([#24013](https://github.com/vllm-project/vllm/pull/24013)) by @WoosukKwon
* [Minor] Fix some random typos in comments ([#24009](https://github.com/vllm-project/vllm/pull/24009)) by @njhill
* [Bugfix] Fix transform_config parsing in Compressed Tensors ([#23945](https://github.com/vllm-project/vllm/pull/23945)) by @kylesayrs
* [BugFix] Fix EXAONE4 rotary embeddings ([#23918](https://github.com/vllm-project/vllm/pull/23918)) by @lkm2835
* [Bugfix] Fix packed_factor missing attribute error ([#23902](https://github.com/vllm-project/vllm/pull/23902)) by @kyuyeunk
* [Bugfix][DP] DP distribution does not require ray[default] ([#23822](https://github.com/vllm-project/vllm/pull/23822)) by @kebe7jun
* [Frontend] Gemma3n audio `transcriptions`/`translations` endpoint ([#23735](https://github.com/vllm-project/vllm/pull/23735)) by @NickLucche
* [Compile] Fix Compile Warning for `w4a8_mm_entry.cu` ([#23660](https://github.com/vllm-project/vllm/pull/23660)) by @yewentao256
* Migrate Interns1 inputs to TensorSchema ([#23510](https://github.com/vllm-project/vllm/pull/23510)) by @bbeckca
* Migrate whisper inputs to TensorSchema ([#23505](https://github.com/vllm-project/vllm/pull/23505)) by @bbeckca
* Migrate ultravox inputs to TensorSchema ([#23503](https://github.com/vllm-project/vllm/pull/23503)) by @bbeckca
* Migrate Phi4 inputs to TensorSchema ([#23471](https://github.com/vllm-project/vllm/pull/23471)) by @bbeckca
* [Bugfix] Fixing division by zero in triton_attn if query_heads/kv_heads > 16  ([#23424](https://github.com/vllm-project/vllm/pull/23424)) by @bringlein
* [RFC] allow cancelation after shutdown in blocking collective_rpc ([#23390](https://github.com/vllm-project/vllm/pull/23390)) by @842974287
* [Log] Only Print Profiler Results on Rank 0 ([#23370](https://github.com/vllm-project/vllm/pull/23370)) by @yewentao256
* [Attention] Blackwell FP8 MLA support with CUTLASS_MLA backend ([#23289](https://github.com/vllm-project/vllm/pull/23289)) by @MatthewBonanni
* [Core] Allow disabling TP sharding for parallel Linear layer ([#23024](https://github.com/vllm-project/vllm/pull/23024)) by @Isotr0py
* Upgrade xgrammar to 0.1.23 ([#22988](https://github.com/vllm-project/vllm/pull/22988)) by @russellb
* vllm fix check on max vocab size ([#22471](https://github.com/vllm-project/vllm/pull/22471)) by @xw285cornell
* Migrate OvisImagePatchInputs to TensorSchema ([#22024](https://github.com/vllm-project/vllm/pull/22024)) by @bbeckca
* [Misc] Have AsyncLLM `custom_stat_loggers` extend default logger list ([#20952](https://github.com/vllm-project/vllm/pull/20952)) by @eicherseiji
* [Nixl] Heterogeneous TP support FlashInfer ([#20189](https://github.com/vllm-project/vllm/pull/20189)) by @NickLucche

## Breaking Changes

* Lora bias(enable_lora_bias) deprecate warning ([#24339](https://github.com/vllm-project/vllm/pull/24339)) by @ashwin-phadke
* Remove deprecated `PyNcclConnector` ([#24151](https://github.com/vllm-project/vllm/pull/24151)) by @panpan0000
* [distributed][rl] remove nccl cumem env var override ([#24141](https://github.com/vllm-project/vllm/pull/24141)) by @youkaichao
* [Metrics] Deprecate TPOT in favor of ITL ([#24110](https://github.com/vllm-project/vllm/pull/24110)) by @markmc
* [Chore][V0 Deprecation] Move LogProb to a separate file ([#24055](https://github.com/vllm-project/vllm/pull/24055)) by @WoosukKwon
* Remove runtime checks based on pooling params ([#24051](https://github.com/vllm-project/vllm/pull/24051)) by @maxdebayser
* [Misc] Enable V1 FP16 inference on pre-Ampere GPUs ([#24022](https://github.com/vllm-project/vllm/pull/24022)) by @Isotr0py
* Fix MiniMax attention module prefix and remove useless code ([#23982](https://github.com/vllm-project/vllm/pull/23982)) by @qscqesze
* [CI] Enable all hf transformers baselines in test_hybrid ([#23936](https://github.com/vllm-project/vllm/pull/23936)) by @tdoublep
* [Bugfix][Misc] Fix silu_and_mul_nvfp4_quant issue and extract common utils for nvfp4 kernel source files ([#23727](https://github.com/vllm-project/vllm/pull/23727)) by @elvischenv
* [Misc] Removed force_fp8_e4m3fnuz from FP8LinearOp ([#23725](https://github.com/vllm-project/vllm/pull/23725)) by @nvjullin
* [Misc] refactor code by import as for torch._inductor.config ([#23677](https://github.com/vllm-project/vllm/pull/23677)) by @andyxning
* [Core][Multimodal] Allow passing `multi_modal_uuids` as multimodal identifiers. ([#23394](https://github.com/vllm-project/vllm/pull/23394)) by @ywang96
* [Bugfix] Add support for `<tool_call>` format in streaming mode for XLAM Tool Parser ([#22769](https://github.com/vllm-project/vllm/pull/22769)) by @DevonPeroutky
* [V0 deprecation] Deprecate V0 Neuron backend ([#21159](https://github.com/vllm-project/vllm/pull/21159)) by @WoosukKwon
* [Frontend] Update the warning log when using VLLM_ALLOW_LONG_MAX_MODEL_LEN ([#20904](https://github.com/vllm-project/vllm/pull/20904)) by @noooop

## Upgrade Notes

- - After #20358, we upgraded Triton to 3.4.0, which have fixed the issue that broke pre-Ampere FP16 calculation, so we can fully enable V1 on Volta and Turing now.
- Note: To support (piecewise) CUDA graphs and `torch.compile`, the `Plamo2MambaMixer` is added to `CompilationConfig._attention_ops` by default. I think it's OK to do since the diff is minimal.
- **Note:** @maxdebayser @DarkLight1337 @christian-pinto this PR has marginal impact on #22820. The Prithvi and Terratorch model executors provide the same behaviour. The only additional requirement is that to use the Terratorch backend we need to pass an additional argument (`--model-impl terratorch`
- Upgrade Xgrammar to 0.1.23, which includes some important fixes which allow us

## Contributors

@22quinn, @842974287, @AzizCode92, @DamonJiang777, @DevonPeroutky, @Isotr0py, @JartX, @Josephasafg, @Jun-Howie, @Kwai-Keye, @LucasWilkinson, @MatthewBonanni, @NagyGeorge, @NickLucche, @WoosukKwon, @ZJY0516, @aarnphm, @afeldman-nm, @andyxning, @anthonsu, @ashwin-phadke, @bangshengtang, @bbeckca, @benchislett, @biba10, @bigPYJ1151, @bingchen-mi, @bnellnm, @bringlein, @cberge908, @chaojun-zhang, @chaunceyjiang, @christian-pinto, @co63oc, @david6666666, @didier-durand, @divyanshsinghvi, @dongbo910220, @eicherseiji, @elvischenv, @faaany, @huachenheli, @hypdeb, @ignaciosica, @jeejeelee, @jikunshang, @kebe7jun, @kylesayrs, @kyuyeunk, @lkm2835, @louie-tsai, @luccafong, @markmc, @maxdebayser, @mayuyuace, @mgazz, @mgoin, @mohankku, @nathanrchn, @njhill, @noooop, @nopperl, @nvjullin, @nvpohanh, @orozery, @panpan0000, @pratapyash, @qscqesze, @qthequartermasterman, @rafvasq, @rasmith, @russellb, @samanamp, @sarckk, @sfeng33, @tdoublep, @tjtanaa, @wdhongtw, @whx-sjtu, @wuhang2014, @xw285cornell, @yankay, @yeqcharlotte, @yewentao256, @yma11, @youkaichao, @youzhedian, @ywang96, @zhenwei-intel, @zhuohan123
