# Weekly Release Report for vllm-project/vllm (2025-10-03)

This week merged 217 PRs from 121 contributors. Key areas: features 8, fixes 16, performance 20.

## Executive Summary

本周发布包含多项关键更新。功能方面，新增Hugging Face Inference Endpoints部署指南、Phi4FlashForCausalLM模型支持、聊天模板参数过滤及媒体域名限制选项。错误修复重点解决了V1引擎在Ray分布式环境下的序列化问题、CUTLASS MLA挂起、Blackwell GPU的INT8量化错误，以及多模态嵌入合并逻辑。性能优化涉及reshape_and_cache内核改进、FP8 MOE后端选择、内存分析增强，并修复了基准测试中的重复请求问题。

模型与硬件支持新增DeepSeek-V3.2，完善了多模态模型配置合并机制，修复了Qwen-VL等模型的推理异常。AMD设备新增GLM-4.5的MoE调优配置。重要变更：CI/Build中将vllm.entrypoints.openai.api_server入口点替换为vllm serve命令，可能影响现有部署脚本，升级时需相应调整。

## Highlights

* [amd_dev] branch rebase ([#25753](https://github.com/vllm-project/vllm/pull/25753)) by @HAIAI
* [Bugfix] Merge MM embeddings by index instead of token IDs ([#16229](https://github.com/vllm-project/vllm/pull/16229)) by @DarkLight1337
* [CI/Build] Replace `vllm.entrypoints.openai.api_server` entrypoint with `vllm serve` command ([#25967](https://github.com/vllm-project/vllm/pull/25967)) by @DarkLight1337
* [V1] [P/D] Add Support for KV Load Failure Recovery ([#19330](https://github.com/vllm-project/vllm/pull/19330)) by @sdavidbd
* [Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 ([#25858](https://github.com/vllm-project/vllm/pull/25858)) by @Josephasafg

## Features & Enhancements

* Add Hugging Face Inference Endpoints guide to Deployment docs ([#25886](https://github.com/vllm-project/vllm/pull/25886)) by @sergiopaniego
* Add Phi4FlashForCausalLM to _PREVIOUSLY_SUPPORTED_MODELS ([#25832](https://github.com/vllm-project/vllm/pull/25832)) by @tdoublep
* Add filtering for chat template kwargs ([#25794](https://github.com/vllm-project/vllm/pull/25794)) by @russellb
* Add option to restrict media domains ([#25783](https://github.com/vllm-project/vllm/pull/25783)) by @russellb
* Add flashinfer-build.sh and register precompiled cu128 wheel in Dockerfile ([#25782](https://github.com/vllm-project/vllm/pull/25782)) by @mgoin
* Add explicit pooling classes for the Transformers backend ([#25322](https://github.com/vllm-project/vllm/pull/25322)) by @hmellor
* Support LongCat-Flash-Chat tool call ([#24083](https://github.com/vllm-project/vllm/pull/24083)) by @Xu-Wenqing
* Support RL online quantization with torchao ([#23014](https://github.com/vllm-project/vllm/pull/23014)) by @jerryzh168

## Bug Fixes

* Fix V1 engine serialization error with Ray distributed executor ([#26148](https://github.com/vllm-project/vllm/pull/26148)) by @nrghosh
* Fix undefined symbol: cutlass_moe_mm_sm100 ([#26098](https://github.com/vllm-project/vllm/pull/26098)) by @jasl
* [BugFix] Fix FI accuracy issue when used for MLA prefill ([#26063](https://github.com/vllm-project/vllm/pull/26063)) by @LucasWilkinson
* [BugFix] ChunkedLocalAttention is currently not CG compatible ([#26034](https://github.com/vllm-project/vllm/pull/26034)) by @LucasWilkinson
* [BugFix][DP/EP] Fix CUTLASS MLA hang under load ([#26026](https://github.com/vllm-project/vllm/pull/26026)) by @LucasWilkinson
* Fix INT8 quantization error on Blackwell GPUs (SM100+) ([#25935](https://github.com/vllm-project/vllm/pull/25935)) by @padg9912
* [Bug] Fix Weight Loading for Block FP8 Cutlass SM90 ([#25909](https://github.com/vllm-project/vllm/pull/25909)) by @yewentao256
* Fix MTP with deepep_low_latency ([#25904](https://github.com/vllm-project/vllm/pull/25904)) by @MatthewBonanni
* [Bugfix] Fix accuracy issue of TRTLLM FP8 MOE and improve logging ([#25895](https://github.com/vllm-project/vllm/pull/25895)) by @pavanimajety
* [Llama4] [multimodal] Fix misplaced dtype cast of `cos_sin_cache` in `Llama4VisionRotaryEmbedding` ([#25889](https://github.com/vllm-project/vllm/pull/25889)) by @cjackal
* Update launch_bounds_utils.h for correct compile on Multiple Cuda Arch - PTXAS out of range Warning ([#25843](https://github.com/vllm-project/vllm/pull/25843)) by @DrStone71
* Fix GPTQ model loading in Transformers backend ([#25770](https://github.com/vllm-project/vllm/pull/25770)) by @hmellor
* fix: print outputt offline_inference/base/chat.py example ([#25744](https://github.com/vllm-project/vllm/pull/25744)) by @Iceber
* fix: revert cast to cpu in `MsgpackEncoder._encode_tensor` to avoid hidden performance regressions ([#25738](https://github.com/vllm-project/vllm/pull/25738)) by @qthequartermasterman
* [Core] Force PIECEWISE CUDAGraph mode for encoder-decoder ([#25701](https://github.com/vllm-project/vllm/pull/25701)) by @russellb
* Fix random dataset mismatched token length with config. ([#24937](https://github.com/vllm-project/vllm/pull/24937)) by @weireweire

## Performance

* [Bug][Benchmark] Fix duplicate req in oversampling ([#26140](https://github.com/vllm-project/vllm/pull/26140)) by @ekagra-ranjan
* [DeepSeek] Improve performance of DS MLA cache kernel ([#26132](https://github.com/vllm-project/vllm/pull/26132)) by @MatthewBonanni
* [Log] Optimize DeepGEMM Missing Log ([#26106](https://github.com/vllm-project/vllm/pull/26106)) by @yewentao256
* [Refactor] Optimize FP8 MOE Backend Choice and Log ([#26044](https://github.com/vllm-project/vllm/pull/26044)) by @yewentao256
* [Misc] Add penalties sampling parameters to serve tool ([#25974](https://github.com/vllm-project/vllm/pull/25974)) by @southfreebird
* [Quantization/NVFP4] Speed up TRTLLM NVFP4 MOE weight loading and fix K/V scale loading for MLA Attn ([#25968](https://github.com/vllm-project/vllm/pull/25968)) by @pavanimajety
* [CI/Build] Replace `vllm.entrypoints.openai.api_server` entrypoint with `vllm serve` command ([#25967](https://github.com/vllm-project/vllm/pull/25967)) by @DarkLight1337
* [Bench] Add DeepSeekV32 to MoE benchmark ([#25962](https://github.com/vllm-project/vllm/pull/25962)) by @jeejeelee
* [Perf] Optimize `reshape_and_cache` CUDA Kernel ([#25955](https://github.com/vllm-project/vllm/pull/25955)) by @ZJY0516
* EAGLE 3: Fix preamble so that measured speedup over Eagle 1 becomes 32% instead of 5% on MTBench ([#25916](https://github.com/vllm-project/vllm/pull/25916)) by @ekagra-ranjan
* [Benchmark] Support benchmark throughput for external launcher DP ([#25913](https://github.com/vllm-project/vllm/pull/25913)) by @zhuohan123
* [perf] Use CPU tensor to reduce GPU->CPU sync ([#25884](https://github.com/vllm-project/vllm/pull/25884)) by @lhtin
* [Fix] Improve CPU backend compatibility for RISC-V ([#25816](https://github.com/vllm-project/vllm/pull/25816)) by @ihb2032
* [MM] Optimize memory profiling for scattered multimodal embeddings ([#25810](https://github.com/vllm-project/vllm/pull/25810)) by @ywang96
* [amd_dev] branch rebase ([#25753](https://github.com/vllm-project/vllm/pull/25753)) by @HAIAI
* perf: Avoid copying inputs_embeds tensors to GPU unless prompt_embeds is enabled ([#25739](https://github.com/vllm-project/vllm/pull/25739)) by @qthequartermasterman
* [Log] Optimize Log for FP8MOE ([#25709](https://github.com/vllm-project/vllm/pull/25709)) by @yewentao256
* [Perf] Fix and reapply move apply w8a8 block fp8 linear to class ([#25696](https://github.com/vllm-project/vllm/pull/25696)) by @ElizaWszola
* [Bugfix] Optimize CpuGpuBuffer initialization ([#25447](https://github.com/vllm-project/vllm/pull/25447)) by @namanlalitnyu
* [Multimodal][Speculative Decoding]Eagle Eagle3 mm support, enablement on qwen2.5vl ([#22872](https://github.com/vllm-project/vllm/pull/22872)) by @david6666666

## Model Support

* [Model] Use `merge_by_field_config` for MM models (InternVL family) ([#26153](https://github.com/vllm-project/vllm/pull/26153)) by @DarkLight1337
* [BUG] Reorder model config creation ([#26124](https://github.com/vllm-project/vllm/pull/26124)) by @ahao-anyscale
* [BugFix][QWEN-VL]fix wrong apply_rotary_emb_torch selection introduced by #24642 ([#26123](https://github.com/vllm-project/vllm/pull/26123)) by @xuechendi
* [Model] Use `merge_by_field_config` for MM models (G) ([#26117](https://github.com/vllm-project/vllm/pull/26117)) by @DarkLight1337
* [Model] Use `merge_by_field_config` for MM models (D-F) ([#26076](https://github.com/vllm-project/vllm/pull/26076)) by @DarkLight1337
* [Model] Use `merge_by_field_config` for MM models (A-C) ([#26073](https://github.com/vllm-project/vllm/pull/26073)) by @DarkLight1337
* [CI] Tweaks to GPT-OSS Eval (Blackwell) for stability ([#26030](https://github.com/vllm-project/vllm/pull/26030)) by @mgoin
* [Model] Fixed stream generator for gpt-oss + spec-decoding ([#26027](https://github.com/vllm-project/vllm/pull/26027)) by @astralord
* [BugFix][MM] Fix Nonetype error when video is cache in qwen2.5-omni-thinker ([#26004](https://github.com/vllm-project/vllm/pull/26004)) by @wwl2755
* [MM] Add text-only mode for Qwen3-VL ([#26000](https://github.com/vllm-project/vllm/pull/26000)) by @ywang96
* [Model] MTP fallback to eager for DeepSeek v32 ([#25982](https://github.com/vllm-project/vllm/pull/25982)) by @luccafong
* [Doc] Improve MM Pooling model documentation ([#25966](https://github.com/vllm-project/vllm/pull/25966)) by @DarkLight1337
* [Bugfix] Relax tokenizer regex for mixtral to include 'tokenizer.model' ([#25964](https://github.com/vllm-project/vllm/pull/25964)) by @BowenBao
* [Model] Move `vision_feature_select_strategy` into `resolve_visual_encoder_outputs` ([#25938](https://github.com/vllm-project/vllm/pull/25938)) by @DarkLight1337
* [Bugfix][Model]fix ernie45 moe gate&bias dtype to float32 ([#25936](https://github.com/vllm-project/vllm/pull/25936)) by @CSWYF3634076
* [New Model] DeepSeek-V3.2 (Rebased to Main) ([#25896](https://github.com/vllm-project/vllm/pull/25896)) by @zyongye
* [Model][Bugfix] Fix issues in MiDashengLM implementation for quantized models ([#25854](https://github.com/vllm-project/vllm/pull/25854)) by @zhoukezi
* [Bugfix] fix Qwen3VLMoe load when pp > 1 ([#25838](https://github.com/vllm-project/vllm/pull/25838)) by @JJJYmmm
* Update GLM-4.5 Doc transformers version ([#25830](https://github.com/vllm-project/vllm/pull/25830)) by @zRzRzRzRzRzRzR
* [Bugfix] Fix Qwen3-VL regression from #24982 ([#25814](https://github.com/vllm-project/vllm/pull/25814)) by @ywang96
* [Model] Supplement to PR 24862: Pass param prefix to LLMHead ([#25805](https://github.com/vllm-project/vllm/pull/25805)) by @whx-sjtu
* [CI/Build] Add timing to Model Executor Test ([#25799](https://github.com/vllm-project/vllm/pull/25799)) by @22quinn
* [gpt-oss] disable tool server initialization if no tool in request ([#25790](https://github.com/vllm-project/vllm/pull/25790)) by @qandrew
* [Bugfix] Allow Only SDPA Backend for ViT on B200 for Qwen3-VL ([#25788](https://github.com/vllm-project/vllm/pull/25788)) by @yewentao256
* [CI/Build] Consolidate model loader tests and requirements ([#25765](https://github.com/vllm-project/vllm/pull/25765)) by @DarkLight1337
* [Qwen3-Next][GDN] fixes cuda graph capturing bug in GDN metadata and a stride bug in causal_conv_1d. ([#25743](https://github.com/vllm-project/vllm/pull/25743)) by @sighingnow
* Test Prompt Embeds/LoRA compatibility and Enable LoRA Support for OPT Models  ([#25717](https://github.com/vllm-project/vllm/pull/25717)) by @qthequartermasterman
* [Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI300X ([#25703](https://github.com/vllm-project/vllm/pull/25703)) by @xaguilar-amd
* [Spec decode] automatically disable mm for text-only draft models ([#25667](https://github.com/vllm-project/vllm/pull/25667)) by @jmkuebler
* [VLM] Update Qwen3-VL max_num_video_tokens calculation for configurable video profiling ([#25557](https://github.com/vllm-project/vllm/pull/25557)) by @Isotr0py
* [Quantization] Add field to skip unquantized modules for GPTQ config ([#25455](https://github.com/vllm-project/vllm/pull/25455)) by @Isotr0py
* [Bugfix][Model] Fix inference for Hunyuan dense models ([#25354](https://github.com/vllm-project/vllm/pull/25354)) by @Anionex
* [gpt-oss] use vLLM instead of openai types for streaming ([#25186](https://github.com/vllm-project/vllm/pull/25186)) by @qandrew
* Llamas 3.1 405B fp4 changes upstreaming from 355_wip ([#25135](https://github.com/vllm-project/vllm/pull/25135)) by @maleksan85
* [Mamba][KVCacheManager] Simplify kv cache manage logic for mamba + MTP ([#25119](https://github.com/vllm-project/vllm/pull/25119)) by @heheda12345
* [Core] Refactor self.model() to call a helper for subclassing. ([#25084](https://github.com/vllm-project/vllm/pull/25084)) by @patrick-toulme
* [Bugfix][WideEP] Apply TP Attn + EP MoE fix to other models ([#24982](https://github.com/vllm-project/vllm/pull/24982)) by @tlrmchlsmth
* Run:ai model streamer add GCS package support ([#24909](https://github.com/vllm-project/vllm/pull/24909)) by @pwschuurman
* [Qwen][ROCm] Flash Attention Rotary Embeddings ([#24642](https://github.com/vllm-project/vllm/pull/24642)) by @vllmellm
* Re-enable prefill of max model length ([#24446](https://github.com/vllm-project/vllm/pull/24446)) by @yannicks1
* Eagle3 that supports the Minicpm3 model ([#24243](https://github.com/vllm-project/vllm/pull/24243)) by @LDLINGLINGLING
* [Model] Mamba2 varlen and metadata refactor  ([#21467](https://github.com/vllm-project/vllm/pull/21467)) by @cyang49

## Hardware & Backend

* Avoid division by zero in cache DS MLA kernel ([#26174](https://github.com/vllm-project/vllm/pull/26174)) by @MatthewBonanni
* [ROCm] [VL] [Bugfix] Fix vit flash attn dispatcher logic for ROCm ([#26104](https://github.com/vllm-project/vllm/pull/26104)) by @tjtanaa
* Change size of single CUDA graph for CI to 4 ([#26089](https://github.com/vllm-project/vllm/pull/26089)) by @tdoublep
* [Bugfix] Fix import `gemm_afp4wfp4` failure on AMD ([#26068](https://github.com/vllm-project/vllm/pull/26068)) by @zhewenl
* [ROCm][Bugfix] Add missing parameter to ROCm backend ([#26029](https://github.com/vllm-project/vllm/pull/26029)) by @gshtras
* [Bugfix] Fix `__syncwarp` on ROCM ([#25996](https://github.com/vllm-project/vllm/pull/25996)) by @zhewenl
* [CI/Build] do not enforce precompilation on tpu ci tests ([#25992](https://github.com/vllm-project/vllm/pull/25992)) by @sixiang-google
* Quick fix for IMA with the Prefix Prefill kernel during graph capture ([#25983](https://github.com/vllm-project/vllm/pull/25983)) by @SageMoore
* [bugfix][deepseek] fix flashmla kernel selection ([#25956](https://github.com/vllm-project/vllm/pull/25956)) by @youkaichao
* [CI] Only capture a single CUDA graph size in CI by default ([#25951](https://github.com/vllm-project/vllm/pull/25951)) by @hmellor
* [ROCm][Build] Add support for AMD Ryzen AI MAX / AI 300 Series ([#25908](https://github.com/vllm-project/vllm/pull/25908)) by @hyoon1
* [Attention] Move Backend enum into registry ([#25893](https://github.com/vllm-project/vllm/pull/25893)) by @MatthewBonanni
* [torch.compile] serialize cudagraph_mode as its enum name instead of value ([#25868](https://github.com/vllm-project/vllm/pull/25868)) by @ZJY0516
* [Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 ([#25858](https://github.com/vllm-project/vllm/pull/25858)) by @Josephasafg
* [XPU]Fix xpu spec decoding UTs, avoid using cuda graph ([#25847](https://github.com/vllm-project/vllm/pull/25847)) by @jikunshang
* [MISC] Fix misleading batch_size_capture_list when cuda_graph_sizes < 4 ([#25829](https://github.com/vllm-project/vllm/pull/25829)) by @billishyahao
* [Misc] Make EP kernels install script support uv ([#25785](https://github.com/vllm-project/vllm/pull/25785)) by @LucasWilkinson
* Reduce the Cuda Graph memory footprint when running with DBO ([#25779](https://github.com/vllm-project/vllm/pull/25779)) by @SageMoore
* [Bug]: Set LD_LIBRARY_PATH to include the 'standard' CUDA location ([#25766](https://github.com/vllm-project/vllm/pull/25766)) by @smarterclayton
* [Bug] Fix Negative Cuda Memory Usage ([#25683](https://github.com/vllm-project/vllm/pull/25683)) by @yewentao256
* [Bugfix][ROCm] Fixing trying to import non-existent symbols from libnccl.so ([#25605](https://github.com/vllm-project/vllm/pull/25605)) by @gshtras
* Kernel-override Determinism [1/n] ([#25603](https://github.com/vllm-project/vllm/pull/25603)) by @bwasti
* [Docs] Add moe kernel features doc  ([#25297](https://github.com/vllm-project/vllm/pull/25297)) by @bnellnm
* Move`VllmConfig` from `config/__init__.py` to `config/vllm.py` ([#25271](https://github.com/vllm-project/vllm/pull/25271)) by @hmellor
* add(v1): RequestStatesStats to RequestOutput ([#24947](https://github.com/vllm-project/vllm/pull/24947)) by @huijjj
* [Cuda2CPU][P/D] Add cuda2cpu support in NixlConnector ([#24690](https://github.com/vllm-project/vllm/pull/24690)) by @chenxi-yang
* [NVIDIA] Blackwell Family ([#24673](https://github.com/vllm-project/vllm/pull/24673)) by @johnnynunez
* [backends][short_conv] CUDA graph piecewise edits ([#24215](https://github.com/vllm-project/vllm/pull/24215)) by @paulpak58
* [CI] Move applicable tests to CPU ([#24080](https://github.com/vllm-project/vllm/pull/24080)) by @rzabarazesh

## Refactoring & Core

* Validate API tokens in constant time ([#25781](https://github.com/vllm-project/vllm/pull/25781)) by @russellb
* [responsesAPI] add better error messaging for long prompts ([#25724](https://github.com/vllm-project/vllm/pull/25724)) by @qandrew
* [misc] refactor speculative config ([#25657](https://github.com/vllm-project/vllm/pull/25657)) by @yyzxw
* [docs] Resolve transcriptions API TODO ([#25446](https://github.com/vllm-project/vllm/pull/25446)) by @yyzxw

## Build, CI & Testing

* [CI] Fix Pre-commit Mypy Error ([#26181](https://github.com/vllm-project/vllm/pull/26181)) by @yewentao256
* [test utils] correct wrong typing ([#26159](https://github.com/vllm-project/vllm/pull/26159)) by @yannicks1
* [CI] Fix distributed hybrid tests in CI ([#26155](https://github.com/vllm-project/vllm/pull/26155)) by @tdoublep
* [CI/Build] Conditionally register cutlass_fp4_group_mm to fix building on Hopper ([#26138](https://github.com/vllm-project/vllm/pull/26138)) by @mgoin
* [CI] Add Blackwell DeepSeek FP8 FlashInfer MoE tests ([#26040](https://github.com/vllm-project/vllm/pull/26040)) by @mgoin
* [CI/Build] Include Transformers backend test in nightly transformers test ([#25885](https://github.com/vllm-project/vllm/pull/25885)) by @Isotr0py
* update to latest deepgemm for dsv3.2 ([#25871](https://github.com/vllm-project/vllm/pull/25871)) by @youkaichao
* [Misc] fix tests failure by using current_platform ([#25825](https://github.com/vllm-project/vllm/pull/25825)) by @kingsmad
* [CI/Build] Reorganize root-level V1 tests ([#25767](https://github.com/vllm-project/vllm/pull/25767)) by @DarkLight1337
* [CI] Push multiarch manifests as nightly builds ([#25764](https://github.com/vllm-project/vllm/pull/25764)) by @csahithi
* [CI] Fix test_shared_storage_connector_hashes ([#25748](https://github.com/vllm-project/vllm/pull/25748)) by @chaunceyjiang
* [CI/Build] fix doc build warning: Failed to get 'name: description' pair ([#25733](https://github.com/vllm-project/vllm/pull/25733)) by @yitingdc
* [CI] Fix FlashInfer AOT in release docker image ([#25730](https://github.com/vllm-project/vllm/pull/25730)) by @mgoin
* [CI] Add E2E Blackwell Quantized MoE Test ([#25723](https://github.com/vllm-project/vllm/pull/25723)) by @mgoin
* [CI/Build] Split up Distributed Tests ([#25572](https://github.com/vllm-project/vllm/pull/25572)) by @DarkLight1337
* [CI/Build] Fix some V1 tests not being run ([#25569](https://github.com/vllm-project/vllm/pull/25569)) by @DarkLight1337
* [Platform][CI] Added OOT platform interface e2e test that running on Ascend NPU ([#25470](https://github.com/vllm-project/vllm/pull/25470)) by @leo-pony

## Documentation

* [Small] Prevent bypassing media domain restriction via HTTP redirects ([#26035](https://github.com/vllm-project/vllm/pull/26035)) by @huachenheli
* [Doc] updating torch.compile doc link ([#25989](https://github.com/vllm-project/vllm/pull/25989)) by @nadathurv
* [Doc] Add Cambricon MLU support ([#25942](https://github.com/vllm-project/vllm/pull/25942)) by @a120092009
* [Doc] Polish example for torchrun dp ([#25899](https://github.com/vllm-project/vllm/pull/25899)) by @zhuohan123
* [NIXL] Increase default KV block eviction timeout on P ([#25897](https://github.com/vllm-project/vllm/pull/25897)) by @NickLucche
* [Bugfix] Fix requirements paths in install instructions ([#25827](https://github.com/vllm-project/vllm/pull/25827)) by @yingjun-mou
* [Doc] Add documentation for vLLM continuous benchmarking and profiling ([#25819](https://github.com/vllm-project/vllm/pull/25819)) by @namanlalitnyu
* [Misc] Update openai client example file for multimodal ([#25795](https://github.com/vllm-project/vllm/pull/25795)) by @ywang96
* [Docs] Add Toronto Meetup ([#25773](https://github.com/vllm-project/vllm/pull/25773)) by @mgoin
* [Doc] Update Batch-level DP docs ([#25757](https://github.com/vllm-project/vllm/pull/25757)) by @DarkLight1337
* Updated TRL integration docs ([#25684](https://github.com/vllm-project/vllm/pull/25684)) by @sergiopaniego
* [Doc] Fixed shape description for fused_batched_moe.py ([#25668](https://github.com/vllm-project/vllm/pull/25668)) by @Egor-Krivov
* [Multi Modal] Configurable MM Profiling ([#25631](https://github.com/vllm-project/vllm/pull/25631)) by @wwl2755
* [Core] GC Debug callback ([#24829](https://github.com/vllm-project/vllm/pull/24829)) by @Jialin
* [Renderer] Move Processor out of AsyncLLM  ([#24138](https://github.com/vllm-project/vllm/pull/24138)) by @KKSK-DON
* `FusedMoE` support for the Transformers backend ([#22650](https://github.com/vllm-project/vllm/pull/22650)) by @hmellor
* [V1] [P/D] Add Support for KV Load Failure Recovery ([#19330](https://github.com/vllm-project/vllm/pull/19330)) by @sdavidbd

## Miscellaneous

* Stop mergify from keeping stale PRs alive ([#26169](https://github.com/vllm-project/vllm/pull/26169)) by @hmellor
* [Renderer] Move Processor out of LLMEngine ([#26165](https://github.com/vllm-project/vllm/pull/26165)) by @DarkLight1337
* [Bug]: Limit num_reqs in dummy_run when max_num_seqs is small ([#26144](https://github.com/vllm-project/vllm/pull/26144)) by @benchislett
* [Bugfix] Disable cascade attention with FlashInfer ([#26130](https://github.com/vllm-project/vllm/pull/26130)) by @mgoin
* [Misc] Make handling of SamplingParams clearer in n>1 case ([#26032](https://github.com/vllm-project/vllm/pull/26032)) by @njhill
* [Bugfix] Apply same sampling parameters for both `n=1` and `n>1` ([#26005](https://github.com/vllm-project/vllm/pull/26005)) by @kmaehashi
* [Misc] Factor out common `_apply_feature_select_strategy` ([#26003](https://github.com/vllm-project/vllm/pull/26003)) by @DarkLight1337
* [Deepseek v3.2] Support indexer prefill chunking ([#25999](https://github.com/vllm-project/vllm/pull/25999)) by @heheda12345
* [BugFix] Fix default kv-cache-dtype default for DeepseekV3.2 ([#25988](https://github.com/vllm-project/vllm/pull/25988)) by @LucasWilkinson
* [Bug] Fix AttributeError: 'QKVParallelLinear' object has no attribute 'orig_dtype' ([#25958](https://github.com/vllm-project/vllm/pull/25958)) by @yewentao256
* [Bugfix] Token type and position embeddings fail to be applied to `inputs_embeds` ([#25922](https://github.com/vllm-project/vllm/pull/25922)) by @DarkLight1337
* [BugFix] Pass config_format via try_get_generation_config ([#25912](https://github.com/vllm-project/vllm/pull/25912)) by @acisseJZhong
* [BugFix] Fix DP/EP hang  ([#25906](https://github.com/vllm-project/vllm/pull/25906)) by @LucasWilkinson
* [NIXL] Add support for MLA caches with different latent dim ([#25902](https://github.com/vllm-project/vllm/pull/25902)) by @NickLucche
* [Bugfix][Speculative Decoding] Fix Eagle3 quantization config issue ([#25883](https://github.com/vllm-project/vllm/pull/25883)) by @rahul-tuli
* OffloadingConnector: Fix GPU block tracking bug ([#25856](https://github.com/vllm-project/vllm/pull/25856)) by @orozery
* [Bugfix] Fallback ViT attn backend to SDPA for blackwell ([#25851](https://github.com/vllm-project/vllm/pull/25851)) by @ywang96
* [P/D] NIXL Updates ([#25844](https://github.com/vllm-project/vllm/pull/25844)) by @robertgshaw2-redhat
* [Bugfix][NIXL] Fix Async Scheduler timeout issue ([#25808](https://github.com/vllm-project/vllm/pull/25808)) by @NickLucche
* [Bugfix] Fix triton import precommit failure ([#25803](https://github.com/vllm-project/vllm/pull/25803)) by @tlrmchlsmth
* [Bugfix] Add missing `image_size` for phi4_multimodal ([#25796](https://github.com/vllm-project/vllm/pull/25796)) by @Renovamen
* [Core] Don't count preempted tokens in prefix cache hit rate ([#25787](https://github.com/vllm-project/vllm/pull/25787)) by @zhuohan123
* [Bugfix] Properly abort pooling request. ([#25734](https://github.com/vllm-project/vllm/pull/25734)) by @noooop
* [Fix][torch.compile] fix unique_filepath ([#25732](https://github.com/vllm-project/vllm/pull/25732)) by @ZJY0516
* [Misc] Don't log shm dequeue delay warning on worker side ([#25720](https://github.com/vllm-project/vllm/pull/25720)) by @njhill
* [Bugfix] Use correct key "ignore" for config.json non-quantized layers ([#25706](https://github.com/vllm-project/vllm/pull/25706)) by @leejnau
* [Bugfix] Fix Shared Expert/Zero expert code in FusedMoE.process_chunk ([#25698](https://github.com/vllm-project/vllm/pull/25698)) by @SageMoore
* [torch.compile]: Add VLLM_DEBUG_DUMP_PATH environment variable ([#25651](https://github.com/vllm-project/vllm/pull/25651)) by @ZJY0516
* [BugFix] Fix using `dbo_decode_token_threshold` always (and ignoring `dbo_prefill_token_threshold`) ([#25622](https://github.com/vllm-project/vllm/pull/25622)) by @LucasWilkinson
* [FA/Chore] Bump vllm-flash-attention ([#25537](https://github.com/vllm-project/vllm/pull/25537)) by @LucasWilkinson
* [ray][metrics] Replace ':' with '_' for OpenTelemetry compatibility in Ray ([#25439](https://github.com/vllm-project/vllm/pull/25439)) by @eicherseiji
* [Misc]allow disable pynccl ([#25421](https://github.com/vllm-project/vllm/pull/25421)) by @luccafong
* [NIXL][Misc] Expose metrics from NIXL for logging to CLI ([#25388](https://github.com/vllm-project/vllm/pull/25388)) by @NickLucche
* [env] default nixl side port conflicts  with kv-event zmq port ([#25056](https://github.com/vllm-project/vllm/pull/25056)) by @panpan0000
* [Misc] Fix codeowners override for v1 sample and attention ([#25037](https://github.com/vllm-project/vllm/pull/25037)) by @22quinn
* [openai] Fix missing tool usage check (system message) ([#24768](https://github.com/vllm-project/vllm/pull/24768)) by @levunet
* Update to Transformers `v4.56.2` ([#24638](https://github.com/vllm-project/vllm/pull/24638)) by @hmellor
* [BugFix] Fix de-functionalization pass for rotary_embedding ([#23953](https://github.com/vllm-project/vllm/pull/23953)) by @angelayi
* [V1] address post issues related to #20059 (part 1); cascade attention reenable by default ([#23046](https://github.com/vllm-project/vllm/pull/23046)) by @fhl2000
* EVS Support (Video tokens pruning) ([#22980](https://github.com/vllm-project/vllm/pull/22980)) by @BloodAxe

## Breaking Changes

* [Perf] Remove hardcoded num_warps=1 ([#26183](https://github.com/vllm-project/vllm/pull/26183)) by @coreylowman
* [Misc] Remove typing.List ([#26150](https://github.com/vllm-project/vllm/pull/26150)) by @varun-sundar-rabindranath
* [Build/CI] Revert back to Ubuntu 20.04, install python 3.12 with uv ([#26103](https://github.com/vllm-project/vllm/pull/26103)) by @tlrmchlsmth
* [Input] Remove unused `prompt` field ([#26097](https://github.com/vllm-project/vllm/pull/26097)) by @DarkLight1337
* Update base image to 22.04 (jammy) ([#26065](https://github.com/vllm-project/vllm/pull/26065)) by @huydhn
* [Benchmark] Finish documented v0.11.0 deprecation of --endpoint-type ([#26007](https://github.com/vllm-project/vllm/pull/26007)) by @natoscott
* Fix test_mamba_ssm_ssd.py due to missing _query_start_loc_to_chunk_indices_offsets ([#25995](https://github.com/vllm-project/vllm/pull/25995)) by @hl475
* [Docs] Remove API Reference from search index ([#25949](https://github.com/vllm-project/vllm/pull/25949)) by @hmellor
* [Model][Bugfix] Fix MiDashengLM audio encoder mask by removing incorrect `logical_not` ([#25925](https://github.com/vllm-project/vllm/pull/25925)) by @zhoukezi
* [V0 Deprecation] Remove `vllm.worker` and update according imports ([#25901](https://github.com/vllm-project/vllm/pull/25901)) by @aarnphm
* [Model] Remove MotifForCausalLM ([#25866](https://github.com/vllm-project/vllm/pull/25866)) by @jeejeelee
* [Misc] Remove more `get_input_embeddings_v0` ([#25857](https://github.com/vllm-project/vllm/pull/25857)) by @DarkLight1337
* Remove redundant cudagraph dispatcher warning ([#25841](https://github.com/vllm-project/vllm/pull/25841)) by @mgoin
* [Misc] Remove unnecessary memoryviews in shm_broadcast.py ([#25721](https://github.com/vllm-project/vllm/pull/25721)) by @njhill
* [Refactor] Remove DeepGEMM OP Register ([#25710](https://github.com/vllm-project/vllm/pull/25710)) by @yewentao256
* [Doc]: improve CPU(x86) build-wheel-from-source section ([#25617](https://github.com/vllm-project/vllm/pull/25617)) by @brokedba
* Remove cuda hard-code in compute_causal_conv1d_metadata ([#25555](https://github.com/vllm-project/vllm/pull/25555)) by @wxsIcey
* [BugFix][torch.compile] KV scale calculation issues with FP8 quantization (#21640) ([#25513](https://github.com/vllm-project/vllm/pull/25513)) by @adabeyta
* [Bugfix] Improve GLM4 MoE Reasoning Parser's is_reasoning_end Condition ([#25355](https://github.com/vllm-project/vllm/pull/25355)) by @frankwang28
* [V0 Deprecation][Models] Remove all V0 condition for mm embeddings merge ([#25331](https://github.com/vllm-project/vllm/pull/25331)) by @Isotr0py
* [spec decode] Consolidate speculative decode method name for MTP ([#25232](https://github.com/vllm-project/vllm/pull/25232)) by @zixi-qi
* [Bugfix]: Clean up chunked prefill logging when using whisper ([#25075](https://github.com/vllm-project/vllm/pull/25075)) by @simondanielsson
* [Kernel] Chunk-aligned mamba2 ([#24683](https://github.com/vllm-project/vllm/pull/24683)) by @tdoublep
* [Bugfix] Merge MM embeddings by index instead of token IDs ([#16229](https://github.com/vllm-project/vllm/pull/16229)) by @DarkLight1337

## Upgrade Notes

- NOTE: The accuracy by no means indicates the actual model performance on benchmark and the accuracy is not evaluate through the same procedure used in the official release.
- Revert back to 20.04 to avoid breaking the wheel for users on OSes such as UBI 9 that have an earlier glibc version than 20.04.
- Note: it takes 9:58 with MTP, w/o MTP it is 12:08 (25% speedup)
- Note: This by itself is not sufficient to enable prompt embedding inputs (i.e. user inputs `inputs_embeds` directly), because we still need the information of `token_type_ids`.
- **Note:** This architectural separation enables future x86/ARM-specific improvements without cross-contamination.
- The upcoming NIXL release will also allow setting telemetry through config (thanks @mkhazraee ) so I would propose we get rid of the env var handling altogether with the next upgrade.
- Note: this has incorporated changes from @LiyuanLucasLiu's PR: https://github.com/vllm-project/vllm/pull/23901, although vllm fp8 quant method is not supported yet, we can add that in a separate PR
- ## Breaking change for model developers

## Contributors

@22quinn, @Anionex, @BloodAxe, @BowenBao, @CSWYF3634076, @DarkLight1337, @DrStone71, @Egor-Krivov, @ElizaWszola, @HAIAI, @Iceber, @Isotr0py, @JJJYmmm, @Jialin, @Josephasafg, @KKSK-DON, @LDLINGLINGLING, @LucasWilkinson, @MatthewBonanni, @NickLucche, @Renovamen, @SageMoore, @Xu-Wenqing, @ZJY0516, @a120092009, @aarnphm, @acisseJZhong, @adabeyta, @ahao-anyscale, @angelayi, @astralord, @benchislett, @billishyahao, @bnellnm, @brokedba, @bwasti, @chaunceyjiang, @chenxi-yang, @cjackal, @coreylowman, @csahithi, @cyang49, @david6666666, @eicherseiji, @ekagra-ranjan, @fhl2000, @frankwang28, @gshtras, @heheda12345, @hl475, @hmellor, @huachenheli, @huijjj, @huydhn, @hyoon1, @ihb2032, @jasl, @jeejeelee, @jerryzh168, @jikunshang, @jmkuebler, @johnnynunez, @kingsmad, @kmaehashi, @leejnau, @leo-pony, @levunet, @lhtin, @luccafong, @maleksan85, @mgoin, @nadathurv, @namanlalitnyu, @natoscott, @njhill, @noooop, @nrghosh, @orozery, @padg9912, @panpan0000, @patrick-toulme, @paulpak58, @pavanimajety, @pwschuurman, @qandrew, @qthequartermasterman, @rahul-tuli, @robertgshaw2-redhat, @russellb, @rzabarazesh, @sdavidbd, @sergiopaniego, @sighingnow, @simondanielsson, @sixiang-google, @smarterclayton, @southfreebird, @tdoublep, @tjtanaa, @tlrmchlsmth, @varun-sundar-rabindranath, @vllmellm, @weireweire, @whx-sjtu, @wwl2755, @wxsIcey, @xaguilar-amd, @xuechendi, @yannicks1, @yewentao256, @yingjun-mou, @yitingdc, @youkaichao, @ywang96, @yyzxw, @zRzRzRzRzRzRzR, @zhewenl, @zhoukezi, @zhuohan123, @zixi-qi, @zyongye