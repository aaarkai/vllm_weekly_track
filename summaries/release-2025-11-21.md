# Weekly Release Report for vllm-project/vllm (2025-11-21)

This week merged 238 PRs from 143 contributors. Key areas: features 6, fixes 20, performance 20.

## Executive Summary

本周vLLM发布引入了多项关键更新。主要特性包括CPU WNA16重构、Gemma3 GGUF多模态支持、rope_scaling参数更名为rope_parameters以适配Transformers v5，并新增Eagle模型对独立lm-head和embed_tokens层的支持。此外，ROCm扩展了AMD GPU在Deepseek v3.2和SparseMLA的兼容性。

修复方面解决了多个关键问题，涵盖DeepSeek V3.2 Rope Embedding、kv_offloading部分CPU块加载、精度损坏及异步调度竞争条件等。性能优化聚焦于共享专家重叠、QwenVL旋转嵌入索引简化及DeepGEMM维度限制调整，提升了吞吐与响应效率。模型支持强化了Qwen3-VL多模态处理、LoRA适配及量化功能，但需注意ROCm导入修复可能影响gpt-oss服务，建议升级前验证环境兼容性。

## Highlights

* [CPU] Refactor CPU WNA16  ([#28826](https://github.com/vllm-project/vllm/pull/28826)) by @bigPYJ1151
* [Model] Add Gemma3 GGUF multimodal support ([#27772](https://github.com/vllm-project/vllm/pull/27772)) by @lucianommartins
* Update `rope_scaling` to `rope_parameters` in preparation for Transformers v5 ([#28542](https://github.com/vllm-project/vllm/pull/28542)) by @hmellor
* Add support for Eagle with separate lm-head and embed_tokens layers ([#28549](https://github.com/vllm-project/vllm/pull/28549)) by @eldarkurtic
* [ROCm] Add AMD GPU support on Deepseek v3.2 and SparseMLA ([#26670](https://github.com/vllm-project/vllm/pull/26670)) by @ganyi1996ppo

## Features & Enhancements

* add support for --fully-sharded-loras in fused_moe ([#28761](https://github.com/vllm-project/vllm/pull/28761)) by @gnovack
* Add CPU support model ([#28697](https://github.com/vllm-project/vllm/pull/28697)) by @louie-tsai
* Add output token counting to gsm8k eval ([#28594](https://github.com/vllm-project/vllm/pull/28594)) by @mgoin
* Add support for Eagle with separate lm-head and embed_tokens layers ([#28549](https://github.com/vllm-project/vllm/pull/28549)) by @eldarkurtic
* Add truncate arg to yarn to match openai implementation of gpt-oss ([#28244](https://github.com/vllm-project/vllm/pull/28244)) by @ashors1
* Feature: Support Relu2 in FusedMoE fp8 cutlass path ([#27261](https://github.com/vllm-project/vllm/pull/27261)) by @amirkl94

## Bug Fixes

* Fixes bench ([#29058](https://github.com/vllm-project/vllm/pull/29058)) by @drisspg
* [DeepSeek] Fix DeepSeek V3.2 Rope Embedding ([#28968](https://github.com/vllm-project/vllm/pull/28968)) by @zyongye
* [BugFix] kv_offloading: Fix bug in loading of partial cpu blocks ([#28951](https://github.com/vllm-project/vllm/pull/28951)) by @orozery
* [Bugfix]  Fix precision corruption when shared_experts_stream=None ([#28942](https://github.com/vllm-project/vllm/pull/28942)) by @zhyajie
* [BugFix] Fix PP/async scheduling with pooling models ([#28899](https://github.com/vllm-project/vllm/pull/28899)) by @njhill
* [BugFix] Fix glm4_moe_mtp load weights bug ([#28805](https://github.com/vllm-project/vllm/pull/28805)) by @wuyaoxuehun
* fix comment typo ([#28802](https://github.com/vllm-project/vllm/pull/28802)) by @andyxning
* [BugFix] Fix async scheduling + chunked prefill + preemption ([#28787](https://github.com/vllm-project/vllm/pull/28787)) by @njhill
* [BugFix] Fix PP performance and PP kv connector output regression  ([#28768](https://github.com/vllm-project/vllm/pull/28768)) by @njhill
* Fix gpt oss weight loading with EP + bf16 ([#28765](https://github.com/vllm-project/vllm/pull/28765)) by @ashors1
* [Bugfix] Build hadacore kernels on >SM90 ([#28748](https://github.com/vllm-project/vllm/pull/28748)) by @mgoin
* Fix IntermediateTensors initialization and add type hints ([#28743](https://github.com/vllm-project/vllm/pull/28743)) by @OthmanMohammad
* [Bugfix] Fix incorrect use of hidden_states for shared_experts due to do_naive_dispatch_combine ([#28740](https://github.com/vllm-project/vllm/pull/28740)) by @alexm-redhat
* Fix typo in comment: existance -> existence ([#28737](https://github.com/vllm-project/vllm/pull/28737)) by @OthmanMohammad
* [BugFix] Fix misprint introduced by modular_kernel refactoring. ([#28728](https://github.com/vllm-project/vllm/pull/28728)) by @halyavin
* Fixed gpt-oss _load_weights_other() parameter position bug ([#28715](https://github.com/vllm-project/vllm/pull/28715)) by @River12
* [BugFix] Fix multi-modal async scheduling race condition ([#28706](https://github.com/vllm-project/vllm/pull/28706)) by @njhill
* [BugFix] Fix FA3 IMA with FULL_AND_PIECEWISE and cascade attention (default) ([#28702](https://github.com/vllm-project/vllm/pull/28702)) by @LucasWilkinson
* [CPU][Bugfix] Fix Apple Silicon M1 compilation failure ([#28681](https://github.com/vllm-project/vllm/pull/28681)) by @mgoin
* Fix KV sharing fast prefill with cudagraph enabled ([#28537](https://github.com/vllm-project/vllm/pull/28537)) by @sarckk

## Performance

* Speed up macOS smoke test ([#28954](https://github.com/vllm-project/vllm/pull/28954)) by @mgoin
* [Benchmark] multi_turn: Report warmup-inclusive runtime ([#28937](https://github.com/vllm-project/vllm/pull/28937)) by @segevido
* [Feature] Shared Experts Overlap with FI deepgemm swap kernel, 2.2% throughput improvement and 3.6% TTFT improvement ([#28879](https://github.com/vllm-project/vllm/pull/28879)) by @yewentao256
* [Bugfix][Perf] Revert applying HF processor on text-only inputs for multimodal models  ([#28858](https://github.com/vllm-project/vllm/pull/28858)) by @ywang96
* [Model][Perf] Use cos and sin cache in QwenVL ([#28798](https://github.com/vllm-project/vllm/pull/28798)) by @gcanlin
* Revert "[Core] Performance: Use list[np.ndarray] instead of list[list… ([#28773](https://github.com/vllm-project/vllm/pull/28773)) by @njhill
* [Model][QwenVL] Optimize `Qwen2_5_VisionAttention` q,k preparation ([#28769](https://github.com/vllm-project/vllm/pull/28769)) by @lgeiger
* [Performance][DeepGEMM] Estimate expected_m ([#28694](https://github.com/vllm-project/vllm/pull/28694)) by @varun-sundar-rabindranath
* [Performance] Reduce DeepGEMM N dim restriction from 128 to 64 multiplier  ([#28687](https://github.com/vllm-project/vllm/pull/28687)) by @alexm-redhat
* [Bugfix] Fix host and port join for ipv6 in bench serve ([#28679](https://github.com/vllm-project/vllm/pull/28679)) by @scottzh8
* [Hybrid][torch.compile] Refactor mamba2 forward to avoid obscuring linear projections under custom op ([#28587](https://github.com/vllm-project/vllm/pull/28587)) by @tomeras91
* [Performance][Fix] update nvfp4 code to support renorm routing ([#28569](https://github.com/vllm-project/vllm/pull/28569)) by @jiahanc
* [Benchmark] Fix client seed synchronization in multi-turn benchmark ([#28512](https://github.com/vllm-project/vllm/pull/28512)) by @ai-jz
* [Feat][Perf] Enable deepep-low-latency with round-robin expert placement. ([#28449](https://github.com/vllm-project/vllm/pull/28449)) by @cboss6
* [kernel][perf] support uncontiguous input for rms_norm kernel ([#28103](https://github.com/vllm-project/vllm/pull/28103)) by @izhuhaoran
* [Refactor] Optimize `select_experts` ([#28069](https://github.com/vllm-project/vllm/pull/28069)) by @yewentao256
* Replace `torch.cuda.Event` with `torch.Event` for better hardware compatibility ([#26985](https://github.com/vllm-project/vllm/pull/26985)) by @jikunshang
* [ROCm] Add AMD GPU support on Deepseek v3.2 and SparseMLA ([#26670](https://github.com/vllm-project/vllm/pull/26670)) by @ganyi1996ppo
* [Core] Performance: Use list[np.ndarray] instead of list[list[int]] for output tokens for GC optimization ([#26368](https://github.com/vllm-project/vllm/pull/26368)) by @Jialin
* [Frontend] Optimize beam search loop by sorting and then splicing ([#19347](https://github.com/vllm-project/vllm/pull/19347)) by @zhanggzh

## Model Support

* [AITER] [ROCm] Fix crash when loading llama4 model with old aiter version installed, fallback to forward_native implementation ([#29124](https://github.com/vllm-project/vllm/pull/29124)) by @xli
* Update model references for OLMo3 ([#29099](https://github.com/vllm-project/vllm/pull/29099)) by @mgoin
* [DeepSeek + LMCache Multiprocess] handle MLA for deepseek model + LMCache Multiprocess connector ([#29039](https://github.com/vllm-project/vllm/pull/29039)) by @KuntaiDu
* [Model][QwenVL] Replace `torch.repeat_interleave` with faster `np.repeat` ([#28964](https://github.com/vllm-project/vllm/pull/28964)) by @lgeiger
* [Model][QwenVL] Simplify cos/sin rotary embedding indexing  ([#28962](https://github.com/vllm-project/vllm/pull/28962)) by @lgeiger
* [config] Expose `get_total_num_hidden_layers()` in ModelConfig ([#28961](https://github.com/vllm-project/vllm/pull/28961)) by @ptovam
* [Bugfix] Fix typo in Qwen3 Next model executor ([#28960](https://github.com/vllm-project/vllm/pull/28960)) by @Nepherpitou
* Relax Transformers modeling backend MoE experts check ([#28952](https://github.com/vllm-project/vllm/pull/28952)) by @hmellor
* Supress verbose logs from model_hosting_container_standards ([#28949](https://github.com/vllm-project/vllm/pull/28949)) by @mgoin
* GLM-V video segmentation solution adjustment ([#28941](https://github.com/vllm-project/vllm/pull/28941)) by @zRzRzRzRzRzRzR
* [Feature] EPLB on Qwen3VLMoe and CompressedTensorsWNA16MoEMethod ([#28849](https://github.com/vllm-project/vllm/pull/28849)) by @JartX
* [Models] Replace all `nn.Conv2d` with vLLM's Conv2dLayer ([#28842](https://github.com/vllm-project/vllm/pull/28842)) by @Isotr0py
* [Doc] Add llama4 LoRA tag ([#28825](https://github.com/vllm-project/vllm/pull/28825)) by @jeejeelee
* [Bugfix] Fix GPT-OSS on AMD after #28603 ([#28816](https://github.com/vllm-project/vllm/pull/28816)) by @zhewenl
* [Model] Fix lmhead init bug of bailing_moe ([#28777](https://github.com/vllm-project/vllm/pull/28777)) by @hwhaokun
* [Model][Qwen3VL] Use `mm_position` to compute mrope positions ([#28730](https://github.com/vllm-project/vllm/pull/28730)) by @lgeiger
* [Docs] Update the name of `Transformers backend` -> `Transformers modeling backend` ([#28725](https://github.com/vllm-project/vllm/pull/28725)) by @hmellor
* [Kernel][Moe Configs] llama4 maverick fp8 moe config tp8 on mi325 ([#28709](https://github.com/vllm-project/vllm/pull/28709)) by @zhewenl
* [ROCm][Qwen3-32B] Fix AITER MHA accuracy issue cause by #25763 ([#28670](https://github.com/vllm-project/vllm/pull/28670)) by @sammysun0711
* [Bugfix] resolve Qwen3-VL GPTQModel quantized model loading failure ([#28663](https://github.com/vllm-project/vllm/pull/28663)) by @GuanH
* [ROCm][Quantization] add apply_vllm_mapper in quark config for models like gpt-oss ([#28638](https://github.com/vllm-project/vllm/pull/28638)) by @xuebwang-amd
* LLaMA4 LoRA Adapter Enablement ([#28602](https://github.com/vllm-project/vllm/pull/28602)) by @kfhfar
* [BugFix] Fix Llama4 Pipeline Parallelism Assert Error ([#28577](https://github.com/vllm-project/vllm/pull/28577)) by @River12
* [Model][Qwen3VL] Cache positional embedding indices  ([#28475](https://github.com/vllm-project/vllm/pull/28475)) by @lgeiger
* [Model][MM] Extract conv layer as CustomOp ([#28455](https://github.com/vllm-project/vllm/pull/28455)) by @shen-shanshan
* [Quantization] [Eagle] Add complete quantization support to the draft model in Eagle ([#28435](https://github.com/vllm-project/vllm/pull/28435)) by @shreyas269
* [Model] Add Afmoe architecture implementation ([#28332](https://github.com/vllm-project/vllm/pull/28332)) by @pranav4501
* [Model] Fix bailing_moe accuracy problem ([#28277](https://github.com/vllm-project/vllm/pull/28277)) by @zhaozx-cn
* [Model] Allow users to control skip reading cache per request. ([#28194](https://github.com/vllm-project/vllm/pull/28194)) by @noooop
* Consolidate Nvidia ModelOpt quant config handling for all quantization methods ([#28076](https://github.com/vllm-project/vllm/pull/28076)) by @shengliangxu
* [Model][Mamba] Add selector for mamba attention backend and make it pluggable for other device ([#26487](https://github.com/vllm-project/vllm/pull/26487)) by @shen-shanshan
* Move online quantization to `model.load_weights` ([#26327](https://github.com/vllm-project/vllm/pull/26327)) by @jerryzh168

## Hardware & Backend

* [CI Bugfix] Fix Kernels DeepGEMM Test (H100) ([#29106](https://github.com/vllm-project/vllm/pull/29106)) by @mgoin
* [CI/Build][AMD] Skip if flash_attn_varlen_func not available in test_aiter_flash_attn.py ([#29043](https://github.com/vllm-project/vllm/pull/29043)) by @rasmith
* [CI/Build][AMD] Fix import errors in tests/kernels/attention ([#29032](https://github.com/vllm-project/vllm/pull/29032)) by @rasmith
* [CI/Build] Skip lm-format-enforcer tests in test_struct_output_generate.py for now ([#29021](https://github.com/vllm-project/vllm/pull/29021)) by @rasmith
* [Bugfix] Move flashinfer kernel check into ```__init__``` function of ```FusedMoE``` ([#29018](https://github.com/vllm-project/vllm/pull/29018)) by @maxyanghu
* Updating the mirror of test-amd.yaml as of 2025-11-18 ([#29016](https://github.com/vllm-project/vllm/pull/29016)) by @Alexei-V-Ivanov-AMD
* [Kernels] Improve H200 Fused MoE Config ([#28992](https://github.com/vllm-project/vllm/pull/28992)) by @robertgshaw2-redhat
* [Feat] Iteration-level profiling for Torch and CUDA profiler ([#28987](https://github.com/vllm-project/vllm/pull/28987)) by @benchislett
* [ROCm][CI] Fix Weight Loading With Multiple GPU Tests on ROCm ([#28984](https://github.com/vllm-project/vllm/pull/28984)) by @micah-wil
* [NVIDIA] Guard SM100 CUTLASS MoE macro to SM100 builds v2 ([#28938](https://github.com/vllm-project/vllm/pull/28938)) by @johnnynunez
* [Bugfix] Fix FusedMoEModularKernel for triton backend ([#28913](https://github.com/vllm-project/vllm/pull/28913)) by @xyang16
* [CI/Build] Fix test_prefix_prefill for AMD ([#28905](https://github.com/vllm-project/vllm/pull/28905)) by @rjrock-amd
* [NIXL] fix cpu PD after physical <> logical block_size PR ([#28904](https://github.com/vllm-project/vllm/pull/28904)) by @xuechendi
* Eagle: MM Cuda Graphs with MRope ([#28896](https://github.com/vllm-project/vllm/pull/28896)) by @IzzyPutterman
* refactor(cpu_types_scalar.hpp): Unify scalar loop implementations using unroll_loop ([#28847](https://github.com/vllm-project/vllm/pull/28847)) by @ihb2032
* [XPU] work around for sp, avoid custom op import error ([#28822](https://github.com/vllm-project/vllm/pull/28822)) by @jikunshang
* [Doc]: fix typos in various files ([#28811](https://github.com/vllm-project/vllm/pull/28811)) by @didier-durand
* [Build] Add OpenAI triton_kernels ([#28788](https://github.com/vllm-project/vllm/pull/28788)) by @varun-sundar-rabindranath
* [NIXL][XPU] update install script of NIXL ([#28778](https://github.com/vllm-project/vllm/pull/28778)) by @zhenwei-intel
* [Redo] #26368 ([#28771](https://github.com/vllm-project/vllm/pull/28771)) by @DarkLight1337
* [Attention] FA2&FA3 support more head sizes, ViT support, make default backend ([#28763](https://github.com/vllm-project/vllm/pull/28763)) by @MatthewBonanni
* [Bugfix][cache_kernels]: Fix OOB in cache_kernels.cu ([#28760](https://github.com/vllm-project/vllm/pull/28760)) by @Flink-ddd
* [TPU] Fix import error in tpu launch ([#28758](https://github.com/vllm-project/vllm/pull/28758)) by @QiliangCui
* [ROCm][CI/Build] Upgrade to ROCm 7.1 and AITER main ([#28753](https://github.com/vllm-project/vllm/pull/28753)) by @gshtras
* [ROCm][CI/Build] Change install location of uv ([#28741](https://github.com/vllm-project/vllm/pull/28741)) by @gshtras
* Upstreaming aiter triton attention backend as a new backend ([#28701](https://github.com/vllm-project/vllm/pull/28701)) by @maleksan85
* [XPU][CI]disable lm cache uts ([#28696](https://github.com/vllm-project/vllm/pull/28696)) by @jikunshang
* [CI][CPU] Smoke test for Apple Silicon using GHA MacOS runner ([#28688](https://github.com/vllm-project/vllm/pull/28688)) by @mgoin
* [ROCm][Bugfix] Fix compilation errors with fused_qknorm_rope_kernel.cu ([#28682](https://github.com/vllm-project/vllm/pull/28682)) by @SageMoore
* [ROCm] Bump up the version of amd-smi to 6.4.3 ([#28680](https://github.com/vllm-project/vllm/pull/28680)) by @SageMoore
* [Misc] Update xformers to 0.33.0.post1 ([#28678](https://github.com/vllm-project/vllm/pull/28678)) by @ywang96
* [Bugfix][Nixl] Fix kernel physical<>logical block_size issue  ([#28677](https://github.com/vllm-project/vllm/pull/28677)) by @NickLucche
* [Config] Clean up SchedulerConfig initialization ([#28665](https://github.com/vllm-project/vllm/pull/28665)) by @DarkLight1337
* [cpu][ci] Add initial set of tests for Arm CPUs ([#28657](https://github.com/vllm-project/vllm/pull/28657)) by @fadara01
* [ROCm][BugFix] Fix shared expert loading error when disable `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` ([#28633](https://github.com/vllm-project/vllm/pull/28633)) by @ganyi1996ppo
* [CI] Reorganize compile tests so new tests are automatically included in CI ([#28625](https://github.com/vllm-project/vllm/pull/28625)) by @gmagogsfm
* Mirrored test group definitions for AMD (2025-11-11) ([#28573](https://github.com/vllm-project/vllm/pull/28573)) by @Alexei-V-Ivanov-AMD
* [Misc] don't cache `CUTLASS_REVISION` var in CMakeLists.txt ([#28518](https://github.com/vllm-project/vllm/pull/28518)) by @jinzhen-lin
* [Hybrid] [Kernel] Fix chunk scan kernel when BLOCK_SIZE_DSTATE > 128 ([#28295](https://github.com/vllm-project/vllm/pull/28295)) by @tdoublep
* [Misc] add ignore mapper for quark quantization ([#28275](https://github.com/vllm-project/vllm/pull/28275)) by @haoyangli-amd
* [BugFix] [FEAT] Enable fastsafetensors for ROCm platform ([#28225](https://github.com/vllm-project/vllm/pull/28225)) by @tjtanaa
* [Docs] Update oneshot imports ([#28188](https://github.com/vllm-project/vllm/pull/28188)) by @UranusSeven
* [KVConnector][Core] Support cross-layer KV blocks ([#27743](https://github.com/vllm-project/vllm/pull/27743)) by @orozery
* [AMD] Use Decoupled Kernel Block Size to Support AITER MLA block_size=1 ([#27715](https://github.com/vllm-project/vllm/pull/27715)) by @zq1997
* [Test] Batch Invariant: Rename and organize tests ([#27421](https://github.com/vllm-project/vllm/pull/27421)) by @yewentao256
* [Kernels] Enable FlashInfer FP8 Blockscale on SM90 (for TEP DSR1) ([#27134](https://github.com/vllm-project/vllm/pull/27134)) by @djmmoss
* [FEAT] [AITER] [ROCm] integrate aiter sampling ops ([#26084](https://github.com/vllm-project/vllm/pull/26084)) by @vllmellm
* [MoE] Nvfp4 Masked Gemm: Add flashinfer grouped_gemm_nt_masked ([#25990](https://github.com/vllm-project/vllm/pull/25990)) by @wenscarl
* [DCP] Support Decode Context Parallel (DCP) for GQA with Flashinfer ([#25438](https://github.com/vllm-project/vllm/pull/25438)) by @gjc0824
* Avoid bytecode hook and simplify TorchCompileWrapperWithCustomDipatch ([#25110](https://github.com/vllm-project/vllm/pull/25110)) by @laithsakka

## Refactoring & Core

* cleanup at::Tag::needs_fixed_stride_order ([#28974](https://github.com/vllm-project/vllm/pull/28974)) by @BoyuanFeng
* [CI Sprint] Quantization CI Cleanup ([#24130](https://github.com/vllm-project/vllm/pull/24130)) by @killershrimp

## Build, CI & Testing

* [CI/Build] Make test_attention_selector.py run tests on correct platform ([#29064](https://github.com/vllm-project/vllm/pull/29064)) by @rasmith
* [CI] Fix precommit `rope_theta` issue ([#29040](https://github.com/vllm-project/vllm/pull/29040)) by @yewentao256
* [CI/Build] Fix broken build on Apple M1 ([#28999](https://github.com/vllm-project/vllm/pull/28999)) by @j20120307
* [Bugfix] Fix precision loss in LoRA-wrapped RowParallelLinear by fusing bias into GEMM ([#28972](https://github.com/vllm-project/vllm/pull/28972)) by @prashanth058
* [CI][NIXL] Change default `block_size` for tests ([#28927](https://github.com/vllm-project/vllm/pull/28927)) by @NickLucche
* [CI/Build] Replace wikipedia url with local server ones ([#28908](https://github.com/vllm-project/vllm/pull/28908)) by @Isotr0py
* [CI] Fix async scheduling + spec decoding test flake ([#28902](https://github.com/vllm-project/vllm/pull/28902)) by @njhill
* [CI] Fix broken pipeline ([#28781](https://github.com/vllm-project/vllm/pull/28781)) by @njhill
* [Doc] Fix failing doc build ([#28772](https://github.com/vllm-project/vllm/pull/28772)) by @DarkLight1337
* Run macos smoke test workflow on main commit ([#28752](https://github.com/vllm-project/vllm/pull/28752)) by @mgoin
* [Test] Rework e2e async scheduling tests ([#28744](https://github.com/vllm-project/vllm/pull/28744)) by @njhill
* [CI] Fix macos smoke test uv cache issue ([#28736](https://github.com/vllm-project/vllm/pull/28736)) by @mgoin
* [Fix] improve aspect ratio in dummy image generation and add common  VLM tests for PaddleOCR-VL ([#28711](https://github.com/vllm-project/vllm/pull/28711)) by @dongbo910220
* [Misc] fix comment in test_envs ([#28529](https://github.com/vllm-project/vllm/pull/28529)) by @xingliu14
* [Bugfix][CI/Test][Spec Decode] Fix illegal memory access in offline_inference/spec_decode.py (Issue  27619) ([#28432](https://github.com/vllm-project/vllm/pull/28432)) by @rasmith
* [Misc] Update embedding/cross encoder tests to use `mteb` v2 ([#27329](https://github.com/vllm-project/vllm/pull/27329)) by @Samoed

## Documentation

* [Core] Add audio_embeds support to chat completions ([#29059](https://github.com/vllm-project/vllm/pull/29059)) by @jeremyteboul
* [Doc]: fix typos in various files ([#29010](https://github.com/vllm-project/vllm/pull/29010)) by @didier-durand
* [Docs] Take env var definition out of folded admonition ([#29005](https://github.com/vllm-project/vllm/pull/29005)) by @hmellor
* [Doc]: fix typos in various files ([#28945](https://github.com/vllm-project/vllm/pull/28945)) by @didier-durand
* [chore] Move the rest of wikimedia url to S3 ([#28921](https://github.com/vllm-project/vllm/pull/28921)) by @khluu
* [Doc]: fix typos in various files ([#28863](https://github.com/vllm-project/vllm/pull/28863)) by @didier-durand
* [Docs] Enable some more markdown lint rules for the docs ([#28731](https://github.com/vllm-project/vllm/pull/28731)) by @hmellor
* [Doc]: fix typos in various files ([#28567](https://github.com/vllm-project/vllm/pull/28567)) by @didier-durand
* Allow Gemma3 to take image embeddings ([#28483](https://github.com/vllm-project/vllm/pull/28483)) by @tingtingtangmeta
* docs(lora_resolvers): clarify multi-resolver order and storage path requirement ([#28153](https://github.com/vllm-project/vllm/pull/28153)) by @wangchen615
* [Doc] Fix macOS installation dependency resolution issue ([#26721](https://github.com/vllm-project/vllm/pull/26721)) by @shahfasal
* [DisaggEverything] Tokens in<>out `/generate` endpoint ([#24261](https://github.com/vllm-project/vllm/pull/24261)) by @NickLucche

## Miscellaneous

* [Bugfix] - Add Trace Headers to Beam Search Path ([#29100](https://github.com/vllm-project/vllm/pull/29100)) by @dsuhinin
* [BugFix] Fix flash_attn import in `siglip2navit.py` ([#29082](https://github.com/vllm-project/vllm/pull/29082)) by @faaany
* [Bug] Fix torch dynamo warning Dynamo detected a call to a `functools.lru_cache` ([#29038](https://github.com/vllm-project/vllm/pull/29038)) by @yewentao256
* [BugFix] Fix false assertion with spec-decode=[2,4,..] and TP>2 ([#29036](https://github.com/vllm-project/vllm/pull/29036)) by @LucasWilkinson
* [GC Debugger] Simply and improve GC Debugger Utils ([#29029](https://github.com/vllm-project/vllm/pull/29029)) by @Jialin
* [Misc] Colorize logs ([#29017](https://github.com/vllm-project/vllm/pull/29017)) by @njhill
* [Bugfix] Handle broken frames in video loading ([#29001](https://github.com/vllm-project/vllm/pull/29001)) by @gcanlin
* [Bugfix] Revert custom attention mask for gemma3-mm ([#28995](https://github.com/vllm-project/vllm/pull/28995)) by @Isotr0py
* [BugFix] Fix async-scheduling + FlashAttn MLA ([#28990](https://github.com/vllm-project/vllm/pull/28990)) by @LucasWilkinson
* [Bugfix][NIXL] Fix `block_size_ratio` when logical !=physical blocks   ([#28925](https://github.com/vllm-project/vllm/pull/28925)) by @NickLucche
* [Core] Reuse created spec tokens lists to mitigate GC cost ([#28917](https://github.com/vllm-project/vllm/pull/28917)) by @Jialin
* [Core] Switch Flat logprob control from environment variable to SamplingParams ([#28914](https://github.com/vllm-project/vllm/pull/28914)) by @Jialin
* [Minor] Rename `ec_producer` field to `is_ec_producer` ([#28884](https://github.com/vllm-project/vllm/pull/28884)) by @njhill
* [Misc] Fix wrong comment in scheduler ([#28880](https://github.com/vllm-project/vllm/pull/28880)) by @zhuohan123
* [BugFix] Ray with multiple nodes ([#28873](https://github.com/vllm-project/vllm/pull/28873)) by @juliendenize
* [Bugfix] Safeguard against missing backend in AttentionBackendEnum ([#28846](https://github.com/vllm-project/vllm/pull/28846)) by @jesse996
* [Bugfix] Fix Kimi-K2 tool parser concatenated tool calls parsing ([#28831](https://github.com/vllm-project/vllm/pull/28831)) by @bbartels
* [Frontend] Allow parsed tool arguments ([#28820](https://github.com/vllm-project/vllm/pull/28820)) by @qgallouedec
* [Bugfix] Fix spec decode memory regression after #28549 ([#28819](https://github.com/vllm-project/vllm/pull/28819)) by @zhewenl
* Cast return value to int64_t for cache size ([#28814](https://github.com/vllm-project/vllm/pull/28814)) by @tiehexue
* [BugFix] Corner case that could cause out-of-sync with external launcher mode and dp >1 ([#28774](https://github.com/vllm-project/vllm/pull/28774)) by @bangshengtang
* [Feature] Prefill Context Parallel (PCP) basic support ([#28718](https://github.com/vllm-project/vllm/pull/28718)) by @pisceskkk
* [Bugfix] fix dots.ocr pp support ([#28705](https://github.com/vllm-project/vllm/pull/28705)) by @ZJY0516
* use default CCL_ZE_IPC_EXCHANGE ([#28700](https://github.com/vllm-project/vllm/pull/28700)) by @yma11
* [Minor] avoid register new custom and just import silly_attn ([#28578](https://github.com/vllm-project/vllm/pull/28578)) by @BoyuanFeng
* [feat]: log number of preempted requests ([#28522](https://github.com/vllm-project/vllm/pull/28522)) by @610lyn
* [quantization][config] enable override existing quant_config ([#28510](https://github.com/vllm-project/vllm/pull/28510)) by @ILikeIneine
* [BugFix] Temporary fix for IMA with MTP = 2 and full-cg ([#28315](https://github.com/vllm-project/vllm/pull/28315)) by @LucasWilkinson
* [Chore] Update `xgrammar` version from 0.1.25 to 0.1.27 ([#28221](https://github.com/vllm-project/vllm/pull/28221)) by @cjackal
* [torchao] fix safetensors for sharding ([#28169](https://github.com/vllm-project/vllm/pull/28169)) by @liangel-02
*  [Frontend] Added chat-style multimodal support to /classify. ([#27516](https://github.com/vllm-project/vllm/pull/27516)) by @WorldExplored
* [compile] Enable sequence parallelism matching w/o custom ops enabled  ([#27126](https://github.com/vllm-project/vllm/pull/27126)) by @angelayi
* [NIXL] heterogeneous block_size support ([#26759](https://github.com/vllm-project/vllm/pull/26759)) by @xuechendi
* [Core] Async Scheduling X Spec Decoding Compatibility ([#24799](https://github.com/vllm-project/vllm/pull/24799)) by @Ronald1995
* [V1] Support MP Executor for multi node distributed inference ([#23691](https://github.com/vllm-project/vllm/pull/23691)) by @luccafong

## Breaking Changes

* [ROCm] Fix for import when building with upstream triton for gfx1100 for gpt-oss serving ([#29127](https://github.com/vllm-project/vllm/pull/29127)) by @hongxiayang
* [Bug] Fix torch warning of tf32 usage ([#29112](https://github.com/vllm-project/vllm/pull/29112)) by @yewentao256
* [Bugfix] Fix Plamo3 rope handling ([#29092](https://github.com/vllm-project/vllm/pull/29092)) by @DarkLight1337
* [chore] Update annotate release scripts ([#29077](https://github.com/vllm-project/vllm/pull/29077)) by @khluu
* [Doc] cleanup TPU documentation and remove outdated examples ([#29048](https://github.com/vllm-project/vllm/pull/29048)) by @RobMulla
* [CI] Fix mypy for `vllm/v1/worker` ([#29037](https://github.com/vllm-project/vllm/pull/29037)) by @yewentao256
* [CI/Build] Remove skip global cleanup in test_struct_output_generate.py ([#29022](https://github.com/vllm-project/vllm/pull/29022)) by @rasmith
* [V0 Deprecation] Remove `num_lookahead_slots` ([#29000](https://github.com/vllm-project/vllm/pull/29000)) by @DarkLight1337
* [Bug] Fix Batch Invariant MLA test ([#28967](https://github.com/vllm-project/vllm/pull/28967)) by @yewentao256
* Re-enable FlashInfer for Llama4 on Blackwell in e2e fusion tests ([#28966](https://github.com/vllm-project/vllm/pull/28966)) by @copilot-swe-agent
* [MISC] Remove format.sh ([#28906](https://github.com/vllm-project/vllm/pull/28906)) by @KuntaiDu
* [Misc] Remove unnecessary parentheses from log statements ([#28897](https://github.com/vllm-project/vllm/pull/28897)) by @andyxning
* [Refactor] Remove Unused Func in Batch Invariant ([#28881](https://github.com/vllm-project/vllm/pull/28881)) by @yewentao256
* [Bugfix] Fix wrong CLI defaults for dynamic `SchedulerConfig` fields ([#28872](https://github.com/vllm-project/vllm/pull/28872)) by @DarkLight1337
* [MODEL] Implement plamo3 ([#28834](https://github.com/vllm-project/vllm/pull/28834)) by @Alnusjaponica
* [CPU] Refactor CPU WNA16  ([#28826](https://github.com/vllm-project/vllm/pull/28826)) by @bigPYJ1151
* [CPU][Bugfix] Fix _to_list in CPU model runner ([#28824](https://github.com/vllm-project/vllm/pull/28824)) by @bigPYJ1151
* [Metrics] Fix KV cache usage percent metric multiproc ([#28792](https://github.com/vllm-project/vllm/pull/28792)) by @jaywonchung
* [RL] [V1] Remove unused device argument from reset_kv_cache ([#28766](https://github.com/vllm-project/vllm/pull/28766)) by @zhuohan123
* Use narrow over indexing in `hadacore_transform` to prep for ABI stable ([#28756](https://github.com/vllm-project/vllm/pull/28756)) by @janeyx99
* [PERF] Remove TRTLLM Gen attn kernel limitation `max_seq_len <=131072` ([#28755](https://github.com/vllm-project/vllm/pull/28755)) by @vadiklyutiy
* [BugFix] Fix `AssertionError: DCP not support reorder_batch_threshold > 1 now.`  ([#28751](https://github.com/vllm-project/vllm/pull/28751)) by @LucasWilkinson
* [ci][amd] fix EPLB execution test ([#28742](https://github.com/vllm-project/vllm/pull/28742)) by @bradleyhd
* [Bugfix] Fix ChunkedLocalAttention CUDA Graph setting ([#28739](https://github.com/vllm-project/vllm/pull/28739)) by @benchislett
* [Chore] Rename `SchedulerConfig.chunked_prefill_enabled` ([#28735](https://github.com/vllm-project/vllm/pull/28735)) by @DarkLight1337
* [Misc] Make `SchedulerConfig.max_model_len` init-only ([#28733](https://github.com/vllm-project/vllm/pull/28733)) by @DarkLight1337
* Remove audio optional dependency for mistral-common ([#28722](https://github.com/vllm-project/vllm/pull/28722)) by @juliendenize
* [Bugfix] [ROCm] [AITER]: Fix aiter block quant not compatible with torch compile dynamo ([#28716](https://github.com/vllm-project/vllm/pull/28716)) by @tjtanaa
* [kernel] Improve FP8 PTPC on Hopper for larger shapes ([#28692](https://github.com/vllm-project/vllm/pull/28692)) by @czhu-cohere
* [ci][amd] fix basic models extra init test ([#28676](https://github.com/vllm-project/vllm/pull/28676)) by @bradleyhd
* [Misc] Remove `warn_for_unimplemented_methods` ([#28613](https://github.com/vllm-project/vllm/pull/28613)) by @DarkLight1337
* [LoRA][2/2]Remove LoRA extra vocab  ([#28545](https://github.com/vllm-project/vllm/pull/28545)) by @jeejeelee
* [Bugfix][Model] Prevent special token leakage in KimiK2ToolParser streaming mode ([#28543](https://github.com/vllm-project/vllm/pull/28543)) by @jscaldwell55
* Update `rope_scaling` to `rope_parameters` in preparation for Transformers v5 ([#28542](https://github.com/vllm-project/vllm/pull/28542)) by @hmellor
* [Docs] Clean up moe_kernel_features.md ([#28530](https://github.com/vllm-project/vllm/pull/28530)) by @windsonsea
* [Attention] Bump FA for removed method ([#28429](https://github.com/vllm-project/vllm/pull/28429)) by @MatthewBonanni
* [BugFix][CI/Build][ROCM] Fix import error and apply assert in appropriate case in test_struct_output_generate ([#28311](https://github.com/vllm-project/vllm/pull/28311)) by @rasmith
* Adding a benchmark for batch invariance ([#28161](https://github.com/vllm-project/vllm/pull/28161)) by @bwasti
* [Log] Save profiler results to file instead of stdout ([#28144](https://github.com/vllm-project/vllm/pull/28144)) by @rasmith
* [RL] Add Pause and Resume Generation for Asynchronous RL Training ([#28037](https://github.com/vllm-project/vllm/pull/28037)) by @SamitHuang
* [Model] Add Gemma3 GGUF multimodal support ([#27772](https://github.com/vllm-project/vllm/pull/27772)) by @lucianommartins
* [Bugfix] TypeError: 'NoneType' object is not callable ([#27410](https://github.com/vllm-project/vllm/pull/27410)) by @mostrowskix
* Enable bitsandbytes quantization on AMD GPUs that use warp size 32 ([#27307](https://github.com/vllm-project/vllm/pull/27307)) by @sstamenk
* [BugFix][PD]: make example proxy usable with P2pNcclConnector ([#26628](https://github.com/vllm-project/vllm/pull/26628)) by @pandalee99
* [torch.compile] caching of config fields should be opt-out by default ([#26468](https://github.com/vllm-project/vllm/pull/26468)) by @vnadathur

## Upgrade Notes

- Cleans up vLLM documentation to reflect the migration of TPU support to the `tpu-inference` backend.
- vllm/v1/worker/tpu_model_runner.py:731: note: Right operand is of type "int | None"
- vllm/v1/worker/tpu_model_runner.py:743: note: Right operand is of type "int | None"
- vllm/v1/worker/tpu_model_runner.py:747: note: Right operand is of type "int | None"
- Per @simon-mo's suggestion to move the control knob from environment variable to SamplingParams, which would make the migration easier.
- Fix an issue caused by https://github.com/vllm-project/vllm/pull/28665 because the CLI still uses the defaults from `SchedulerConfig`. Sorry for breaking this!
- Breaking command:
- NOTE: for these unit tests we use the added config as a default config (not gated by the condition on `M` and `K`) to test the correctness, since the test case shapes are not large.

## Contributors

@610lyn, @Alexei-V-Ivanov-AMD, @Alnusjaponica, @BoyuanFeng, @DarkLight1337, @Flink-ddd, @GuanH, @ILikeIneine, @Isotr0py, @IzzyPutterman, @JartX, @Jialin, @KuntaiDu, @LucasWilkinson, @MatthewBonanni, @Nepherpitou, @NickLucche, @OthmanMohammad, @QiliangCui, @River12, @RobMulla, @Ronald1995, @SageMoore, @SamitHuang, @Samoed, @UranusSeven, @WorldExplored, @ZJY0516, @ai-jz, @alexm-redhat, @amirkl94, @andyxning, @angelayi, @ashors1, @bangshengtang, @bbartels, @benchislett, @bigPYJ1151, @bradleyhd, @bwasti, @cboss6, @cjackal, @copilot-swe-agent, @czhu-cohere, @didier-durand, @djmmoss, @dongbo910220, @drisspg, @dsuhinin, @eldarkurtic, @faaany, @fadara01, @ganyi1996ppo, @gcanlin, @gjc0824, @gmagogsfm, @gnovack, @gshtras, @halyavin, @haoyangli-amd, @hmellor, @hongxiayang, @hwhaokun, @ihb2032, @izhuhaoran, @j20120307, @janeyx99, @jaywonchung, @jeejeelee, @jeremyteboul, @jerryzh168, @jesse996, @jiahanc, @jikunshang, @jinzhen-lin, @johnnynunez, @jscaldwell55, @juliendenize, @kfhfar, @khluu, @killershrimp, @laithsakka, @lgeiger, @liangel-02, @louie-tsai, @luccafong, @lucianommartins, @maleksan85, @maxyanghu, @mgoin, @micah-wil, @mostrowskix, @njhill, @noooop, @orozery, @pandalee99, @pisceskkk, @pranav4501, @prashanth058, @ptovam, @qgallouedec, @rasmith, @rjrock-amd, @robertgshaw2-redhat, @sammysun0711, @sarckk, @scottzh8, @segevido, @shahfasal, @shen-shanshan, @shengliangxu, @shreyas269, @sstamenk, @tdoublep, @tiehexue, @tingtingtangmeta, @tjtanaa, @tomeras91, @vadiklyutiy, @varun-sundar-rabindranath, @vllmellm, @vnadathur, @wangchen615, @wenscarl, @windsonsea, @wuyaoxuehun, @xingliu14, @xli, @xuebwang-amd, @xuechendi, @xyang16, @yewentao256, @yma11, @ywang96, @zRzRzRzRzRzRzR, @zhanggzh, @zhaozx-cn, @zhenwei-intel, @zhewenl, @zhuohan123, @zhyajie, @zq1997, @zyongye