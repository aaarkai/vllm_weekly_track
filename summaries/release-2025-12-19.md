# Weekly Release Report for vllm-project/vllm (2025-12-19)

This week merged 229 PRs from 127 contributors. Key areas: features 2, fixes 15, performance 17.

## Executive Summary

本周发布在核心功能与模型支持方面取得多项进展。重点包括将 MMEncoderAttention 提取为 CustomOp 并用于优化 QwenVisionAttention 后端，新增对 AudioFlamingo3 和 BAGEL（仅 AR）模型的支持。性能优化工作主要涉及修复 DeepEPHighThroughput 与 DeepGEMM 在工作空间分配中的问题，为 DeepSeek 旋转编码启用 flashinfer 自定义算子以提升效率，并对多模态编码器缓存管理等进行了改进。

错误修复覆盖了多个关键领域，包括修正 Speculative Decoding 与结构化输出在特定边缘情况下的问题、解决 FP8 在线量化在张量并行度大于 1 时的流式处理错误、修复 GLM-4 工具调用中的 Unicode 问题以及多个模型（如 Nemotron-NAS, Qwen3-VL MOE）的配置计算错误。此次更新包含多项破坏性变更，例如移除了 mm_prefix triton attention 的 `tile_size=64` 参数、废弃了 `VLLM_ATTENTION_BACKEND` 环境变量引用、更新了 `enable_fusion` 等配置项，并清理了部分遗留代码。建议用户在升级前审阅相关变更。

## Highlights

* [CustomOp][MM] Extract MMEncoderAttention as CustomOp and replace the backend of QwenVisionAttention with it. ([#30125](https://github.com/vllm-project/vllm/pull/30125)) by @shen-shanshan
* Add AudioFlamingo3 model support ([#30539](https://github.com/vllm-project/vllm/pull/30539)) by @lashahub
* [New Model] BAGEL support (AR only) ([#28439](https://github.com/vllm-project/vllm/pull/28439)) by @princepride
* [Attention] Update tests to remove deprecated env vars ([#30563](https://github.com/vllm-project/vllm/pull/30563)) by @MatthewBonanni
* [Attention] Use sparse prefill kernel for fp8 kv-cache in DeepSeek-v3.2 ([#27532](https://github.com/vllm-project/vllm/pull/27532)) by @LucasWilkinson

## Features & Enhancements

* Add IBM and Red Hat to compute resources sponsors ([#30581](https://github.com/vllm-project/vllm/pull/30581)) by @mgoin
* Add AudioFlamingo3 model support ([#30539](https://github.com/vllm-project/vllm/pull/30539)) by @lashahub

## Bug Fixes

* Check for truthy `rope_parameters` not the existence of it ([#30983](https://github.com/vllm-project/vllm/pull/30983)) by @hmellor
* [BugFix] Fix spec decode + structured outputs + preemption edge case ([#30916](https://github.com/vllm-project/vllm/pull/30916)) by @njhill
* fix fp8 online quantization streaming with tp > 1 ([#30900](https://github.com/vllm-project/vllm/pull/30900)) by @vkuzo
* [BugFix] Handle errors when preprocessing added requests ([#30895](https://github.com/vllm-project/vllm/pull/30895)) by @njhill
* Fix lazy import ([#30858](https://github.com/vllm-project/vllm/pull/30858)) by @hmellor
* Fix nemotron_nas intermediate_size computation ([#30795](https://github.com/vllm-project/vllm/pull/30795)) by @grzegorz-k-karch
* Fix instantiation of `HfHubHTTPError` in LoRA test ([#30768](https://github.com/vllm-project/vllm/pull/30768)) by @hmellor
* [Bugfix][DSV32] Fix overflow in topk. ([#30754](https://github.com/vllm-project/vllm/pull/30754)) by @dcampora
* [Fix] uniform decode batch check ([#30747](https://github.com/vllm-project/vllm/pull/30747)) by @Jialin
* [Bugfix] Fail instead of ignoring when CompilationConfig gets invalid args ([#30708](https://github.com/vllm-project/vllm/pull/30708)) by @mgoin
* fix: add warmup for audio preprocessing ([#30706](https://github.com/vllm-project/vllm/pull/30706)) by @TheCodeWrangler
* [Bug] Fix attention_backend arg string parsing ([#30534](https://github.com/vllm-project/vllm/pull/30534)) by @mgoin
* [Bugfix][Model] Fix Afmoe rope_parameters issue ([#30505](https://github.com/vllm-project/vllm/pull/30505)) by @mgoin
* fix(gguf): Disable bfloat16 for GGUF on blackwell device ([#30408](https://github.com/vllm-project/vllm/pull/30408)) by @kitaekatt
* fix: Update json features supported by xGrammar ([#30390](https://github.com/vllm-project/vllm/pull/30390)) by @johannesflommersfeld

## Performance

* [BugFix] Workspace allocation during profile run : DeepEPHighThroughput + DeepGEMM  ([#30899](https://github.com/vllm-project/vllm/pull/30899)) by @varun-sundar-rabindranath
* [UX] Make `vllm bench serve` discover model by default and use --input-len ([#30816](https://github.com/vllm-project/vllm/pull/30816)) by @mgoin
* [Perf] enable flashinfer rotary_embedding custom ops in DeepSeek rotary ([#30729](https://github.com/vllm-project/vllm/pull/30729)) by @jiahanc
* feat(api): Eager chat template warmup to eliminate first-request latency ([#30700](https://github.com/vllm-project/vllm/pull/30700)) by @TheCodeWrangler
* [Chore] Adjust tokenizer import to avoid circular imports ([#30601](https://github.com/vllm-project/vllm/pull/30601)) by @DarkLight1337
* [Bugfix][benchmarks] Fix input token calculation for rerank benchmark metrics ([#30596](https://github.com/vllm-project/vllm/pull/30596)) by @Flink-ddd
* [Feat] Enable eplb with default all2all backend ([#30559](https://github.com/vllm-project/vllm/pull/30559)) by @yewentao256
* [Benchmarks] `auto_tune.sh`: Use hostname variable for server requests ([#30529](https://github.com/vllm-project/vllm/pull/30529)) by @KevinMusgrave
* [Perf] Set split_k to 1 for triton_kernels ([#30528](https://github.com/vllm-project/vllm/pull/30528)) by @xyang16
* [Core][MM] Optimize encoder cache manager by operating with embeddings only ([#30475](https://github.com/vllm-project/vllm/pull/30475)) by @ywang96
* [Perf] Do FP4 quant before All gather on flashinfer trtllmgen MOE  ([#30014](https://github.com/vllm-project/vllm/pull/30014)) by @jiahanc
* [Misc] Add a script to benchmark compilation time ([#29919](https://github.com/vllm-project/vllm/pull/29919)) by @desertfire
* [Logs] Optimize startup logs 4 ([#29903](https://github.com/vllm-project/vllm/pull/29903)) by @yewentao256
* [Misc] support nsys profile for bench latency ([#29776](https://github.com/vllm-project/vllm/pull/29776)) by @izhuhaoran
* [Bugfix] Fix prefix_repetition routing in bench throughput ([#29663](https://github.com/vllm-project/vllm/pull/29663)) by @jr-shen
* [NIXL][Bugfix] Fix NIXL/RDMA registration failure over CuMemAllocator ([#29569](https://github.com/vllm-project/vllm/pull/29569)) by @Somoku
* [Perf][Kernels] Vectorize `csrc/activations_kernels.cu` ([#29512](https://github.com/vllm-project/vllm/pull/29512)) by @mgoin

## Model Support

* [Bugfix] Fix Unicode issues in GLM-4 tool calling ([#30920](https://github.com/vllm-project/vllm/pull/30920)) by @chaunceyjiang
* [compile] Fix CI for test_gpt2_cache_hit ([#30902](https://github.com/vllm-project/vllm/pull/30902)) by @zhxchen17
* [Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony models ([#30867](https://github.com/vllm-project/vllm/pull/30867)) by @HaloWorld
* [Bugfix][CPU] Fix CPU backend ROPE dispatch for VL models ([#30829](https://github.com/vllm-project/vllm/pull/30829)) by @bigPYJ1151
* [Bugfix][torch2.10] Fix test_qwen2_5_vl_compilation with 2.10 RC ([#30822](https://github.com/vllm-project/vllm/pull/30822)) by @Lucaskabela
* Update model-hosting-container-standards to 0.1.10 ([#30815](https://github.com/vllm-project/vllm/pull/30815)) by @mgoin
* [Metrics] Model FLOPs Utilization estimation ([#30738](https://github.com/vllm-project/vllm/pull/30738)) by @SungMinCho
* [CI] Generalize gsm8k test args and add Qwen3-Next MTP B200 test ([#30723](https://github.com/vllm-project/vllm/pull/30723)) by @mgoin
* [BugFix] Add embed_input_ids method to make QWenLMHeadModel a vllm model ([#30674](https://github.com/vllm-project/vllm/pull/30674)) by @iwzbi
* [Bugfix] Fix multimodal configuration for Qwen3VL MOE model ([#30670](https://github.com/vllm-project/vllm/pull/30670)) by @maxyanghu
* [Model] Automatic conversion of TokenClassification model ([#30666](https://github.com/vllm-project/vllm/pull/30666)) by @noooop
* [main][BugFix] Fixed an accuracy bug of Qwen3-next-MTP when batched inferring ([#30632](https://github.com/vllm-project/vllm/pull/30632)) by @drslark
* [Scheduer] Simplify stop checking for pooling models ([#30591](https://github.com/vllm-project/vllm/pull/30591)) by @njhill
* [Bugfix] Revert Qwen2-VL part of change in #28271 ([#30542](https://github.com/vllm-project/vllm/pull/30542)) by @zifeitong
* [CI] Update several models in registry that are available online now ([#30514](https://github.com/vllm-project/vllm/pull/30514)) by @mgoin
* [Bugfix] Qwen3-next with  --hf-overrides \{\"num_hidden_layers\":8\}  ([#30433](https://github.com/vllm-project/vllm/pull/30433)) by @heheda12345
* [ROCm][CI][Bugfix] Multi-Modal Model Support Fixes and Attention Backend Improvements ([#30270](https://github.com/vllm-project/vllm/pull/30270)) by @AndreasKaratzas
* gptq marlin quantization support for fused moe with lora ([#30254](https://github.com/vllm-project/vllm/pull/30254)) by @Bhanu068
* [Bugfix] Fix fusion for VL models ([#30244](https://github.com/vllm-project/vllm/pull/30244)) by @ElizaWszola
* [Bugfix]  Improve error messages in ModelConfig validation ([#30213](https://github.com/vllm-project/vllm/pull/30213)) by @yifant-code
* [Model] adds jais 2 support ([#30188](https://github.com/vllm-project/vllm/pull/30188)) by @sarathc-cerebras
* Nvidia ModelOpt workaround for issue 28072 ([#30164](https://github.com/vllm-project/vllm/pull/30164)) by @shengliangxu
* [Model][Quantization] Override HF defaults to GGUF ones (incl. Qwen3 MoE) ([#30118](https://github.com/vllm-project/vllm/pull/30118)) by @a4lg
* [Feature]Add EVS (Efficient Video Sampling) Support for Qwen3-VL ([#29752](https://github.com/vllm-project/vllm/pull/29752)) by @skyloevil
* [MoE-FP8-modelopt] Add FlashInfer alignment padding for intermediate dimensions ([#29748](https://github.com/vllm-project/vllm/pull/29748)) by @danielafrimi
* [Bugfix] Multiple fixes for gpt-oss Chat Completion prompting ([#28729](https://github.com/vllm-project/vllm/pull/28729)) by @bbrowning

## Hardware & Backend

* [Bugfix][CPU] Fix Mac CPU build ([#30955](https://github.com/vllm-project/vllm/pull/30955)) by @bigPYJ1151
* [ROCm] Serving Fails on Radeon Due to AITER Dtype Import  ([#30952](https://github.com/vllm-project/vllm/pull/30952)) by @vllmellm
* [Doc] Add Sophgo TPU Support ([#30949](https://github.com/vllm-project/vllm/pull/30949)) by @wzyrrr
* [XPU] allow custom workers (e.g. vllm-omni workers) to be used on XPU ([#30935](https://github.com/vllm-project/vllm/pull/30935)) by @faaany
* [AMD][CI] fix lm eval ci arg ([#30911](https://github.com/vllm-project/vllm/pull/30911)) by @divakar-amd
* [ROCm][Bugfix] Fix `fa_version` argument error in `flash_attn_maxseqlen_wrapper` for ROCm without aiter ([#30909](https://github.com/vllm-project/vllm/pull/30909)) by @AndreasKaratzas
* [UX] Reduce DeepGEMM warmup log output to single progress bar ([#30903](https://github.com/vllm-project/vllm/pull/30903)) by @MatthewBonanni
* [XPU] fix broken fp8 online quantization for XPU platform ([#30831](https://github.com/vllm-project/vllm/pull/30831)) by @yma11
* [KV connector][LMCache] Only record the cuda event when there are request to store/load ([#30814](https://github.com/vllm-project/vllm/pull/30814)) by @ApostaC
* [ROCm][CI] Reduce Flakiness For test_async_scheduling Using ROCM_ATTN With FP32 ([#30811](https://github.com/vllm-project/vllm/pull/30811)) by @micah-wil
* [Doc][CPU] Update CPU doc ([#30765](https://github.com/vllm-project/vllm/pull/30765)) by @bigPYJ1151
* [Bugfix] Fix broken ViT attention selection for Blackwell device ([#30731](https://github.com/vllm-project/vllm/pull/30731)) by @Isotr0py
* [ROCm][Bugfix] fix(structured_output): Skip guidance backend for schemas with patternProperties ([#30730](https://github.com/vllm-project/vllm/pull/30730)) by @AndreasKaratzas
* update piecewise cudagraph warning when splitting_ops=[] ([#30728](https://github.com/vllm-project/vllm/pull/30728)) by @BoyuanFeng
* [Bugfix] Fix ViT with FlashAttention on ROCm ([#30703](https://github.com/vllm-project/vllm/pull/30703)) by @MatthewBonanni
* [Refactor] [3/N] Move tool parser tests and run on CPU ([#30693](https://github.com/vllm-project/vllm/pull/30693)) by @DarkLight1337
* [MM Encoder]: Migrate legacy ViT `MultiHeadAttention` to new `MMEncoderAttention` interface ([#30684](https://github.com/vllm-project/vllm/pull/30684)) by @Isotr0py
* [CPU] Add action to automatically label CPU related PRs ([#30678](https://github.com/vllm-project/vllm/pull/30678)) by @fadara01
* [Log] Skip piecewise cudagraph warn when using full cudagraph ([#30657](https://github.com/vllm-project/vllm/pull/30657)) by @BoyuanFeng
* [Bugfix] Fix RequestOutput miss lora_request ([#30636](https://github.com/vllm-project/vllm/pull/30636)) by @jeejeelee
* [docs][fix]  Update Arm CPU vLLM wheel installation docs ([#30594](https://github.com/vllm-project/vllm/pull/30594)) by @fadara01
* [CPU] Refactor CPU fused MOE ([#30531](https://github.com/vllm-project/vllm/pull/30531)) by @bigPYJ1151
* [ROCm][CI] Skip multi-GPU speculative decoding tests when insufficient GPUs available ([#30527](https://github.com/vllm-project/vllm/pull/30527)) by @AndreasKaratzas
* [ROCm][CI] Use mi325_4 agent pool for V1 e2e tests ([#30526](https://github.com/vllm-project/vllm/pull/30526)) by @AndreasKaratzas
* [CI/Build][AMD] Skip test_cutlass_w4a8_moe tests on ROCm sine they require cutlass_pack_scale_fp8 ([#30508](https://github.com/vllm-project/vllm/pull/30508)) by @rasmith
* [Refactor] Reduce duplicate code in `per_token_group_quant` cuda kernels ([#30496](https://github.com/vllm-project/vllm/pull/30496)) by @yewentao256
* [Feature] Add SM103 (Blackwell Ultra) Support to vLLM ([#30484](https://github.com/vllm-project/vllm/pull/30484)) by @LopezCastroRoberto
* [CPU][FIX] Fix build failures on Arm CPUs with torch nightly ([#30481](https://github.com/vllm-project/vllm/pull/30481)) by @fadara01
* [NIXL][BUG FIX] Fix both failing issue and accuracy issue with nixl + host_buffer on CUDA ([#30419](https://github.com/vllm-project/vllm/pull/30419)) by @xuechendi
* [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  ([#30417](https://github.com/vllm-project/vllm/pull/30417)) by @rasmith
* [Doc] Add instructions for building docker image on GB300 with CUDA13 ([#30414](https://github.com/vllm-project/vllm/pull/30414)) by @soodoshll
* [v1] Add PrefixLM support to TritonAttention backend ([#30386](https://github.com/vllm-project/vllm/pull/30386)) by @Isotr0py
* [fix] fix SM check for Flashinfer TRTLLM MOE ([#30314](https://github.com/vllm-project/vllm/pull/30314)) by @jiahanc
* [CI/Build][Kernel][BugFix][AMD] Fix per_token_group_quant_fp8 to use correct fp8 min/max values and update atol/rtol in test_quantfp8_group_functionality  ([#30292](https://github.com/vllm-project/vllm/pull/30292)) by @rasmith
* [CI/Build][AMD] Fix ref_dynamic_per_token_quant reference implementation on ROCm. ([#30291](https://github.com/vllm-project/vllm/pull/30291)) by @rasmith
* [Feat] Refactor for `parallel_config` in `FusedMoEModularKernel` ([#30282](https://github.com/vllm-project/vllm/pull/30282)) by @yewentao256
* [CI/Build] Use spawn subprocess for ROCm ([#30272](https://github.com/vllm-project/vllm/pull/30272)) by @rjrock
* [Bugfix] fix streaming final output for non harmony ([#30237](https://github.com/vllm-project/vllm/pull/30237)) by @penfree
* [Platform] Refactor Platform attention backend selection to avoid breakpoint for OOT platform ([#30212](https://github.com/vllm-project/vllm/pull/30212)) by @Isotr0py
* [responsesAPI][8] input/output messages for ResponsesParser ([#30158](https://github.com/vllm-project/vllm/pull/30158)) by @qandrew
* [Quantization] Support Quark int4-fp8 w4a8 for MoE ([#30071](https://github.com/vllm-project/vllm/pull/30071)) by @BowenBao
* [Kernel][Quantization][MoE] add marlin kernel support for turing (sm75) ([#29901](https://github.com/vllm-project/vllm/pull/29901)) by @jinzhen-lin
* [Cleanup] Refactor FlashInferMetadataBuilder ([#29128](https://github.com/vllm-project/vllm/pull/29128)) by @benchislett
* CPU KV Offloading: Use more CUDA streams ([#29013](https://github.com/vllm-project/vllm/pull/29013)) by @orozery
* [Bugfix] fix DP-aware routing in OpenAI API requests ([#29002](https://github.com/vllm-project/vllm/pull/29002)) by @inkcherry
* [CI/Build] Add x86 CPU wheel release pipeline ([#28848](https://github.com/vllm-project/vllm/pull/28848)) by @bigPYJ1151
* [ROCm][MTP] Support MTP for AITER MLA backend ([#28624](https://github.com/vllm-project/vllm/pull/28624)) by @ganyi1996ppo
* 2.9.1 PyTorch release update ([#28495](https://github.com/vllm-project/vllm/pull/28495)) by @atalman
* [Kernel] Support CUDA Graphs in 3D Triton Attention Kernel ([#28306](https://github.com/vllm-project/vllm/pull/28306)) by @jvlunteren
* [Attention] Use sparse prefill kernel for fp8 kv-cache in DeepSeek-v3.2 ([#27532](https://github.com/vllm-project/vllm/pull/27532)) by @LucasWilkinson
* [ROCm] Enable Triton ScaledMM fallback + kernel selection fix ([#26668](https://github.com/vllm-project/vllm/pull/26668)) by @shivampr

## Refactoring & Core

* [Doc][ResponsesAPI] add documentation ([#30840](https://github.com/vllm-project/vllm/pull/30840)) by @qandrew
* [refactor] Add prefix support to embed_tokens in DeepSeek MTP ([#30788](https://github.com/vllm-project/vllm/pull/30788)) by @zzhx1
* [Refactor] [4/N] Move VLLM_SERVER_DEV endpoints into the serve directory ([#30749](https://github.com/vllm-project/vllm/pull/30749)) by @chaunceyjiang
* [MoE][Refactor 1/N] Separate Online Quantization ([#30627](https://github.com/vllm-project/vllm/pull/30627)) by @robertgshaw2-redhat
* [Refactor] `TokenizerRegistry` only uses lazy imports ([#30609](https://github.com/vllm-project/vllm/pull/30609)) by @DarkLight1337
* [Refactor] Small refactor for group topk ([#30562](https://github.com/vllm-project/vllm/pull/30562)) by @yewentao256
* [responsesAPI]add extra body parameters ([#30532](https://github.com/vllm-project/vllm/pull/30532)) by @Ri0S
* [Core] Refactor `_build_attention_metadata` ([#29628](https://github.com/vllm-project/vllm/pull/29628)) by @LucasWilkinson

## Build, CI & Testing

* [CI][Bugfix] Fix flaky `tests/entrypoints/openai/test_audio.py::test_chat_streaming_audio` ([#30878](https://github.com/vllm-project/vllm/pull/30878)) by @NickLucche
* [CI][Feature] Adds auto-rebase PR rule ([#30875](https://github.com/vllm-project/vllm/pull/30875)) by @rafvasq
* [ci] Sync test areas yaml file with test-pipeline ([#30862](https://github.com/vllm-project/vllm/pull/30862)) by @khluu
* [CI] Skip ci failure test ([#30804](https://github.com/vllm-project/vllm/pull/30804)) by @yewentao256
* [CI/Build] Fix compatibility between #30244 and #30396 ([#30787](https://github.com/vllm-project/vllm/pull/30787)) by @DarkLight1337
* [CI/Build] Skip broken ViT backend functionality test tempoarily ([#30782](https://github.com/vllm-project/vllm/pull/30782)) by @Isotr0py
* improve lazy import test ([#30733](https://github.com/vllm-project/vllm/pull/30733)) by @BoyuanFeng
* [BUILD] use sm_100f when compiling flashmla to fix support on sm103 ([#30705](https://github.com/vllm-project/vllm/pull/30705)) by @Harry-Chen
* Strengthen input validation and tests for 'parse_raw_prompts’. ([#30652](https://github.com/vllm-project/vllm/pull/30652)) by @mivehk
* [docker] Restructure Dockerfile for more efficient and cache-friendly builds ([#30626](https://github.com/vllm-project/vllm/pull/30626)) by @amrmahdi
* [CI/Build] Fix broken mm processor test Mistral-3-large ([#30597](https://github.com/vllm-project/vllm/pull/30597)) by @Isotr0py
* [ci] Mark PrimeRL integration test as soft fail ([#30578](https://github.com/vllm-project/vllm/pull/30578)) by @khluu
* [CI] Fix mypy for vllm/v1/executor ([#30517](https://github.com/vllm-project/vllm/pull/30517)) by @yewentao256
* Improve parse_raw_prompt test cases for invalid input .v2 ([#30512](https://github.com/vllm-project/vllm/pull/30512)) by @mivehk
* [CI] Whisper logprobs tests ([#30504](https://github.com/vllm-project/vllm/pull/30504)) by @NickLucche
* [Attention] Cache attention metadata builds across hybrid KV-cache groups ([#29627](https://github.com/vllm-project/vllm/pull/29627)) by @LucasWilkinson

## Documentation

* [docs]: add ecosystem projects sr in docs/governance ([#30844](https://github.com/vllm-project/vllm/pull/30844)) by @Xunzhuo
* [docker] Allow kv_connectors install to fail on arm64 ([#30806](https://github.com/vllm-project/vllm/pull/30806)) by @amrmahdi
* [Docs] fix function name ([#30748](https://github.com/vllm-project/vllm/pull/30748)) by @lengrongfu
* [Docs] Clarify Expert Parallel behavior for attention and MoE layers ([#30615](https://github.com/vllm-project/vllm/pull/30615)) by @majiayu000
* [Doc] Add documents for multi-node distributed serving with MP backend ([#30509](https://github.com/vllm-project/vllm/pull/30509)) by @Isotr0py
* [feature] extend DBO to XBO ([#30120](https://github.com/vllm-project/vllm/pull/30120)) by @jiangkuaixue123

## Miscellaneous

* [Fix][FlexAttention] return max logical block index to handle reused blocks ([#30915](https://github.com/vllm-project/vllm/pull/30915)) by @ivanium
* [BugFix] Partial revert of #29558 (DeepEP HT + PIECEWISE CG support) ([#30910](https://github.com/vllm-project/vllm/pull/30910)) by @LucasWilkinson
* [Chore] Factor out logic for requesting initial memory ([#30868](https://github.com/vllm-project/vllm/pull/30868)) by @DarkLight1337
* Adapt the old parameter enable_thinking in chat_template_kwargs ([#30852](https://github.com/vllm-project/vllm/pull/30852)) by @SongDI911
* [Bugfix] deepseek-V3.2 self.weights_proj has no bias ([#30841](https://github.com/vllm-project/vllm/pull/30841)) by @baoqian426
* [Bug] Fix AttributeError: 'ColumnParallelLinear' object has no attribute `weight_scale_inv` ([#30823](https://github.com/vllm-project/vllm/pull/30823)) by @yewentao256
* [Bug] Fix compressed tensor not using deepgemm ([#30820](https://github.com/vllm-project/vllm/pull/30820)) by @yewentao256
* [compile] Disable aot when eager backend is used. ([#30810](https://github.com/vllm-project/vllm/pull/30810)) by @zhxchen17
* [compile] Ignore VLLM_FORCE_AOT_LOAD from cache factors ([#30809](https://github.com/vllm-project/vllm/pull/30809)) by @zhxchen17
* bump up compressed tensors version to 0.13.0 ([#30799](https://github.com/vllm-project/vllm/pull/30799)) by @shanjiaz
* [Fix]Load kv-cache dtype from hf_quant_config.json automatically (fix for reverted PR) ([#30785](https://github.com/vllm-project/vllm/pull/30785)) by @danielafrimi
* [Bugfix] Whisper fix number of allocated CrossAttn blocks per-request ([#30772](https://github.com/vllm-project/vllm/pull/30772)) by @NickLucche
* [Frontend] Add `max-completion-token` option to transcription/translation endpoints ([#30769](https://github.com/vllm-project/vllm/pull/30769)) by @NickLucche
* [MM] Pass FA version in ViT Attn ([#30756](https://github.com/vllm-project/vllm/pull/30756)) by @NickLucche
* [BugFix]Reclaim resources to prevent memory leaks when use LMCacheMPConnector ([#30745](https://github.com/vllm-project/vllm/pull/30745)) by @wz1qqx
* [BugFix] Fix memory spike in workspace allocation ([#30744](https://github.com/vllm-project/vllm/pull/30744)) by @LucasWilkinson
* [compile] Recompile graph module during Dynamo cache loading. ([#30743](https://github.com/vllm-project/vllm/pull/30743)) by @zhxchen17
* fused_moe_lora PDL improvements ([#30716](https://github.com/vllm-project/vllm/pull/30716)) by @gnovack
* Update note comment for flashinfer attention warmup ([#30711](https://github.com/vllm-project/vllm/pull/30711)) by @mgoin
* [UX][Attention] Add `attention_config` argument to `LLM()` ([#30710](https://github.com/vllm-project/vllm/pull/30710)) by @MatthewBonanni
* chores: adjust the attn register param order ([#30688](https://github.com/vllm-project/vllm/pull/30688)) by @ILikeIneine
* [Bugfix] Fix missing first token in tool calls during reasoning-to-tool transition ([#30671](https://github.com/vllm-project/vllm/pull/30671)) by @mondaylord
* [Bugfix] Fix  deepseek_v32 tokenizer_mode  ([#30658](https://github.com/vllm-project/vllm/pull/30658)) by @jeejeelee
* Revert "[Fix]Load kv-cache dtype from hf_quant_config.json automatically" ([#30653](https://github.com/vllm-project/vllm/pull/30653)) by @robertgshaw2-redhat
* additional protection for CVE-2025-62164 ([#30649](https://github.com/vllm-project/vllm/pull/30649)) by @wenqiglantz
* tuned fused configs for B300 ([#30629](https://github.com/vllm-project/vllm/pull/30629)) by @navmarri14
* [LoRA] Set default MXFP4 LoRA backend to Marlin ([#30598](https://github.com/vllm-project/vllm/pull/30598)) by @xyang16
* [Bugfix] Update get_processor_data to use get_all method ([#30583](https://github.com/vllm-project/vllm/pull/30583)) by @dbotwinick
* [Bugfix] Pass FA version in `MultiHeadAttention` ([#30575](https://github.com/vllm-project/vllm/pull/30575)) by @MatthewBonanni
* [Bugfix][Frontend] Prevent IndexError in MiniMax M2 tool parser during streaming extraction ([#30555](https://github.com/vllm-project/vllm/pull/30555)) by @WangErXiao
* typing: Add type hints to TurnMetrics class in context.py ([#30552](https://github.com/vllm-project/vllm/pull/30552)) by @yurekami
* [CustomOp] Support object-level enable for CustomOp ([#30547](https://github.com/vllm-project/vllm/pull/30547)) by @shen-shanshan
* Filter safetensors files to download if .safetensors.index.json exists ([#30537](https://github.com/vllm-project/vllm/pull/30537)) by @mgoin
* [compile] Parse compile range cache keys as Range during cache loading. ([#30516](https://github.com/vllm-project/vllm/pull/30516)) by @zhxchen17
* [Bugfix] Dictionary MM embeddings for online chat ([#30507](https://github.com/vllm-project/vllm/pull/30507)) by @DarkLight1337
* [DeepSeek V3.2] Proper drop_thinking logic ([#30490](https://github.com/vllm-project/vllm/pull/30490)) by @vladnosiv
* [torch.compile] Add encoder tag for compilation ([#30489](https://github.com/vllm-project/vllm/pull/30489)) by @ilmarkov
* enable unbacked with aot_compile ([#30462](https://github.com/vllm-project/vllm/pull/30462)) by @laithsakka
* set assume_32bit_indexing and pass unbacked hints ([#30459](https://github.com/vllm-project/vllm/pull/30459)) by @laithsakka
* [LMCache] Relax lmcache version requirement ([#30425](https://github.com/vllm-project/vllm/pull/30425)) by @njhill
* [NIXL][BUG FIX] Fix a bug for PD with host_buffer after merging 29665 ([#30420](https://github.com/vllm-project/vllm/pull/30420)) by @xuechendi
* [Bugfix] awq_gemm: fix argument order swap ([#30364](https://github.com/vllm-project/vllm/pull/30364)) by @mgehre-amd
* [BugFix] Spec decode with VLLM_ENABLE_V1_MULTIPROCESSING=0 ([#30319](https://github.com/vllm-project/vllm/pull/30319)) by @heheda12345
* [Misc][Quantization] Clarify the intent of GGUF `FusedMoE` weight materialization ([#30310](https://github.com/vllm-project/vllm/pull/30310)) by @a4lg
* [Frontend] Fixes anthropic streaming message_start usage nesting ([#30266](https://github.com/vllm-project/vllm/pull/30266)) by @bbartels
* [bugfix] fix bug when top_logprobs=0 with spec decoding ([#30059](https://github.com/vllm-project/vllm/pull/30059)) by @realliujiaxu
* [Bugfix] fix _get_quant_method of FusedMoE for deepseekV3.2 on non-NV… ([#30057](https://github.com/vllm-project/vllm/pull/30057)) by @tom-zju
* [Frontend] add tools for dsv32 developer role ([#30040](https://github.com/vllm-project/vllm/pull/30040)) by @yjc9696
* [Fix]Load kv-cache dtype from hf_quant_config.json automatically ([#29980](https://github.com/vllm-project/vllm/pull/29980)) by @danielafrimi
* [moe] Use enable_chunking func (to support disabling chunking) ([#29935](https://github.com/vllm-project/vllm/pull/29935)) by @minosfuture
* [Misc][Hybrid allocator + kv connector] Optionally enable hybrid allocator + KV cache connector ([#29805](https://github.com/vllm-project/vllm/pull/29805)) by @NickLucche
* [Bugfix] Schedule failure due to wrong get_image_size_with_most_features ([#29692](https://github.com/vllm-project/vllm/pull/29692)) by @tomtomjhj
* CustomOp: grouped topk ([#29575](https://github.com/vllm-project/vllm/pull/29575)) by @xinyu-intel
* [BugFix] Add sleep to fix tight loop and release GIL ([#29476](https://github.com/vllm-project/vllm/pull/29476)) by @alec-flowers
* [NIXL] Support P tensor-parallel-size > D tensor-parallel-size ([#27274](https://github.com/vllm-project/vllm/pull/27274)) by @NickLucche
* [Bugfix] Fix CMakeLists Environment Variable ([#21804](https://github.com/vllm-project/vllm/pull/21804)) by @wu-kan

## Breaking Changes

* [Bugfix] Remove `tile_size=64` for mm_prefix triton attention ([#30973](https://github.com/vllm-project/vllm/pull/30973)) by @Isotr0py
* [Bug] Fix batch invariant in torch 2.10 ([#30907](https://github.com/vllm-project/vllm/pull/30907)) by @yewentao256
* [Chore] Remove v0 dead code for Qwen2.5-omni ([#30883](https://github.com/vllm-project/vllm/pull/30883)) by @Isotr0py
* [Kernels][FI] Skip trtllm attention when num_kv_heads=1 ([#30842](https://github.com/vllm-project/vllm/pull/30842)) by @yeqcharlotte
* [Model] Gemma3: Support untied word embeddings ([#30827](https://github.com/vllm-project/vllm/pull/30827)) by @www-spam
* Replace deprecated enable_fusion with fuse_norm_quant in test_rms_group_quant ([#30817](https://github.com/vllm-project/vllm/pull/30817)) by @mgoin
* [ROCm] [Bugfix] Fix torch sdpa hallucination ([#30789](https://github.com/vllm-project/vllm/pull/30789)) by @tjtanaa
* [Docs][API] Remove warning about LoRARequest being internal-only ([#30774](https://github.com/vllm-project/vllm/pull/30774)) by @markmc
* Update where `bytes_to_unicode` is imported from ([#30771](https://github.com/vllm-project/vllm/pull/30771)) by @hmellor
* Don't assume `position_embedding_type` will be present for BERT and RoBERTa models ([#30770](https://github.com/vllm-project/vllm/pull/30770)) by @hmellor
* Remove `head_mask` from Ultravox and Swin ([#30764](https://github.com/vllm-project/vllm/pull/30764)) by @hmellor
* [TRTLLM] Remove the MoE GEMM weight name change ([#30713](https://github.com/vllm-project/vllm/pull/30713)) by @minosfuture
* [Mamba] Removed disable cascade attn in MambaModelConfig ([#30712](https://github.com/vllm-project/vllm/pull/30712)) by @Josephasafg
* Update batch invariant to use attention config ([#30704](https://github.com/vllm-project/vllm/pull/30704)) by @MatthewBonanni
* Remove `SkipValidation` from `ModelConfig` ([#30695](https://github.com/vllm-project/vllm/pull/30695)) by @hmellor
* [Refactor] [2/N] Move tool parsers into the vLLM main directory ([#30675](https://github.com/vllm-project/vllm/pull/30675)) by @chaunceyjiang
* [XPU] fix Dockerfile.xpu, avoid wheel conflicts ([#30662](https://github.com/vllm-project/vllm/pull/30662)) by @jikunshang
* [Bugfix] Drop empty tool_calls lists to keep assistant replies in chat template ([#30648](https://github.com/vllm-project/vllm/pull/30648)) by @seokhyunan
* [Chore] Remove redundant `RequestPrompt` ([#30612](https://github.com/vllm-project/vllm/pull/30612)) by @DarkLight1337
* [ROCm][CI] Add "Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test" Back Into AMD CI ([#30590](https://github.com/vllm-project/vllm/pull/30590)) by @micah-wil
* [ROCm] [AITER] [DOC] Add usage description about check functions in `_aiter_ops` ([#30586](https://github.com/vllm-project/vllm/pull/30586)) by @tjtanaa
* [Bug][KVConnector][Metrics] Remove a vacuous assertion breaking external-launcher ([#30577](https://github.com/vllm-project/vllm/pull/30577)) by @QierLi
* [Docs] Remove references to `VLLM_ATTENTION_BACKEND` ([#30564](https://github.com/vllm-project/vllm/pull/30564)) by @MatthewBonanni
* [Attention] Update tests to remove deprecated env vars ([#30563](https://github.com/vllm-project/vllm/pull/30563)) by @MatthewBonanni
* [Doc]: fixing typos in various files ([#30540](https://github.com/vllm-project/vllm/pull/30540)) by @didier-durand
* Add removal version for all2all backend env var ([#30363](https://github.com/vllm-project/vllm/pull/30363)) by @elizabetht
* [Platform] Let EPD work with non-cuda platform ([#30225](https://github.com/vllm-project/vllm/pull/30225)) by @wangxiyuan
* [Cleanup] Remove unused ModelRunner V1 `InputBatch.num_tokens` field ([#30218](https://github.com/vllm-project/vllm/pull/30218)) by @njhill
* [CustomOp][MM] Extract MMEncoderAttention as CustomOp and replace the backend of QwenVisionAttention with it. ([#30125](https://github.com/vllm-project/vllm/pull/30125)) by @shen-shanshan
* [CustomOp] Extract ApplyRotaryEmb as CustomOp and unify the dispatch logic ([#29873](https://github.com/vllm-project/vllm/pull/29873)) by @shen-shanshan
* [PERF] Qwen3-next. Add fp8 cutlass MoE tuned configs. `chmod -x *MI308X.json` ([#29553](https://github.com/vllm-project/vllm/pull/29553)) by @vadiklyutiy
* [New Model] BAGEL support (AR only) ([#28439](https://github.com/vllm-project/vllm/pull/28439)) by @princepride

## Upgrade Notes

- In torch 2.10, we will rely more heavily on aot_precompile; part of this upgrade leads to a successful load and re-instantion of the `VLLMBackend` in caching.py; however, for multimodal encoders that previously relied on the `set_model_tag` context manager to forward `is_encoder` information to comp
- Note: This is to show that it works well under multiple-engine scenarios (i.e. DP > 1)
- *Note: Actual timings vary based on model, hardware, and configuration.*
- **Note:** The KV heads error was not tested as it requires specific model configurations that trigger that code path.
- NOTE: This PR must be merged in after #30230, this builds on top of https://github.com/vllm-project/vllm/issues/30115
- Download a quantized (note: *not* BF16) Qwen3 MoE model GGUF file.
- Side note: current design is flexible and allows for _dynamic_ discovery of remotes with different tp_sizes. However this is not a feature that is currently supported, but it helps to take into account when considering impl choices.  It's more of an optional route I'd like to keep open.
- note: This error originates from a subprocess, and is likely not a problem with pip.

## Contributors

@AndreasKaratzas, @ApostaC, @Bhanu068, @BowenBao, @BoyuanFeng, @DarkLight1337, @ElizaWszola, @Flink-ddd, @HaloWorld, @Harry-Chen, @ILikeIneine, @Isotr0py, @Jialin, @Josephasafg, @KevinMusgrave, @LopezCastroRoberto, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @NickLucche, @QierLi, @Ri0S, @Somoku, @SongDI911, @SungMinCho, @TheCodeWrangler, @WangErXiao, @Xunzhuo, @a4lg, @alec-flowers, @amrmahdi, @atalman, @baoqian426, @bbartels, @bbrowning, @benchislett, @bigPYJ1151, @chaunceyjiang, @danielafrimi, @dbotwinick, @dcampora, @desertfire, @didier-durand, @divakar-amd, @drslark, @elizabetht, @faaany, @fadara01, @ganyi1996ppo, @gnovack, @grzegorz-k-karch, @heheda12345, @hmellor, @ilmarkov, @inkcherry, @ivanium, @iwzbi, @izhuhaoran, @jeejeelee, @jiahanc, @jiangkuaixue123, @jikunshang, @jinzhen-lin, @johannesflommersfeld, @jr-shen, @jvlunteren, @khluu, @kitaekatt, @laithsakka, @lashahub, @lengrongfu, @majiayu000, @markmc, @maxyanghu, @mgehre-amd, @mgoin, @micah-wil, @minosfuture, @mivehk, @mondaylord, @navmarri14, @njhill, @noooop, @orozery, @penfree, @princepride, @qandrew, @rafvasq, @rasmith, @realliujiaxu, @rjrock, @robertgshaw2-redhat, @sarathc-cerebras, @seokhyunan, @shanjiaz, @shen-shanshan, @shengliangxu, @shivampr, @skyloevil, @soodoshll, @tjtanaa, @tom-zju, @tomtomjhj, @vadiklyutiy, @varun-sundar-rabindranath, @vkuzo, @vladnosiv, @vllmellm, @wangxiyuan, @wenqiglantz, @wu-kan, @www-spam, @wz1qqx, @wzyrrr, @xinyu-intel, @xuechendi, @xyang16, @yeqcharlotte, @yewentao256, @yifant-code, @yjc9696, @yma11, @yurekami, @ywang96, @zhxchen17, @zifeitong, @zzhx1