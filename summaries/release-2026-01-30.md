# Weekly Release Report for vllm-project/vllm (2026-01-30)

This week merged 213 PRs from 112 contributors. Key areas: features 6, fixes 43, performance 15.

## Executive Summary

本周发布聚焦于核心功能增强、广泛问题修复与性能优化。主要特性包括为v1 API新增基于会话的流式输入支持，并持续推进MoE（混合专家）组件的整合与优化。性能方面，针对FP4量化内核及MoE相关操作进行了专项优化。

本次更新包含大量问题修复，覆盖Transformers v5兼容性、XPU/CPU后端、多种内核（如GDN、FP8 MoE）以及包括DeepseekV32、Qwen3-VL在内的多个模型加载与运行问题，显著提升了系统稳定性。此外，新增了对Qwen3-ASR和Kimi-K2.5模型的支持。升级时需注意，部分修复涉及与Transformers库v5版本的兼容性调整。

## Highlights

* [Feature] add session based streaming input support to v1 ([#28973](https://github.com/vllm-project/vllm/pull/28973)) by @joshuadeng
* [MoE Refactor] Integrate Naive Prepare Finalize into MK ([#32567](https://github.com/vllm-project/vllm/pull/32567)) by @robertgshaw2-redhat
* [Perf][Kernel] Optimize FP4 quantization kernels (SM100F) ([#32520](https://github.com/vllm-project/vllm/pull/32520)) by @LopezCastroRoberto
* Add attention benchmarking tools ([#26835](https://github.com/vllm-project/vllm/pull/26835)) by @MatthewBonanni
* [5/N][Attention] Finish eliminating `vllm/attention` folder ([#32064](https://github.com/vllm-project/vllm/pull/32064)) by @MatthewBonanni

## Features & Enhancements

* support returning tokenids in responses api ([#33212](https://github.com/vllm-project/vllm/pull/33212)) by @cmunley1
* Support compress-tensors with nvfp4 or fp8 weights and modelopt with nvfp4 weights on Turing ([#33076](https://github.com/vllm-project/vllm/pull/33076)) by @ir1ka
* feature: support eagle3 for HunyuanVL & Hunyuan ([#33035](https://github.com/vllm-project/vllm/pull/33035)) by @irisliu10
* Add Triton fused MoE config for B200 (Nemotron Nano) ([#32804](https://github.com/vllm-project/vllm/pull/32804)) by @danisereb
* feat: Complete LoRA support for MiniMaxM2 Fixes #32736 ([#32763](https://github.com/vllm-project/vllm/pull/32763)) by @Chenhao-Guan
* Support heterogeneous NemotronHPuzzle model ([#32549](https://github.com/vllm-project/vllm/pull/32549)) by @danielafrimi

## Bug Fixes

* Fix `tie_word_embeddings` for multimodal models in Transformers v5 ([#33359](https://github.com/vllm-project/vllm/pull/33359)) by @hmellor
* [BUGFIX][XPU] fix memory check after XPU reuse GPU_worker  ([#33358](https://github.com/vllm-project/vllm/pull/33358)) by @xuechendi
* [Bugfix] Fix broken GLM-OCR initialization ([#33350](https://github.com/vllm-project/vllm/pull/33350)) by @Isotr0py
* [Bugfix][Kernel] Fix negative memory offset in GDN Triton kernel ([#33326](https://github.com/vllm-project/vllm/pull/33326)) by @CarstyYou
* [Bugfix][CPU] Fix thread num for shared memory communication ([#33317](https://github.com/vllm-project/vllm/pull/33317)) by @bigPYJ1151
* [Bugfix] Enable Triton MoE for FP8 per-tensor dynamic ([#33300](https://github.com/vllm-project/vllm/pull/33300)) by @mgoin
* [Bugfix] Fix Qwen3-VL-Reranker load. ([#33298](https://github.com/vllm-project/vllm/pull/33298)) by @noooop
* [Bugfix] Register fp8 cutlass_group_gemm as supported for only SM90+SM100 ([#33285](https://github.com/vllm-project/vllm/pull/33285)) by @mgoin
* [Bugfix] Add missing encoder only guard for do_kv_cache_update ([#33269](https://github.com/vllm-project/vllm/pull/33269)) by @gshtras
* [BugFix] Fix EPLB fail for MoeFP4 model with Marlin backend ([#33262](https://github.com/vllm-project/vllm/pull/33262)) by @ilmarkov
* [Bugfix] Disable CG for Whisper+FA2 ([#33164](https://github.com/vllm-project/vllm/pull/33164)) by @NickLucche
* Fix tool call indexing double-counting ([#33141](https://github.com/vllm-project/vllm/pull/33141)) by @wangln19
* Fix IndexError with encoder-decoder models when using Custom Paged Attention ([#33112](https://github.com/vllm-project/vllm/pull/33112)) by @sstamenk
* [Bugfix] Fix DeepseekV32 `AssertionError: num_kv_heads == 1` ([#33090](https://github.com/vllm-project/vllm/pull/33090)) by @NickLucche
* [Bugfix] Fix Voxtral streaming slot_mapping ([#33073](https://github.com/vllm-project/vllm/pull/33073)) by @NickLucche
* [Bugfix] Fix Can't instantiate abstract class DeepseekV32IndexerBackend ([#33052](https://github.com/vllm-project/vllm/pull/33052)) by @chaunceyjiang
* [BugFix] Fix P/D with non-MoE DP ([#33037](https://github.com/vllm-project/vllm/pull/33037)) by @njhill
* [Bugfix] Fix Dtypes for Pynccl Wrapper ([#33030](https://github.com/vllm-project/vllm/pull/33030)) by @robertgshaw2-redhat
* [ROCm][Bugfix] Fix ptpc scale load issue for fused shared expert path in deepseek mtp ([#33018](https://github.com/vllm-project/vllm/pull/33018)) by @ganyi1996ppo
* [CPU Backend][BugFix] Fix failing Darwin pipelines ([#33002](https://github.com/vllm-project/vllm/pull/33002)) by @fadara01
* [AMD][Kernel][BugFix] Use correct scale in concat_and_cache_ds_mla_kernel when on gfx942 ([#32976](https://github.com/vllm-project/vllm/pull/32976)) by @rasmith
* [Core][Bugfix] allow graceful worker termination ([#32965](https://github.com/vllm-project/vllm/pull/32965)) by @joerunde
* [Bugfix][CI] Fix pre-commit ([#32956](https://github.com/vllm-project/vllm/pull/32956)) by @MatthewBonanni
* [Bug] Fix benchmark script `moe_permute_unpermute` ([#32949](https://github.com/vllm-project/vllm/pull/32949)) by @yewentao256
* [Bugfix] Fix missing is_layer_skipped check for FusedMoE in AWQConfig ([#32935](https://github.com/vllm-project/vllm/pull/32935)) by @joninco
* [Bugfix] Fix getting vision features in Transformer Multimodal backend ([#32933](https://github.com/vllm-project/vllm/pull/32933)) by @zucchini-nlp
* [Benchmark][Bugfix] Fix race condtion when starting server for sweep benchmark ([#32927](https://github.com/vllm-project/vllm/pull/32927)) by @Isotr0py
* [Bugfix] Disable tma_aligned_scales in test_fusions_e2e ([#32916](https://github.com/vllm-project/vllm/pull/32916)) by @xyang16
* [Bugfix][TPU] Return a Default fp8 MoE Backend ([#32908](https://github.com/vllm-project/vllm/pull/32908)) by @vanbasten23
* [Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache test ([#32904](https://github.com/vllm-project/vllm/pull/32904)) by @mawong-amd
* [Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama4 ([#32886](https://github.com/vllm-project/vllm/pull/32886)) by @baonudesifeizhai
* [BugFix] deepseek_v32_encoding: Replace asserts with proper exceptions ([#32884](https://github.com/vllm-project/vllm/pull/32884)) by @RishabhSaini
* [CPU Backend][BugFix] Fix failing CPU MoE test ([#32876](https://github.com/vllm-project/vllm/pull/32876)) by @fadara01
* [Bugfix]: resolve torch.compile cache conflict between mm_encoder_tp_modes ([#32842](https://github.com/vllm-project/vllm/pull/32842)) by @HirokenOvo
* [BugFix]  Add env variable to control PDL in LoRA ([#32836](https://github.com/vllm-project/vllm/pull/32836)) by @jeejeelee
* [CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm wrappers) to ensure tensors passed will be made contiguous for the kernel ([#32831](https://github.com/vllm-project/vllm/pull/32831)) by @rasmith
* [Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config not set ([#32777](https://github.com/vllm-project/vllm/pull/32777)) by @karanb192
* fix: preserve native tool call ID in multi-turn tool calling ([#32768](https://github.com/vllm-project/vllm/pull/32768)) by @wangln19
* [Bugfix] fix encoder cache hang in Qwen3VL ([#32684](https://github.com/vllm-project/vllm/pull/32684)) by @JJJYmmm
* Bugfix: Pass router logits dtype in nemotron shared experts ([#32669](https://github.com/vllm-project/vllm/pull/32669)) by @amirkl94
* [Bugfix] Fix E2E latency calculation and add warmup support in mm_processor benchmark ([#32646](https://github.com/vllm-project/vllm/pull/32646)) by @HirokenOvo
* fix: Add glm4_moe_lite to MLA detection ([#32614](https://github.com/vllm-project/vllm/pull/32614)) by @marksverdhei
* [Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound value ([#32279](https://github.com/vllm-project/vllm/pull/32279)) by @xyang16

## Performance

* Add flake8-implicit-str-concat rules to Ruff ([#33191](https://github.com/vllm-project/vllm/pull/33191)) by @hmellor
* [torch.compile] Speed up MOE handling in forward_context ([#33184](https://github.com/vllm-project/vllm/pull/33184)) by @zou3519
* [Benchmark] Add startup benchmarking to buildkite run ([#33183](https://github.com/vllm-project/vllm/pull/33183)) by @desertfire
* [Perf] Optimize dcp allocate tensor ([#33102](https://github.com/vllm-project/vllm/pull/33102)) by @yewentao256
* [Perf] avoid duplicate mem_get_info() call in get_current_memory_usage ([#33064](https://github.com/vllm-project/vllm/pull/33064)) by @pacoxu
* [Perf] Cache exc.errors() result in validation exception handler ([#32984](https://github.com/vllm-project/vllm/pull/32984)) by @sjhddh
* [Perf] Cache xpu_get_mem_info() result to avoid duplicate calls ([#32983](https://github.com/vllm-project/vllm/pull/32983)) by @sjhddh
* [Refactor] Rename `gptq_marlin` to `marlin` to match MoE ([#32952](https://github.com/vllm-project/vllm/pull/32952)) by @mgoin
* [MLA] Fuse cat and qaunt for fp8 kv-cache ([#32950](https://github.com/vllm-project/vllm/pull/32950)) by @LucasWilkinson
* [Perf] Optimize `moe_permute` kernel, 40%~300% kernel performance improvement ([#32892](https://github.com/vllm-project/vllm/pull/32892)) by @yewentao256
* [Performance] Tune Mamba selective scan kernel for B200 ([#32873](https://github.com/vllm-project/vllm/pull/32873)) by @danisereb
* [Perf][Kernel] Optimize FP4 quantization kernels (SM100F) ([#32520](https://github.com/vllm-project/vllm/pull/32520)) by @LopezCastroRoberto
* [AMD][QWEN3-NEXT] FP8 Tunings ([#32042](https://github.com/vllm-project/vllm/pull/32042)) by @draftbk
* [Frontend] add logprob, compression_rate to 'verbose_json' features ([#31059](https://github.com/vllm-project/vllm/pull/31059)) by @sangbumlikeagod
* Add attention benchmarking tools ([#26835](https://github.com/vllm-project/vllm/pull/26835)) by @MatthewBonanni

## Model Support

* [Models] Qwen3-ASR ([#33312](https://github.com/vllm-project/vllm/pull/33312)) by @ywang96
* [Models] Kimi-K2.5 ([#33131](https://github.com/vllm-project/vllm/pull/33131)) by @ywang96
* [DOC]: Add warning about max_num_batched_tokens and max_model_len when chunked prefill is disabled ([#33109](https://github.com/vllm-project/vllm/pull/33109)) by @VincentG1234
* [Model] Bump transformers version for test registry ([#33100](https://github.com/vllm-project/vllm/pull/33100)) by @DarkLight1337
* [Chore] Update type annotation of `input_ids` in model forward ([#33063](https://github.com/vllm-project/vllm/pull/33063)) by @DarkLight1337
* [Model Runner V2] Add LoRAState to consolidate lora logic ([#33062](https://github.com/vllm-project/vllm/pull/33062)) by @WoosukKwon
* [Model Runner V2] Use a different stream for grammar bitmask h2d copy ([#33059](https://github.com/vllm-project/vllm/pull/33059)) by @WoosukKwon
* Adds FunAudioChat multimodal audio model support (#2) ([#33058](https://github.com/vllm-project/vllm/pull/33058)) by @nemoramo
* [Model Runner V2] Minor simplification for finish_requests ([#33048](https://github.com/vllm-project/vllm/pull/33048)) by @WoosukKwon
* [Model Runner V2] Fix slot_mapping after #25954 ([#33046](https://github.com/vllm-project/vllm/pull/33046)) by @WoosukKwon
* [Doc] Add Qwen2.5 models to batch invariance tested models ([#33016](https://github.com/vllm-project/vllm/pull/33016)) by @ZhanqiuHu
* [Model] Use mm_position to compute mrope positions for Qwen3-Omni ([#33010](https://github.com/vllm-project/vllm/pull/33010)) by @Etelis
* [GLM-OCR] GLM-OCR with MTP Support ([#33005](https://github.com/vllm-project/vllm/pull/33005)) by @zRzRzRzRzRzRzR
* Adding optional speculator tests for larger models ([#32943](https://github.com/vllm-project/vllm/pull/32943)) by @shanjiaz
* [CI][Models] Add VLM Support for Sequence Classification Conversion ([#32885](https://github.com/vllm-project/vllm/pull/32885)) by @AndreasKaratzas
* [Model Runner V2] Add KV Connector support ([#32742](https://github.com/vllm-project/vllm/pull/32742)) by @njhill
* [Model][Multimodal] Add explicit MusicFlamingo adapter ([#32696](https://github.com/vllm-project/vllm/pull/32696)) by @WangHaoyuuu
* [Model] Enable LoRA support for internvl2 ([#32397](https://github.com/vllm-project/vllm/pull/32397)) by @MatteoFari
* [LoRA][Spec Decode] Support LoRA for Nemotron-H MTP models ([#32265](https://github.com/vllm-project/vllm/pull/32265)) by @danisereb
* [Models] Add `SharedFusedMoE` support to Qwen3MoE ([#32082](https://github.com/vllm-project/vllm/pull/32082)) by @Isotr0py
* [Models]: Make Multimodal config implicit in ViT implementation ([#31972](https://github.com/vllm-project/vllm/pull/31972)) by @Isotr0py
* Use aiter triton fused_add_rmsnorm_pad for gpt-oss ([#30976](https://github.com/vllm-project/vllm/pull/30976)) by @Rohan138
* [V1][Hybrid] Mamba Prefix Caching with align mode ([#30877](https://github.com/vllm-project/vllm/pull/30877)) by @peakcrosser7

## Hardware & Backend

* [Backport] [Kimi-K2.5] Replace torch.cuda with current_platform for d… ([#33320](https://github.com/vllm-project/vllm/pull/33320)) by @flyrae
* [CI] Update job dependency syntax for Intel and AMD jobs ([#33240](https://github.com/vllm-project/vllm/pull/33240)) by @khluu
* [CI] Update job dependency for hardware and CPU jobs ([#33237](https://github.com/vllm-project/vllm/pull/33237)) by @khluu
* [XPU]disable test_acceptance_length UT ([#33226](https://github.com/vllm-project/vllm/pull/33226)) by @yma11
* [Quantization][Refactor]  use platform dict to choose kernel ([#33130](https://github.com/vllm-project/vllm/pull/33130)) by @zufangzhu
* [ROCm] Enabling forward_includes_kv_cache on ROCm MHA backends ([#33106](https://github.com/vllm-project/vllm/pull/33106)) by @gshtras
* [CI] Fix AssertionError: MCP tool call not found in output_messages ([#33093](https://github.com/vllm-project/vllm/pull/33093)) by @chaunceyjiang
* [DOC] [ROCm] Update doc for v0.14.1 ([#32998](https://github.com/vllm-project/vllm/pull/32998)) by @tjtanaa
* [Docs] Fix Apple silicon include path in CPU installation docs ([#32977](https://github.com/vllm-project/vllm/pull/32977)) by @sjhddh
* Update CPU doc according to feedback ([#32963](https://github.com/vllm-project/vllm/pull/32963)) by @louie-tsai
* [NVIDIA] [feat] Integrate flashinfer Trtllmgen bf16 moe ([#32954](https://github.com/vllm-project/vllm/pull/32954)) by @Linda-Stadter
* [Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VARIANT during install ([#32948](https://github.com/vllm-project/vllm/pull/32948)) by @mgoin
* [cudagraphs] Refactor cudagraph capture loop ([#32946](https://github.com/vllm-project/vllm/pull/32946)) by @LucasWilkinson
* [ROCm][ViT] Enable Flash Attention Triton backend on RDNA3/RDNA4 ([#32944](https://github.com/vllm-project/vllm/pull/32944)) by @monajafi-amd
* [fix] CPUDNNLGEMMHandler pointer baked into inductor artifact ([#32913](https://github.com/vllm-project/vllm/pull/32913)) by @dolpm
* [CI/Build][CPU] Fix failed pooling tests and macos smoke test ([#32907](https://github.com/vllm-project/vllm/pull/32907)) by @bigPYJ1151
* [Intel GPU] refine xpu worker ([#32894](https://github.com/vllm-project/vllm/pull/32894)) by @jikunshang
* Set splitk=1 for fused-moe-lora expand kernel ([#32882](https://github.com/vllm-project/vllm/pull/32882)) by @dcmaddix
* [CPU][Feat] Update PyTorch to v2.10 for CPU Backend ([#32869](https://github.com/vllm-project/vllm/pull/32869)) by @fadara01
* Enabling "2 node" distributed tests in the AMD CI pipeline. ([#32719](https://github.com/vllm-project/vllm/pull/32719)) by @Alexei-V-Ivanov-AMD
* [Refactor] Clean up unused variables & func ([#32692](https://github.com/vllm-project/vllm/pull/32692)) by @yewentao256

## Refactoring & Core

* [Refactor] Define MM data parser in processing info instead of processor itself ([#33260](https://github.com/vllm-project/vllm/pull/33260)) by @DarkLight1337
* [Frontend] Cleanup api server ([#33158](https://github.com/vllm-project/vllm/pull/33158)) by @noooop
* [Misc] Cleanup Kimi-K2.5's vision chunk modality entrypoints ([#33157](https://github.com/vllm-project/vllm/pull/33157)) by @Isotr0py
* [Refactor] Use data parser for matching data items to multi-modal UUIDs ([#32955](https://github.com/vllm-project/vllm/pull/32955)) by @DarkLight1337

## Build, CI & Testing

* [CI] Change GPU key to device key for B200 test ([#33275](https://github.com/vllm-project/vllm/pull/33275)) by @khluu
* [CI] Enable mypy import following for `vllm/compilation` ([#33199](https://github.com/vllm-project/vllm/pull/33199)) by @hmellor
* [CI] minor fixes to pipeline generator and tests ([#33151](https://github.com/vllm-project/vllm/pull/33151)) by @khluu
* [CI] Whisper tests `enforce_eager=False` ([#33098](https://github.com/vllm-project/vllm/pull/33098)) by @NickLucche
* [ci] Sync test areas with test-pipeline.yaml and enable new pipeline generator ([#33080](https://github.com/vllm-project/vllm/pull/33080)) by @khluu
* [Tests] Clarify pytest skip reasons with actionable context ([#32981](https://github.com/vllm-project/vllm/pull/32981)) by @sjhddh
* [CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh ([#32971](https://github.com/vllm-project/vllm/pull/32971)) by @Harry-Chen
* [torch.compile][CI] Add back attn fusion on hopper/ada ([#32940](https://github.com/vllm-project/vllm/pull/32940)) by @ProExpertProg
* [CI][Pooling] Stabilize ModernBERT test ([#32909](https://github.com/vllm-project/vllm/pull/32909)) by @AndreasKaratzas
* [lora/moe] Avoid extra intermediate buffer & Python slicing in expand phase when split_k == 1 ([#32774](https://github.com/vllm-project/vllm/pull/32774)) by @cwazai

## Documentation

* [Doc]: fixing multiple typos in diverse files ([#33256](https://github.com/vllm-project/vllm/pull/33256)) by @didier-durand
* Revert "Enable Cross layers KV cache layout at NIXL Connector (#30207)" ([#33241](https://github.com/vllm-project/vllm/pull/33241)) by @orozery
* [docs] Improve tlparse section ([#33211](https://github.com/vllm-project/vllm/pull/33211)) by @angelayi
* [Docs] Use definition lists for CLI reference docs ([#33186](https://github.com/vllm-project/vllm/pull/33186)) by @hmellor
* [Doc] Improve serve parameter documentation with meaningful defaults ([#33082](https://github.com/vllm-project/vllm/pull/33082)) by @karanb192
* [StepVL] add step vl offline example ([#33054](https://github.com/vllm-project/vllm/pull/33054)) by @ltd0924
* [Voxtral] Streaming example ([#33042](https://github.com/vllm-project/vllm/pull/33042)) by @patrickvonplaten
* [Doc] Ignore typo check on governance doc ([#32999](https://github.com/vllm-project/vllm/pull/32999)) by @ywang96
* [docs] Update governance process links ([#32995](https://github.com/vllm-project/vllm/pull/32995)) by @esmeetu
* Auth_token added in documentation as it is required ([#32988](https://github.com/vllm-project/vllm/pull/32988)) by @ruizcrp
* [Frontend][3/n] Make pooling entrypoints request schema consensus | EmbedRequest & ClassifyRequest ([#32905](https://github.com/vllm-project/vllm/pull/32905)) by @noooop
* [Docs] Adding links and intro to Speculators and LLM Compressor ([#32849](https://github.com/vllm-project/vllm/pull/32849)) by @aireilly
* [Doc] Update outdated link to Ray documentation ([#32660](https://github.com/vllm-project/vllm/pull/32660)) by @graftim
* [7/N][Attention][Docs] Add documentation for attention backends ([#32477](https://github.com/vllm-project/vllm/pull/32477)) by @MatthewBonanni
* [feat][log]: add `--disable-access-log-for-endpoints` CLI option ([#30011](https://github.com/vllm-project/vllm/pull/30011)) by @JaredforReal
* [Misc] HF Hub LoRA Resolver ([#20320](https://github.com/vllm-project/vllm/pull/20320)) by @alex-jw-brooks

## Miscellaneous

* [Chore] Move `MediaConnector` to `vllm.multimodal.media` ([#33324](https://github.com/vllm-project/vllm/pull/33324)) by @DarkLight1337
* [ez] Delete more torch version checks <= 2.8 ([#33288](https://github.com/vllm-project/vllm/pull/33288)) by @angelayi
* [ez] Delete torch25_custom_graph_pass ([#33287](https://github.com/vllm-project/vllm/pull/33287)) by @angelayi
* [Misc] Add orozery to CODEOWNERS (core, kv_transfer, kv_offload) ([#33227](https://github.com/vllm-project/vllm/pull/33227)) by @orozery
* [Misc] Provide a DeepSeek ReasoningParser with thinking enabled by default ([#33221](https://github.com/vllm-project/vllm/pull/33221)) by @chaunceyjiang
* Relax protobuf library version constraints ([#33202](https://github.com/vllm-project/vllm/pull/33202)) by @jeffreywang-anyscale
* [UX] Enable nested configs in config yaml files ([#33193](https://github.com/vllm-project/vllm/pull/33193)) by @mgoin
* [Feature]: Container image WORKDIR consistency ([#33159](https://github.com/vllm-project/vllm/pull/33159)) by @SouthWest7
* [PluggableLayer][2/N] Apply PluggableLayer to linear layers ([#33152](https://github.com/vllm-project/vllm/pull/33152)) by @whx-sjtu
* [Frontend] Frontend will only attach supported tasks corresponding entrypoints. ([#33139](https://github.com/vllm-project/vllm/pull/33139)) by @noooop
* [torch.compile] Stop assuming 32 bit indexing ([#33113](https://github.com/vllm-project/vllm/pull/33113)) by @zou3519
* [Metrics][MFU] Fix UnembedMetrics FLOP overcounting for prefill (#33045) ([#33045](https://github.com/vllm-project/vllm/pull/33045)) by @omkhalil
* [UX] Deduplicate sampling parameter startup logs ([#32953](https://github.com/vllm-project/vllm/pull/32953)) by @DarkLight1337
* [StepVL] support close img patch ([#32923](https://github.com/vllm-project/vllm/pull/32923)) by @ltd0924
* [fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile factors ([#32912](https://github.com/vllm-project/vllm/pull/32912)) by @dolpm
* [Voxtral] Add new streaming arch ([#32861](https://github.com/vllm-project/vllm/pull/32861)) by @patrickvonplaten
* [torch.compile] Compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` to avoid raw torch ops inside opaque custom ops ([#32806](https://github.com/vllm-project/vllm/pull/32806)) by @ProExpertProg
* [Misc] Log vLLM logo when starting server ([#32796](https://github.com/vllm-project/vllm/pull/32796)) by @njhill
* [fix] tesdt mcp_tool_calling_streaming with a more complex math question ([#32769](https://github.com/vllm-project/vllm/pull/32769)) by @daniel-salib
* [Feature] Add LoRA support for Gemma3 vision components ([#32764](https://github.com/vllm-project/vllm/pull/32764)) by @vihaan-that
* [Misc] Add `get_name` to missing AttentionBackends ([#32698](https://github.com/vllm-project/vllm/pull/32698)) by @NickLucche
* Using max_loras + 1 to construct grid in fused_moe_lora ([#32277](https://github.com/vllm-project/vllm/pull/32277)) by @yugong333
*  [FIX] Always support TP > 4 for FP4 Gemm ([#31099](https://github.com/vllm-project/vllm/pull/31099)) by @danielafrimi
* [Metrics] [KVConnector] Add Offloading Connector metrics ([#27942](https://github.com/vllm-project/vllm/pull/27942)) by @omerpaz95

## Breaking Changes

* [Multimodal] Simplify MM input definitions ([#33331](https://github.com/vllm-project/vllm/pull/33331)) by @DarkLight1337
* [Release] [ROCm] Remove old build step ([#33316](https://github.com/vllm-project/vllm/pull/33316)) by @tjtanaa
* [Chore] Remove `use_data_parallel` kwargs from ViT implementation  ([#33310](https://github.com/vllm-project/vllm/pull/33310)) by @Isotr0py
* [Misc] Remove missed `pad_for_cudagraph` ([#33283](https://github.com/vllm-project/vllm/pull/33283)) by @LucasWilkinson
* [UX] Remove noisy CT UnquantizedLinearMethod warn ([#33273](https://github.com/vllm-project/vllm/pull/33273)) by @mgoin
* [ModelRunner V2] Misc code simplification and cleanup ([#33266](https://github.com/vllm-project/vllm/pull/33266)) by @njhill
* [ez] Remove checks for torch version <= 2.8 ([#33209](https://github.com/vllm-project/vllm/pull/33209)) by @angelayi
* Don't use `min_pixels`/`max_pixels` from Qwen2VL's processor ([#33208](https://github.com/vllm-project/vllm/pull/33208)) by @hmellor
* Make `mypy` opt-out instead of opt-in ([#33205](https://github.com/vllm-project/vllm/pull/33205)) by @hmellor
* [Misc][Build] Lazy load cv2 in nemotron_parse.py ([#33189](https://github.com/vllm-project/vllm/pull/33189)) by @kiersten-stokes
* [Attention] Use `has_flashinfer` helper ([#33177](https://github.com/vllm-project/vllm/pull/33177)) by @MatthewBonanni
* Fix weight mapping test for Transfomers v5 ([#33162](https://github.com/vllm-project/vllm/pull/33162)) by @hmellor
* [Release] [CI] Optim release pipeline ([#33156](https://github.com/vllm-project/vllm/pull/33156)) by @tjtanaa
* [code clean] remove duplicate code ([#33135](https://github.com/vllm-project/vllm/pull/33135)) by @andyxning
* [release] Minor fixes to release annotation and wheel upload ([#33129](https://github.com/vllm-project/vllm/pull/33129)) by @khluu
* [CI/Build][BugFix] fix cuda/compat loading order issue in docker build ([#33116](https://github.com/vllm-project/vllm/pull/33116)) by @wpc
* [Refactor] Remove unused `_moe_permute` function ([#33108](https://github.com/vllm-project/vllm/pull/33108)) by @yewentao256
* [Bugfix][MXFP4] Call `trtllm_fp4_block_scale_moe` with kwargs ([#33104](https://github.com/vllm-project/vllm/pull/33104)) by @wpc
* [Frontend] Cleanup serving engine ([#33103](https://github.com/vllm-project/vllm/pull/33103)) by @DarkLight1337
* [Frontend] Reduce mixin usage in serving pooling ([#33101](https://github.com/vllm-project/vllm/pull/33101)) by @DarkLight1337
* Remove unused logic in `models/mistral.py` ([#33095](https://github.com/vllm-project/vllm/pull/33095)) by @andylolu2
* [Docs] Simplify CPU x86 Docker build documentation ([#33071](https://github.com/vllm-project/vllm/pull/33071)) by @maryamtahhan
* [Doc] Further update multi-modal impl doc ([#33065](https://github.com/vllm-project/vllm/pull/33065)) by @DarkLight1337
* [Model Runner V2] Remove UvaBufferPool for cpu->gpu copy ([#33055](https://github.com/vllm-project/vllm/pull/33055)) by @WoosukKwon
* [CI] Fix MHA attention test failure (AttributeError when model_config is None in ViT attention backend) ([#33033](https://github.com/vllm-project/vllm/pull/33033)) by @LucasWilkinson
* [Tests] Remove Duplicates ([#33032](https://github.com/vllm-project/vllm/pull/33032)) by @robertgshaw2-redhat
* [Bugfix] Fix display error (inconsistent with context) ([#33020](https://github.com/vllm-project/vllm/pull/33020)) by @lingebeng
* [Tests] Replace flaky sleep with polling in test_background_cancel ([#32986](https://github.com/vllm-project/vllm/pull/32986)) by @sjhddh
* [Tests] Standardize RNG seed utility across test files ([#32982](https://github.com/vllm-project/vllm/pull/32982)) by @sjhddh
* [Bugfix][VLM] Fix transformers backend embed_multimodal for Qwen2.5-VL profiling ([#32969](https://github.com/vllm-project/vllm/pull/32969)) by @AndreasKaratzas
* [ROCm][PD] Remove unused moriio connector proxy code ([#32939](https://github.com/vllm-project/vllm/pull/32939)) by @markmc
* [ROCm][CI] Add TORCH_NCCL_BLOCKING_WAIT For Distributed Tests (A100) ([#32891](https://github.com/vllm-project/vllm/pull/32891)) by @micah-wil
* [BugFix] Async Eplb fix potential race condition ([#32881](https://github.com/vllm-project/vllm/pull/32881)) by @ilmarkov
* [Misc] Postpone torch_profiler deprecation ([#32867](https://github.com/vllm-project/vllm/pull/32867)) by @NickLucche
* [Bugfix] Lazy import NgramProposer in GPU model runner ([#32821](https://github.com/vllm-project/vllm/pull/32821)) by @22quinn
* [Feature]: Remove DtoH Copy for lfm2_vl On Default Stream ([#32815](https://github.com/vllm-project/vllm/pull/32815)) by @tianshu-Michael-yu
* [EncoderCacheManager] Remove unnecessary copy ([#32800](https://github.com/vllm-project/vllm/pull/32800)) by @lgeiger
* [Model] Use mm_position to compute mrope positions for Qwen2.5-Omni ([#32772](https://github.com/vllm-project/vllm/pull/32772)) by @Etelis
* [lora/moe] Improve fused MoE‑LoRA kernel indexing and memory access ([#32770](https://github.com/vllm-project/vllm/pull/32770)) by @cwazai
* [CI] Fix mypy for `vllm/v1/structured_output` ([#32722](https://github.com/vllm-project/vllm/pull/32722)) by @yewentao256
* [Quantization][Deprecation] Remove Marlin 24 ([#32688](https://github.com/vllm-project/vllm/pull/32688)) by @robertgshaw2-redhat
* [Quantization][Deprecation] Remove BitBlas ([#32683](https://github.com/vllm-project/vllm/pull/32683)) by @robertgshaw2-redhat
* [Feature] Fully support for async scheduling + PP, 30.8% E2E throughput improvement, 31.8% TPOT improvement ([#32618](https://github.com/vllm-project/vllm/pull/32618)) by @yewentao256
* [MoE Refactor] Integrate Naive Prepare Finalize into MK ([#32567](https://github.com/vllm-project/vllm/pull/32567)) by @robertgshaw2-redhat
* [5/N][Attention] Finish eliminating `vllm/attention` folder ([#32064](https://github.com/vllm-project/vllm/pull/32064)) by @MatthewBonanni
* [Bug Fix] Handle variable-length tensors in MultiModalFlatField batching ([#31751](https://github.com/vllm-project/vllm/pull/31751)) by @AndriiPasternak31
* feat(benchmark): add encoder forward pass benchmarking to mm-processor ([#31655](https://github.com/vllm-project/vllm/pull/31655)) by @reaganjlee
* [CPU] Improve CPU Docker build  ([#30953](https://github.com/vllm-project/vllm/pull/30953)) by @maryamtahhan
* [CI][torch nightlies] Use main Dockerfile with flags for nightly torch tests ([#30443](https://github.com/vllm-project/vllm/pull/30443)) by @orionr
* [Feature] add session based streaming input support to v1 ([#28973](https://github.com/vllm-project/vllm/pull/28973)) by @joshuadeng
* [Performance] Split FlashAttn attention and cache update ([#25954](https://github.com/vllm-project/vllm/pull/25954)) by @ElizaWszola

## Upgrade Notes

- Breaking Commit: `9ad7f89f5` - [Models]: Make Multimodal config implicit in ViT implementation (#31972)
- The breaking commit introduced new functions `get_vit_attn_backend()` and `is_vit_use_data_parallel()` in `vllm/model_executor/models/vision.py` that access `vllm_config.model_config.multimodal_config`. However, the code only catches `AssertionError` from `get_current_vllm_config()`, but doesn't han
- **NOTE: We observed a crash in FlashInfer’s autotuning path on SM100  when running Llama-4 FP8 with expert parallelism. During vLLM startup, flashinfer_autotune is invoked in kernel_warmup, and the process segfaults immediately after autotuning begins. This is a hard crash in the C++/CUDA kernel sel
- - ACL upgrade: Upgraded Arm Compute Library (ACL) to v52.6.0, fixing some crashes with very large tensor sizes  [#issues/165654 ](https://github.com/pytorch/pytorch/issues/165654). [#pull/165904](https://github.com/pytorch/pytorch/pull/165904)
- vllm/v1/attention/backends/fa_utils.py:28: note: Original:
- vllm/v1/attention/backends/fa_utils.py:28: note:     def flash_attn_varlen_func(q: Any, k: Any, v: Any, max_seqlen_q: Any, cu_seqlens_q: Any, max_seqlen_k: Any, cu_seqlens_k: Any = ..., seqused_k: Any = ..., q_v: Any = ..., dropout_p: Any = ..., softmax_scale: Any = ..., causal: Any = ..., window_si
- vllm/v1/attention/backends/fa_utils.py:28: note: Redefinition:
- (Note: Mean, Median, and P75.0 are identical due to fallback logic. Also, without warmup, the MM Processor Timing stats show high variance/Std.)

## Contributors

@22quinn, @Alexei-V-Ivanov-AMD, @AndreasKaratzas, @AndriiPasternak31, @CarstyYou, @Chenhao-Guan, @DarkLight1337, @ElizaWszola, @Etelis, @Harry-Chen, @HirokenOvo, @Isotr0py, @JJJYmmm, @JaredforReal, @Linda-Stadter, @LopezCastroRoberto, @LucasWilkinson, @MatteoFari, @MatthewBonanni, @NickLucche, @ProExpertProg, @RishabhSaini, @Rohan138, @SouthWest7, @VincentG1234, @WangHaoyuuu, @WoosukKwon, @ZhanqiuHu, @aireilly, @alex-jw-brooks, @amirkl94, @andylolu2, @andyxning, @angelayi, @baonudesifeizhai, @bigPYJ1151, @chaunceyjiang, @cmunley1, @cwazai, @daniel-salib, @danielafrimi, @danisereb, @dcmaddix, @desertfire, @didier-durand, @dolpm, @draftbk, @esmeetu, @fadara01, @flyrae, @ganyi1996ppo, @graftim, @gshtras, @hmellor, @ilmarkov, @ir1ka, @irisliu10, @jeejeelee, @jeffreywang-anyscale, @jikunshang, @joerunde, @joninco, @joshuadeng, @karanb192, @khluu, @kiersten-stokes, @lgeiger, @lingebeng, @louie-tsai, @ltd0924, @markmc, @marksverdhei, @maryamtahhan, @mawong-amd, @mgoin, @micah-wil, @monajafi-amd, @nemoramo, @njhill, @noooop, @omerpaz95, @omkhalil, @orionr, @orozery, @pacoxu, @patrickvonplaten, @peakcrosser7, @rasmith, @reaganjlee, @robertgshaw2-redhat, @ruizcrp, @sangbumlikeagod, @shanjiaz, @sjhddh, @sstamenk, @tianshu-Michael-yu, @tjtanaa, @vanbasten23, @vihaan-that, @wangln19, @whx-sjtu, @wpc, @xuechendi, @xyang16, @yewentao256, @yma11, @yugong333, @ywang96, @zRzRzRzRzRzRzR, @zou3519, @zucchini-nlp, @zufangzhu