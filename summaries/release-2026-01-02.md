# Weekly Release Report for vllm-project/vllm (2026-01-02)

This week merged 78 PRs from 55 contributors. Key areas: features 8, fixes 4, performance 5.

## Executive Summary

本周发布主要围绕多模态支持、性能优化与模型功能扩展展开。在模型支持方面，新增了对 openPangu MoE 模型的支持，并为 GLM-ASR 引入了多模态处理能力。LoRA 支持得到进一步增强，现已覆盖 DeepSeek-OCR、NemotronHModel 以及 LLaVA 等模型的 Tower 与 Connector 组件。

性能优化包括为 GLM-4.5/4.6 系列模型在特定硬件上添加 Fused MoE Triton 内核，以及对 MiniMax-M2 模型的 QKNorm 进行优化。核心引擎引入了混合分配器与 KV 缓存连接器的支持。关键修复涉及 EAGLE 推测解码的块大小映射、池化模型的异步调度问题以及 Kimi 工具的解析逻辑。

本次发布包含若干重要变更：测试模型已从 MiniCPM3-4B 升级为 MiniCPM4.1-8B；移除了 `_init_model_kwargs` 中未使用的 `num_tokens` 参数；并持续推进 MoE 代码重构。升级时需注意，若要为其他多模态模型的 Tower 和 Connector 启用 LoRA，必须在对应类中实现相应功能函数。

## Highlights

* [Model] Add support for openPangu moe model ([#28775](https://github.com/vllm-project/vllm/pull/28775)) by @yt0428
* Feature/isaac 0.1 ([#28367](https://github.com/vllm-project/vllm/pull/28367)) by @oscardev256
* Add GLM-ASR multimodal support  ([#31436](https://github.com/vllm-project/vllm/pull/31436)) by @baonudesifeizhai
* Add Multimodal Processor Benchmark  ([#29105](https://github.com/vllm-project/vllm/pull/29105)) by @reaganjlee
* [Core][Hybrid allocator + connector] Support hybrid allocator + kv cache connector ([#30166](https://github.com/vllm-project/vllm/pull/30166)) by @ivanium

## Features & Enhancements

* feat: support LoRA for DeepSeek-OCR(Language Model part) ([#31569](https://github.com/vllm-project/vllm/pull/31569)) by @zhima771
* Add get_expert_mapping to NemotronHModel (for LoRA support) ([#31539](https://github.com/vllm-project/vllm/pull/31539)) by @danisereb
* Add docker buildx bake configuration ([#31477](https://github.com/vllm-project/vllm/pull/31477)) by @amrmahdi
* Add GLM-ASR multimodal support  ([#31436](https://github.com/vllm-project/vllm/pull/31436)) by @baonudesifeizhai
* Add Loraconfig parameter to  get_punica_wrapper function ([#31408](https://github.com/vllm-project/vllm/pull/31408)) by @ZT-AIA
* add tip for VLLM_USE_PRECOMPILED arg to reduce docker build time ([#31385](https://github.com/vllm-project/vllm/pull/31385)) by @yitingdc
* feat(frontend): add --default-chat-template-kwargs CLI argument ([#31343](https://github.com/vllm-project/vllm/pull/31343)) by @effortprogrammer
* Feature/isaac 0.1 ([#28367](https://github.com/vllm-project/vllm/pull/28367)) by @oscardev256

## Bug Fixes

* [BugFix] Fix async scheduling for pooling models ([#31584](https://github.com/vllm-project/vllm/pull/31584)) by @njhill
* [Bugfix] Fix block size used in EAGLE slot mapping ([#31540](https://github.com/vllm-project/vllm/pull/31540)) by @benchislett
* fix: update kimi k2 tool parser logic ([#31207](https://github.com/vllm-project/vllm/pull/31207)) by @wangln19
* Fix/get raw stream patch #30905 ([#30912](https://github.com/vllm-project/vllm/pull/30912)) by @baonudesifeizhai

## Performance

* Optimize QKNorm for MiniMax-M2/M2.1 ([#31493](https://github.com/vllm-project/vllm/pull/31493)) by @rogeryoungh
* Add Fused MoE Triton kernels for GLM-4.5-Air, GLM-4.5v, GLM-4.6v on 2x RTX Pro 6000 ([#31407](https://github.com/vllm-project/vllm/pull/31407)) by @mratsim
* [benchmark] use model card root instead of id ([#31329](https://github.com/vllm-project/vllm/pull/31329)) by @andyxning
* [Core][Hybrid allocator + connector] Support hybrid allocator + kv cache connector ([#30166](https://github.com/vllm-project/vllm/pull/30166)) by @ivanium
* Add Multimodal Processor Benchmark  ([#29105](https://github.com/vllm-project/vllm/pull/29105)) by @reaganjlee

## Model Support

* [ROCm][CI] Fix failure in Language Models Tests (Extra Standard) by reducing agent pool size ([#31553](https://github.com/vllm-project/vllm/pull/31553)) by @AndreasKaratzas
* [Model] Enable LoRA support for tower and connector in LLaVA ([#31513](https://github.com/vllm-project/vllm/pull/31513)) by @jayhemnani9910
* [Bugfix]Fix pooling model always disabled due to incorrect PP rank check ([#31505](https://github.com/vllm-project/vllm/pull/31505)) by @vintipandey
* Replace `nn.ConvNd` with vLLM's `ConvNdLayer` for Transformers modeling backend ([#31498](https://github.com/vllm-project/vllm/pull/31498)) by @hmellor
* [Model] Add tuned triton fused_moe configs for Qwen3Moe on B200 ([#31448](https://github.com/vllm-project/vllm/pull/31448)) by @Jzz1943
* [Mics] add pcp basic support to MoE model ([#31003](https://github.com/vllm-project/vllm/pull/31003)) by @pisceskkk
* [Core] Initialize LoRA support for tower and connector in multi-modal models ([#26674](https://github.com/vllm-project/vllm/pull/26674)) by @jeejeelee

## Hardware & Backend

* [CI] [Critical] [CUDA] Fix duplicated test name ([#31562](https://github.com/vllm-project/vllm/pull/31562)) by @tjtanaa
* [CPU] Disable async schedule on CPU ([#31525](https://github.com/vllm-project/vllm/pull/31525)) by @bigPYJ1151
* [ROCm][Bugfix] Fix accuracy issue on fmoe when `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` enabled ([#31523](https://github.com/vllm-project/vllm/pull/31523)) by @ganyi1996ppo
* [xpu] [bugfix] upgrade to latest oneccl in dockerfile ([#31522](https://github.com/vllm-project/vllm/pull/31522)) by @rogerxfeng8
* [BugFix] Fix NUMA node validation in CPU platform ([#31520](https://github.com/vllm-project/vllm/pull/31520)) by @SameerAsal
* [Bugfix][ROCm] Fix Static Quant Issue ([#31502](https://github.com/vllm-project/vllm/pull/31502)) by @robertgshaw2-redhat
* [CI/Build][CPU] Update CPU CI test cases ([#31466](https://github.com/vllm-project/vllm/pull/31466)) by @bigPYJ1151
* [ROCm][CI] Skip DeepGemm-dependent test on ROCm platform ([#31462](https://github.com/vllm-project/vllm/pull/31462)) by @AndreasKaratzas
* [CI]Test Group 'NixlConnector PD accuracy tests' is fixed ([#31460](https://github.com/vllm-project/vllm/pull/31460)) by @qli88
* [ROCm][CI] Added perceptron lib in requirements for isaac multi-modal test ([#31441](https://github.com/vllm-project/vllm/pull/31441)) by @AndreasKaratzas
* [XPU][CI]skip test_preprocess_error_handling due to fork/spawn issue ([#31381](https://github.com/vllm-project/vllm/pull/31381)) by @jikunshang
* [BugFix] Re-fix async multimodal cpu tensor race condition ([#31373](https://github.com/vllm-project/vllm/pull/31373)) by @njhill
* [ROCm] Migrate xgrammar to upstream release ([#31327](https://github.com/vllm-project/vllm/pull/31327)) by @AndreasKaratzas
* CustomOp: Unify aiter impl into GroupedTopk ([#31221](https://github.com/vllm-project/vllm/pull/31221)) by @xinyu-intel
* [CI/ROCm] Fixing "V1 Test attention (H100)" test group. ([#31187](https://github.com/vllm-project/vllm/pull/31187)) by @Alexei-V-Ivanov-AMD

## Refactoring & Core

* [Minor] Various small code cleanups/simplifications ([#31508](https://github.com/vllm-project/vllm/pull/31508)) by @njhill
* [Feature] Add offline FastAPI documentation support for air-gapped environments ([#30184](https://github.com/vllm-project/vllm/pull/30184)) by @rickychen-infinirc

## Build, CI & Testing

* [Bugfix] Replace BaseException with specific exceptions in FLA utils ([#31590](https://github.com/vllm-project/vllm/pull/31590)) by @c0de128
* [CI][Bugfix] Fix token counting in chunked prefill streaming test ([#31565](https://github.com/vllm-project/vllm/pull/31565)) by @AndreasKaratzas
* [Docs] Use relative `md` links instead of absolute `html` links for cross referencing ([#31494](https://github.com/vllm-project/vllm/pull/31494)) by @hmellor
* [CI][NIXL] Split DPEP tests ([#31491](https://github.com/vllm-project/vllm/pull/31491)) by @NickLucche
* [CI] fix test_chat_truncation_content_not_null test ([#31488](https://github.com/vllm-project/vllm/pull/31488)) by @chaunceyjiang
* [CI/Build] Ignore max transformers version for more common tests ([#31401](https://github.com/vllm-project/vllm/pull/31401)) by @Isotr0py
* [CI/Build] Ignore max transformers version skipping for initialization tests ([#30619](https://github.com/vllm-project/vllm/pull/30619)) by @Isotr0py

## Documentation

* [Fix] Align fused moe lora_b shape with peft ([#31534](https://github.com/vllm-project/vllm/pull/31534)) by @linitra24
* Migrate meetups & sponsors [2/N] ([#31500](https://github.com/vllm-project/vllm/pull/31500)) by @esmeetu
* Migrate doc to website: Hardware Plugins (1/N) ([#31496](https://github.com/vllm-project/vllm/pull/31496)) by @esmeetu
* [Docs] Fix some snippets ([#31378](https://github.com/vllm-project/vllm/pull/31378)) by @hmellor
* [Docs] Add profiler user docs for http request ([#31370](https://github.com/vllm-project/vllm/pull/31370)) by @lengrongfu
* [Audio] Improve Audio Inference Scripts (offline/online) ([#29279](https://github.com/vllm-project/vllm/pull/29279)) by @ekagra-ranjan
* [Prefix Cache] Include lora_name in BlockStored event for deterministic KV-cache reconstruction ([#27577](https://github.com/vllm-project/vllm/pull/27577)) by @sagearc

## Miscellaneous

* [Bugfix] Fix BAGEL online serving for text and image understanding ([#31546](https://github.com/vllm-project/vllm/pull/31546)) by @Dylan1229
* [Core] Deduplicate generate/encode logic in `AsyncLLM` ([#31510](https://github.com/vllm-project/vllm/pull/31510)) by @njhill
* [Frontend] add continue_final_message parameter to /embeddings endpoint ([#31497](https://github.com/vllm-project/vllm/pull/31497)) by @kevin-pw
* [BugFix]  add select_gemm_impl on CompressedTensorsWNA16MoEMethod to support LoRA ([#31453](https://github.com/vllm-project/vllm/pull/31453)) by @JartX
* [Bugfix][Frontend] Fix Jina reranker multimodal input compatibility ([#31445](https://github.com/vllm-project/vllm/pull/31445)) by @twjww
* [Bugfix] Preserve tool call id/type/name in streaming finish chunk ([#31438](https://github.com/vllm-project/vllm/pull/31438)) by @amittell
* implements register kv caches in lmcache connector ([#31397](https://github.com/vllm-project/vllm/pull/31397)) by @chunxiaozheng
* [BugFix] register quant scale tensors as buffer ([#31395](https://github.com/vllm-project/vllm/pull/31395)) by @BoyuanFeng
* [Bug] Fix log issue with `\n` ([#31390](https://github.com/vllm-project/vllm/pull/31390)) by @yewentao256
* [BugFix] Fix cache issue in compilation_config ([#31376](https://github.com/vllm-project/vllm/pull/31376)) by @BoyuanFeng
* [Mistral common] Ensure all functions are imported from the top & only use public methods ([#31138](https://github.com/vllm-project/vllm/pull/31138)) by @patrickvonplaten
* [Core] Enable async scheduling by default ([#27614](https://github.com/vllm-project/vllm/pull/27614)) by @njhill

## Breaking Changes

* [ROCm][CI] Update MiniCPM model test: MiniCPM3-4B to MiniCPM4.1-8B and simplify attention backend testing ([#31551](https://github.com/vllm-project/vllm/pull/31551)) by @AndreasKaratzas
* [Core] Remove unused `num_tokens` parameter from `_init_model_kwargs` ([#31517](https://github.com/vllm-project/vllm/pull/31517)) by @maang-h
* [MoE Refactor][12/N] Marlin Fp8 MoE Pure Function ([#31499](https://github.com/vllm-project/vllm/pull/31499)) by @robertgshaw2-redhat
* [Chore]: Remove HF format Phi4-MM examples ([#31405](https://github.com/vllm-project/vllm/pull/31405)) by @Isotr0py
* [Misc] Fix Qwen2-MoE shared_expert_gate ([#31339](https://github.com/vllm-project/vllm/pull/31339)) by @jeejeelee
* [CI] Fix flaky vision beam search test with flexible semantic validation ([#31324](https://github.com/vllm-project/vllm/pull/31324)) by @AndreasKaratzas
* [ROCm][CI] Add TorchCodec source build for transcription tests ([#31323](https://github.com/vllm-project/vllm/pull/31323)) by @AndreasKaratzas
* [Bugfix] Support LoRA and GPTQModel for PLaMo 2/3  ([#31322](https://github.com/vllm-project/vllm/pull/31322)) by @Alnusjaponica
* [MoE Refactor][10/N] Cleanup Fp8 Process Weights After Loading ([#31169](https://github.com/vllm-project/vllm/pull/31169)) by @robertgshaw2-redhat
* [ROCm][GPTQ][Bugfix] Fix GPTQ GEMM kernel output zeroing race condition ([#30719](https://github.com/vllm-project/vllm/pull/30719)) by @AndreasKaratzas
* [Model] Add support for openPangu moe model ([#28775](https://github.com/vllm-project/vllm/pull/28775)) by @yt0428

## Upgrade Notes

- #### 1. Model Upgrade: MiniCPM3-4B to MiniCPM4.1-8B
- Upgrade to 2021.15.7.6 to include the fixes for allreduce overflow issues
- Note: those were produced through SGLang tuner.
- **Note:** To support LoRA for the tower and connector of other multi-modal models, these two functions must be implemented in their respective classes.

## Contributors

@Alexei-V-Ivanov-AMD, @Alnusjaponica, @AndreasKaratzas, @BoyuanFeng, @Dylan1229, @Isotr0py, @JartX, @Jzz1943, @NickLucche, @SameerAsal, @ZT-AIA, @amittell, @amrmahdi, @andyxning, @baonudesifeizhai, @benchislett, @bigPYJ1151, @c0de128, @chaunceyjiang, @chunxiaozheng, @danisereb, @effortprogrammer, @ekagra-ranjan, @esmeetu, @ganyi1996ppo, @hmellor, @ivanium, @jayhemnani9910, @jeejeelee, @jikunshang, @kevin-pw, @lengrongfu, @linitra24, @maang-h, @mratsim, @njhill, @oscardev256, @patrickvonplaten, @pisceskkk, @qli88, @reaganjlee, @rickychen-infinirc, @robertgshaw2-redhat, @rogerxfeng8, @rogeryoungh, @sagearc, @tjtanaa, @twjww, @vintipandey, @wangln19, @xinyu-intel, @yewentao256, @yitingdc, @yt0428, @zhima771