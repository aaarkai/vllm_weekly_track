# Weekly Release Notes for vllm-project/vllm (2025-09-26)

## What's Changed

### ‚ú® Features & Enhancements

* Add backward compatibility for `guided_...` API ([#25615](https://github.com/vllm-project/vllm/pull/25615)) by @hmellor
* feat: BF16 FlashInfer Fused Cutlass MOE for Hopper and Blackwell Expert Parallel ([#25503](https://github.com/vllm-project/vllm/pull/25503)) by @djmmoss
* Add `VLLM_NVTX_SCOPES_FOR_PROFILING=1` to enable `nvtx.annotate` scopes ([#25501](https://github.com/vllm-project/vllm/pull/25501)) by @coreylowman
* Add VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE & VLLM_ENABLE_INDUCTOR_COORDINA‚Ä¶ ([#25493](https://github.com/vllm-project/vllm/pull/25493)) by @rouchenzi
* Add backward compatibility for `GuidedDecodingParams` ([#25422](https://github.com/vllm-project/vllm/pull/25422)) by @hmellor
* [feat] Support MRoPE +  YaRN ([#25384](https://github.com/vllm-project/vllm/pull/25384)) by @JJJYmmm
* Add CUTLASS FP8 MOE benchmark scripts and kernel config ([#25302](https://github.com/vllm-project/vllm/pull/25302)) by @chenxi-yang
* feat: Enable engine-level arguments with speculators models ([#25250](https://github.com/vllm-project/vllm/pull/25250)) by @rahul-tuli
* Support mnnvl all2allv from Flashinfer ([#21003](https://github.com/vllm-project/vllm/pull/21003)) by @wenscarl

### üêõ Bug Fixes

* Fix routing_bias dtype  ([#25711](https://github.com/vllm-project/vllm/pull/25711)) by @wenscarl
* [Bugfix] Add triton.language.tensor placeholder ([#25649](https://github.com/vllm-project/vllm/pull/25649)) by @adobrzyn
* [Bugfix] Fix Qwen3-VL max_num_video_tokens calculation for video profiling ([#25648](https://github.com/vllm-project/vllm/pull/25648)) by @Isotr0py
* [Bugfix] Fix InternS1 video processing after Transformers v4.56 ([#25644](https://github.com/vllm-project/vllm/pull/25644)) by @Isotr0py
* [BugFix] Fix DBO hang ([#25625](https://github.com/vllm-project/vllm/pull/25625)) by @LucasWilkinson
* [Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super() ([#25613](https://github.com/vllm-project/vllm/pull/25613)) by @yewentao256
* [fix] Update torch version in cpu-build.txt for AArch64/ppc64le and Darwin ([#25579](https://github.com/vllm-project/vllm/pull/25579)) by @fadara01
* [Bug] fix import and unit test ([#25558](https://github.com/vllm-project/vllm/pull/25558)) by @jmkuebler
* [Bugfix] Fix dummy video number of frames calculation ([#25553](https://github.com/vllm-project/vllm/pull/25553)) by @ywang96
* [Bugfix][CPU] Skip unsupported custom op register on CPU ([#25534](https://github.com/vllm-project/vllm/pull/25534)) by @bigPYJ1151
* Fix triton_reshape_and_cache_flash.py triton import ([#25522](https://github.com/vllm-project/vllm/pull/25522)) by @mgoin
* [Bugfix] Use a separate FlashInfer workspace buffer for trtllm-gen ([#25520](https://github.com/vllm-project/vllm/pull/25520)) by @benchislett
* [Bug] Fix AttributeError: 'FusedMoE' object has no attribute 'w13_weight_scale'. Did you mean: 'w13_weight_scale_inv' ([#25519](https://github.com/vllm-project/vllm/pull/25519)) by @yewentao256
* [Bugfix] [Frontend] Cleanup gpt-oss non-streaming chat tool calls ([#25514](https://github.com/vllm-project/vllm/pull/25514)) by @bbrowning
* [Bugfix] [B200] cutlass_mla - ensure kv_split == 1 for batch size > 1 ([#25509](https://github.com/vllm-project/vllm/pull/25509)) by @alexm-redhat
* [Bugfix] Lower gpt-oss max cudagraph size to 992 to be compatible with FA3 ([#25508](https://github.com/vllm-project/vllm/pull/25508)) by @mgoin
* [BugFix] AssertionError: Do not capture num_reqs > max_num_reqs for uniform batch ([#25505](https://github.com/vllm-project/vllm/pull/25505)) by @LucasWilkinson
* [BugFix] Potential Fix for FA3 full-cudagraph IMA  ([#25490](https://github.com/vllm-project/vllm/pull/25490)) by @LucasWilkinson
* [Bugfix] gpt-oss container tool output bug ([#25485](https://github.com/vllm-project/vllm/pull/25485)) by @alecsolder
* [Bugfix] Fix for the import error from #24588 ([#25481](https://github.com/vllm-project/vllm/pull/25481)) by @gshtras
* [BugFix] Fix MLA assert with CUTLASS MLA ([#25478](https://github.com/vllm-project/vllm/pull/25478)) by @LucasWilkinson
* [BugFix] Register expert_map as named buffer for wake_up and sleep ([#25458](https://github.com/vllm-project/vllm/pull/25458)) by @wuxibin89
* [Bugfix] Fix idefics3 `tie_word_embeddings` ([#25454](https://github.com/vllm-project/vllm/pull/25454)) by @Isotr0py
* [Bugfix] fix custom op test ([#25429](https://github.com/vllm-project/vllm/pull/25429)) by @ProExpertProg
* [Bugfix] Remove contiguous output req for context parallel MLA ([#25414](https://github.com/vllm-project/vllm/pull/25414)) by @mgoin
* [BugFix] [DP/EP] Fix slow execution when BS <= DP ([#25407](https://github.com/vllm-project/vllm/pull/25407)) by @MatthewBonanni
* [Bugfix] Fix DeepSeekV31ToolParser to correctly parse multiple tools in non-streaming output ([#25405](https://github.com/vllm-project/vllm/pull/25405)) by @taohui
* [Bugfix] Fix missing `clear_connector_metadata` ([#25397](https://github.com/vllm-project/vllm/pull/25397)) by @NickLucche
* [BugFix] Fix OOM in vLLM replicas by ensuring consistent NCCL memory accounting ([#25359](https://github.com/vllm-project/vllm/pull/25359)) by @kouroshHakha
* [Bugfix] Typos in error message for missing model config file ([#25339](https://github.com/vllm-project/vllm/pull/25339)) by @simondanielsson
* [Bugfix][V0 Deprecation][CI] use async mock and await for async method ([#25325](https://github.com/vllm-project/vllm/pull/25325)) by @KKSK-DON
* [Bugfix] Fix Qwen3-VL-MoE weight loading for EP ([#25300](https://github.com/vllm-project/vllm/pull/25300)) by @ywang96
* [Bug] Fix Long Context OOM Issue ([#25290](https://github.com/vllm-project/vllm/pull/25290)) by @yewentao256
* [BugFix] Exclude self when checking for port collision ([#25286](https://github.com/vllm-project/vllm/pull/25286)) by @njhill
* [BugFix] Ensure appropriate guards in destructors ([#25284](https://github.com/vllm-project/vllm/pull/25284)) by @njhill
* [BugFix] Fix async scheduling CPU tensor race take 2 ([#25279](https://github.com/vllm-project/vllm/pull/25279)) by @njhill
* [BUGFIX] GPTQ quantization compatibility for Qwen3 Next MOE models (AutoGPTQ and AutoRound-GPTQ) ([#25268](https://github.com/vllm-project/vllm/pull/25268)) by @JartX
* [Bugfix] Fix chunked a2_scales in modular kernels ([#25264](https://github.com/vllm-project/vllm/pull/25264)) by @bnellnm
* [Bugfix][Perf] Misc fixes for Qwen3 VL ([#25238](https://github.com/vllm-project/vllm/pull/25238)) by @ywang96
* [Bugfix] GPT OSS Attritbute error on H100 ([#25228](https://github.com/vllm-project/vllm/pull/25228)) by @varun-sundar-rabindranath
* [Bugfix] fix tool call arguments is empty ([#25223](https://github.com/vllm-project/vllm/pull/25223)) by @chaunceyjiang
* [bugfix] fix structured outputs key missing issue from #24929 ([#25195](https://github.com/vllm-project/vllm/pull/25195)) by @luccafong
* [bugfix] fix MHA for models like OpenGVLab/InternVL3_5-38B ([#25146](https://github.com/vllm-project/vllm/pull/25146)) by @yma11
* [Bugfix] Parse SpeculativeConfig Error ([#25142](https://github.com/vllm-project/vllm/pull/25142)) by @yyzxw
* [Bugfix][CPU] Add placeholder to avoid import errors when using fused_moe ops on platforms without triton ([#25137](https://github.com/vllm-project/vllm/pull/25137)) by @bigPYJ1151
* [Bugfix] Remove VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE #2969 ([#25090](https://github.com/vllm-project/vllm/pull/25090)) by @Lucaskabela
* [BugFix] Make FlashInferMetadataBuilder non-blocking ([#25040](https://github.com/vllm-project/vllm/pull/25040)) by @nvjullin
* [BUG] Allows for RunAI Streamer and Torch.compile cache to be used together ([#24922](https://github.com/vllm-project/vllm/pull/24922)) by @ahao-anyscale
* [BugFix] Fix UB in per_token_group_quant.cu ([#24913](https://github.com/vllm-project/vllm/pull/24913)) by @rivos-shreeasish
* Fix: Correct FusedMoE layer reference in auto_round quantization ([#24818](https://github.com/vllm-project/vllm/pull/24818)) by @David-Wen2025
* [Bugfix] add cache model when from object storage get model ([#24764](https://github.com/vllm-project/vllm/pull/24764)) by @lengrongfu
* [Bugfix] fix apply_temperature to avoid nan in probs ([#24734](https://github.com/vllm-project/vllm/pull/24734)) by @courage17340
* [BUGFIX] Fix crash in Eagle Speculative Decoding models when exceedin‚Ä¶ ([#24662](https://github.com/vllm-project/vllm/pull/24662)) by @AlonKejzman
* [Bugfix] Fix several issues with p2p xPyD in GET type ([#23993](https://github.com/vllm-project/vllm/pull/23993)) by @Csrayz
* [fix]: add Arm 4bit fused moe support ([#23809](https://github.com/vllm-project/vllm/pull/23809)) by @nikhil-arm
* [Bugfix] Fix hermes tool parser handling of non-string argument types ([#22002](https://github.com/vllm-project/vllm/pull/22002)) by @david6666666

### ‚ö°Ô∏è Performance

* optimize: eliminate duplicate split_enc_dec_inputs calls ([#25573](https://github.com/vllm-project/vllm/pull/25573)) by @nicole-lihui
* [Perf] Increase default max splits for FA3 full cudagraphs ([#25495](https://github.com/vllm-project/vllm/pull/25495)) by @LucasWilkinson
* [Perf] Change default CUDAGraphMode from PIECEWISE to FULL_AND_PIECEWISE ([#25444](https://github.com/vllm-project/vllm/pull/25444)) by @mgoin
* [Perf] Fix jit compiles at runtime of fla gated delta rule ([#25432](https://github.com/vllm-project/vllm/pull/25432)) by @coreylowman
* [Perf] Further optimization for Qwen3-VL `fast_pos_embed_interpolate` ([#25347](https://github.com/vllm-project/vllm/pull/25347)) by @Isotr0py
* [Performance] Remove input pads in cutlass_mla and optimize v_proj output handling ([#25184](https://github.com/vllm-project/vllm/pull/25184)) by @alexm-redhat
* [Performance] Move apply_w8a8_block_fp8_linear to an op class ([#24666](https://github.com/vllm-project/vllm/pull/24666)) by @ElizaWszola
* [Perf] Apply torch.compile for `per_block_cast_to_fp8` ([#24611](https://github.com/vllm-project/vllm/pull/24611)) by @yewentao256
* [Perf] Optimize memory peak during EAGLE model loading. ([#24585](https://github.com/vllm-project/vllm/pull/24585)) by @candyzone
* Optimize triton unified attention performance for sliding window attention ([#24390](https://github.com/vllm-project/vllm/pull/24390)) by @zixi-qi
* [Perf] Use FlashInfer RoPE for RotaryEmbedding.forward_cuda when available ([#21126](https://github.com/vllm-project/vllm/pull/21126)) by @mgoin

### ü§ñ Model Support

* [Model] rename NemotronH_Nano_VL -> NemotronH_Nano_VL_V2 ([#25708](https://github.com/vllm-project/vllm/pull/25708)) by @tomeras91
* [Model] Define `merge_by_field_config` MM interface ([#25676](https://github.com/vllm-project/vllm/pull/25676)) by @DarkLight1337
* [Model] Add optional parameter to reasoning parser constructor ([#25554](https://github.com/vllm-project/vllm/pull/25554)) by @taohui
* [Model] Improve DotsOCRForCausalLM ([#25466](https://github.com/vllm-project/vllm/pull/25466)) by @jeejeelee
* [Model] Enable DP for ViT in Qwen2-VL ([#25445](https://github.com/vllm-project/vllm/pull/25445)) by @DarkLight1337
* [gpt-oss][bugfix] remove logic to require resp_ in ResponseAPI ([#25428](https://github.com/vllm-project/vllm/pull/25428)) by @qandrew
* [Model] Cleanup InternViT's data parallel implementation  ([#25306](https://github.com/vllm-project/vllm/pull/25306)) by @Isotr0py
* Llamas 3.1 405B fp4 changes upstreaming from 355_wip ([#25135](https://github.com/vllm-project/vllm/pull/25135)) by @maleksan85
* [gpt-oss] Add ResponseReasoningPartAddedEvent, ResponseReasoningPartDoneEvent for streaming ([#24938](https://github.com/vllm-project/vllm/pull/24938)) by @qandrew
* [Model] Support Dots OCR ([#24645](https://github.com/vllm-project/vllm/pull/24645)) by @ywang96
* [Model] Support SeedOss Reason Parser ([#24263](https://github.com/vllm-project/vllm/pull/24263)) by @LuYanFCP
* [Model] Add LongCat-Flash  ([#23991](https://github.com/vllm-project/vllm/pull/23991)) by @OftenDream

### üîå Hardware & Backend

* [XPU][Triton]add xpu config in triton_reshape_and_cache_flash ([#25643](https://github.com/vllm-project/vllm/pull/25643)) by @jikunshang
* [TPU][Bugfix] fix the missing apply_model in tpu worker ([#25526](https://github.com/vllm-project/vllm/pull/25526)) by @yaochengji
* [XPU] Fix MOE DP accuracy issue on XPU ([#25465](https://github.com/vllm-project/vllm/pull/25465)) by @faaany
* [XPU] Fix `compile_size` is `None` case. ([#25433](https://github.com/vllm-project/vllm/pull/25433)) by @jikunshang
* [ROCm][Build][Bugfix] Fix ROCm base docker whls installation order ([#25415](https://github.com/vllm-project/vllm/pull/25415)) by @gshtras
* [TPU] update torch_xla dependency for PyPI compatibility ([#25278](https://github.com/vllm-project/vllm/pull/25278)) by @jcyang43
* [ROCm][Bugfix] Only enable +rms_norm based on aiter if not explicitly disabled ([#25275](https://github.com/vllm-project/vllm/pull/25275)) by @gshtras
* [TPU][Bugfix][CI] Fix broken tests/build dependency ([#25255](https://github.com/vllm-project/vllm/pull/25255)) by @NickLucche
* [TPU] Deprecate `xm.mark_step` in favor of ``torch_xla.sync`  ([#25254](https://github.com/vllm-project/vllm/pull/25254)) by @NickLucche
* [ROCm] Small functional changes for gptoss ([#25201](https://github.com/vllm-project/vllm/pull/25201)) by @jpvillam-amd
* [ROCm] Add skinny gemm bias support for dtypes fp16,bf16,fp8 ([#24988](https://github.com/vllm-project/vllm/pull/24988)) by @amd-hhashemi

### ‚öôÔ∏è Refactoring & Core

* [Refactor] Remove DeepGEMM OP Register ([#25710](https://github.com/vllm-project/vllm/pull/25710)) by @yewentao256
* [Core] Force PIECEWISE CUDAGraph mode for encoder-decoder ([#25701](https://github.com/vllm-project/vllm/pull/25701)) by @russellb
* [Core] Enable command line logging for LLMEngine ([#25610](https://github.com/vllm-project/vllm/pull/25610)) by @zhuohan123
* [Refactor] Use DeepGEMM Col Major TMA Aligned Tensor ([#25517](https://github.com/vllm-project/vllm/pull/25517)) by @yewentao256
* Remove redundant mutates_args and dispatch_key for direct_register_custom_op ([#25512](https://github.com/vllm-project/vllm/pull/25512)) by @mgoin
* [Core] Ensure LoRA linear respect the base_layer's tp_size and tp_rank ([#25487](https://github.com/vllm-project/vllm/pull/25487)) by @jeejeelee
* Remove RFC review hours reference ([#25416](https://github.com/vllm-project/vllm/pull/25416)) by @simon-mo
* [Core] Drop overly aggressive whisper assertion ([#25408](https://github.com/vllm-project/vllm/pull/25408)) by @russellb
* [Core] Optimize LoRA weight loading ([#25403](https://github.com/vllm-project/vllm/pull/25403)) by @jeejeelee
* Remove V0 attention backends ([#25351](https://github.com/vllm-project/vllm/pull/25351)) by @WoosukKwon
* [Core] Enable sharded state loader for V1 engine and enhance test coverage ([#25308](https://github.com/vllm-project/vllm/pull/25308)) by @lirong-lirong
* [Core] Modify the initialization parameters of the lora manager ([#25249](https://github.com/vllm-project/vllm/pull/25249)) by @jeejeelee
* Remove Redundant Assignment in Qwen3_VisionPatchMerger ([#25224](https://github.com/vllm-project/vllm/pull/25224)) by @LJH-LBJ
* refactor(benchmarks): add type annotations to wait_for_endpoint parameters ([#25218](https://github.com/vllm-project/vllm/pull/25218)) by @samzong
* [Kernel] [Mamba] Remove BLOCK_H=1 from list of tuneable configurations for `_chunk_cumsum_fwd_kernel` ([#25197](https://github.com/vllm-project/vllm/pull/25197)) by @tdoublep
* refactor: abstract graph mode support into platform interface ([#25161](https://github.com/vllm-project/vllm/pull/25161)) by @yiz-liu
* [Kernel] Support DCP for Triton backend  ([#25132](https://github.com/vllm-project/vllm/pull/25132)) by @frank-wei
* [Kernel][Performance] Add Triton kernel for Qwen3-VL interleaved MRoPE ([#25055](https://github.com/vllm-project/vllm/pull/25055)) by @Isotr0py
* [Frontend] Add a new xml-based tool parser for qwen3-coder ([#25028](https://github.com/vllm-project/vllm/pull/25028)) by @Zhikaiiii
* [Core][Prefix Hash] Fix prefix hash metrics sliding window maintainance ([#24990](https://github.com/vllm-project/vllm/pull/24990)) by @Jialin
* [Frontend] Responses API messages out, just harmony for now ([#24985](https://github.com/vllm-project/vllm/pull/24985)) by @alecsolder
* [Core] Use KVCacheBlock as much as possible instead of dict[block_id, KVCacheBlock] ([#24830](https://github.com/vllm-project/vllm/pull/24830)) by @Jialin
* [Frontend] Responses API MCP tools for built in tools and to pass through headers ([#24628](https://github.com/vllm-project/vllm/pull/24628)) by @alecsolder
* [core] add nccl symmetric memory for all reduce ([#24532](https://github.com/vllm-project/vllm/pull/24532)) by @Amir-19
* [CORE] Prompt Embeddings Support for v1 Engine ([#24278](https://github.com/vllm-project/vllm/pull/24278)) by @qthequartermasterman
* [Frontend] Pass API server count to each process ([#23717](https://github.com/vllm-project/vllm/pull/23717)) by @DarkLight1337
* [Core] Support weight_loader_v2 for `UnquantizedLinearMethod` ([#23036](https://github.com/vllm-project/vllm/pull/23036)) by @kylesayrs

### üîß Build, CI & Testing

* [CI] Fix Pre-commit Issue ([#25497](https://github.com/vllm-project/vllm/pull/25497)) by @yewentao256
* [Build] Update Xgrammar to 0.1.25 ([#25467](https://github.com/vllm-project/vllm/pull/25467)) by @chaunceyjiang
* [CI] Skip tests failing on main ([#25326](https://github.com/vllm-project/vllm/pull/25326)) by @WoosukKwon
* [Test]: Hermes tool parser stream output error in Qwen3 case ([#25203](https://github.com/vllm-project/vllm/pull/25203)) by @ahartel
* [Build] Update Xgrammar to 0.1.24 to get a CVE fix ([#25188](https://github.com/vllm-project/vllm/pull/25188)) by @russellb

### üìö Documentation

* [Docs] Enable `fail_on_warning` for the docs build in CI ([#25580](https://github.com/vllm-project/vllm/pull/25580)) by @hmellor
* [docs] fix nixl kv_connector_extra_config.backends key ([#25565](https://github.com/vllm-project/vllm/pull/25565)) by @panpan0000
* [docs] Benchmark Serving Incorrect Arg ([#25474](https://github.com/vllm-project/vllm/pull/25474)) by @vllmellm
* [Docs] Fix griffe warnings in vllm/lora/ops ([#25369](https://github.com/vllm-project/vllm/pull/25369)) by @windsonsea
* [Docs] GSM8K Accuracy Evaluation doc update ([#25360](https://github.com/vllm-project/vllm/pull/25360)) by @david6666666
* [Doc] improve test-pipeline.yaml documentation ([#25305](https://github.com/vllm-project/vllm/pull/25305)) by @hl475
* [docs] Prompt Embedding feature support ([#25288](https://github.com/vllm-project/vllm/pull/25288)) by @qthequartermasterman
* [Docs] Fix warnings in vllm/profiler and vllm/transformers_utils ([#25220](https://github.com/vllm-project/vllm/pull/25220)) by @windsonsea
* [Docs] Fix griffe warnings in vllm/multimodal ([#25216](https://github.com/vllm-project/vllm/pull/25216)) by @windsonsea
* [Docs] Fix warnings in mkdocs build (continued)  ([#25042](https://github.com/vllm-project/vllm/pull/25042)) by @wwl2755
* [Docs] add __init__.py to vllm/model_executor/layers/quantization/compressed_tensors/transform ([#24974](https://github.com/vllm-project/vllm/pull/24974)) by @samzong

### üì¶ Miscellaneous

* [Misc] Don't log shm dequeue delay warning on worker side ([#25720](https://github.com/vllm-project/vllm/pull/25720)) by @njhill
* [Optimization] Streamline `InputPreprocessor` ([#25702](https://github.com/vllm-project/vllm/pull/25702)) by @DarkLight1337
* [Misc] Simplify `test_argsort_mm_positions` ([#25690](https://github.com/vllm-project/vllm/pull/25690)) by @DarkLight1337
* [V0 deprecation] Clean up LoRA  ([#25686](https://github.com/vllm-project/vllm/pull/25686)) by @jeejeelee
* [Optimization] Use a cheaper cache key in `get_model_architecture` ([#25682](https://github.com/vllm-project/vllm/pull/25682)) by @DarkLight1337
* Revert "[Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super()" ([#25681](https://github.com/vllm-project/vllm/pull/25681)) by @mgoin
* [Misc] Remove cruft file in repo ([#25678](https://github.com/vllm-project/vllm/pull/25678)) by @NickLucche
* [V0 deprecation] Clean up V0 fallback in compilation config ([#25675](https://github.com/vllm-project/vllm/pull/25675)) by @Isotr0py
* [CI/Build] Fix flaky entrypoints test ([#25663](https://github.com/vllm-project/vllm/pull/25663)) by @DarkLight1337
* [mypy] Fix wrong type annotations related to tuple ([#25660](https://github.com/vllm-project/vllm/pull/25660)) by @DarkLight1337
* [mypy] Further improve MM type annotations ([#25654](https://github.com/vllm-project/vllm/pull/25654)) by @DarkLight1337
* [CPU] update torch 2.8 and fix missing fields in TorchSDPAMetadata ([#25652](https://github.com/vllm-project/vllm/pull/25652)) by @bigPYJ1151
* [Misc] Fix Qwen3-VL `video_grid_thw` typing ([#25646](https://github.com/vllm-project/vllm/pull/25646)) by @ywang96
* [V0 deprecation] Remove unreachable model_config.supported_tasks ([#25642](https://github.com/vllm-project/vllm/pull/25642)) by @noooop
* typo: remove duplicate `is` ([#25641](https://github.com/vllm-project/vllm/pull/25641)) by @nicole-lihui
* [Misc] Simplify PoolerOutput and move to `v1/outputs` ([#25629](https://github.com/vllm-project/vllm/pull/25629)) by @DarkLight1337
* [misc] warning by default for hanging / busy / idle ([#25627](https://github.com/vllm-project/vllm/pull/25627)) by @youkaichao
* Map CwmForCausalLM to llama and LlamaForCausalLM ([#25611](https://github.com/vllm-project/vllm/pull/25611)) by @jacobkahn
* Enable Fbgemm NVFP4 on Dense models ([#25609](https://github.com/vllm-project/vllm/pull/25609)) by @samanamp
* Revert "[Performance] Move apply_w8a8_block_fp8_linear to an op class‚Ä¶ ([#25607](https://github.com/vllm-project/vllm/pull/25607)) by @tlrmchlsmth
* [MISC] replace c10::optional with std::optional ([#25602](https://github.com/vllm-project/vllm/pull/25602)) by @842974287
* Suppress benign cuBLAS warning when capturing cudagraphs with DBO ([#25596](https://github.com/vllm-project/vllm/pull/25596)) by @SageMoore
* Fixes and updates to bench_per_token_quant_fp8 ([#25591](https://github.com/vllm-project/vllm/pull/25591)) by @mgoin
* [Misc] Improve type annotations for jsontree ([#25577](https://github.com/vllm-project/vllm/pull/25577)) by @DarkLight1337
* [misc] update the warning message ([#25566](https://github.com/vllm-project/vllm/pull/25566)) by @youkaichao
* Move `DeviceConfig`, `ObservabilityConfig`, `SpeechToTextConfig` to their own files ([#25564](https://github.com/vllm-project/vllm/pull/25564)) by @hmellor
* [Misc]] Move processing context to multimodal directory ([#25548](https://github.com/vllm-project/vllm/pull/25548)) by @DarkLight1337
* [CI/Build] Fix v1 OOT registration test ([#25547](https://github.com/vllm-project/vllm/pull/25547)) by @Isotr0py
* [V0 Deprecation] Remove max_seq_len_to_capture ([#25543](https://github.com/vllm-project/vllm/pull/25543)) by @WoosukKwon
* [V0 Deprecation] Remove unused classes in attention ([#25541](https://github.com/vllm-project/vllm/pull/25541)) by @WoosukKwon
* [Misc] Retry HF processing if "Already borrowed" error occurs ([#25535](https://github.com/vllm-project/vllm/pull/25535)) by @DarkLight1337
* [Logging] Remove TORCH_NCCL_AVOID_RECORD_STREAMS to squash a warning ([#25532](https://github.com/vllm-project/vllm/pull/25532)) by @tlrmchlsmth
* [Logging] Improve log for when DeepEP HT disables CUDA Graphs ([#25531](https://github.com/vllm-project/vllm/pull/25531)) by @tlrmchlsmth
* [Compile] Fix AMD Compile Error ([#25518](https://github.com/vllm-project/vllm/pull/25518)) by @yewentao256
* [V0 Deprecation] Remove placeholder attn ([#25510](https://github.com/vllm-project/vllm/pull/25510)) by @tdoublep
* [Benchmark] Fix regression in structured output benchmark ([#25500](https://github.com/vllm-project/vllm/pull/25500)) by @russellb
* [CI/Build] Fix and re-enable v1 PP test on CI ([#25496](https://github.com/vllm-project/vllm/pull/25496)) by @Isotr0py
* [V0 deprecation] Remove _VLLM_V1 suffixes from attention backend names ([#25489](https://github.com/vllm-project/vllm/pull/25489)) by @MatthewBonanni
* Improve output when failing json.loads() on structured output test ([#25483](https://github.com/vllm-project/vllm/pull/25483)) by @dougbtv
* [UX] Change kv-cache-memory log level to debug ([#25479](https://github.com/vllm-project/vllm/pull/25479)) by @mgoin
* [CI/Build] Fix disabled v1 attention backend selection test ([#25471](https://github.com/vllm-project/vllm/pull/25471)) by @Isotr0py
* [Misc] Move DP for ViT code inside model executor dir ([#25459](https://github.com/vllm-project/vllm/pull/25459)) by @DarkLight1337
* [benchmarks]allow skip ready check for bench serve ([#25420](https://github.com/vllm-project/vllm/pull/25420)) by @luccafong
* [V0 deprecation] Remove platform v1 controling interface ([#25410](https://github.com/vllm-project/vllm/pull/25410)) by @Isotr0py
* [V0 deprecation] Remove `_set_default_args_v0` function ([#25409](https://github.com/vllm-project/vllm/pull/25409)) by @Isotr0py
* [Speculators][Speculative Decoding] Fix gpt-oss eagle3 accuracy issue ([#25406](https://github.com/vllm-project/vllm/pull/25406)) by @jiahanc
* [V1] Remove V0 code paths for Hybrid models ([#25400](https://github.com/vllm-project/vllm/pull/25400)) by @tdoublep
* [CI Failure] Fix fp8 kv cache on <SM90 ([#25396](https://github.com/vllm-project/vllm/pull/25396)) by @mgoin
* [CI/Build] Skip Qwen3-VL initialization tests until models are actually released ([#25394](https://github.com/vllm-project/vllm/pull/25394)) by @DarkLight1337
* [Compiler] Disable Inductor standalone compile by default ([#25391](https://github.com/vllm-project/vllm/pull/25391)) by @ElizaWszola
* Make pickle import check fast ([#25379](https://github.com/vllm-project/vllm/pull/25379)) by @hmellor
* [Misc] Remove unused encoder-decoder error strings ([#25374](https://github.com/vllm-project/vllm/pull/25374)) by @DarkLight1337
* [V0 Deprecation] Remove `MultiModalPlaceholderMap` ([#25366](https://github.com/vllm-project/vllm/pull/25366)) by @DarkLight1337
* [V0 Deprecation] Remove V0-only methods in multi-modal registry ([#25362](https://github.com/vllm-project/vllm/pull/25362)) by @DarkLight1337
* Use macro guard CUDA functions for back compatibility in grouped_topk_kernel.cu ([#25346](https://github.com/vllm-project/vllm/pull/25346)) by @minosfuture
* [V0 Deprecation] Remove V0 sampling metadata ([#25345](https://github.com/vllm-project/vllm/pull/25345)) by @WoosukKwon
* [Optimization] Cache chat template result when processor fails to be loaded ([#25341](https://github.com/vllm-project/vllm/pull/25341)) by @DarkLight1337
* [MM][Perf] Minor Optimization on Qwen3-VL `fast_pos_embed_interpolate` ([#25337](https://github.com/vllm-project/vllm/pull/25337)) by @ywang96
* [V0 Deprecation] Remove async_output_proc, preemption mode, delay factor ([#25334](https://github.com/vllm-project/vllm/pull/25334)) by @WoosukKwon
* [V0 Deprecation] Remove V0 Sequence class & Sampler ([#25332](https://github.com/vllm-project/vllm/pull/25332)) by @WoosukKwon
* [V0 Deprecation] Remove from_seq_group methods ([#25330](https://github.com/vllm-project/vllm/pull/25330)) by @WoosukKwon
* [V0 Deprecation] Remove V0 MP executor ([#25329](https://github.com/vllm-project/vllm/pull/25329)) by @WoosukKwon
* [V0 Deprecation] Remove V0 model runner base & simplify worker base ([#25328](https://github.com/vllm-project/vllm/pull/25328)) by @WoosukKwon
* [Chore] Remove unused sampler in models ([#25324](https://github.com/vllm-project/vllm/pull/25324)) by @WoosukKwon
* [V0 Deprecation] Remove V0 core ([#25321](https://github.com/vllm-project/vllm/pull/25321)) by @WoosukKwon
* [V0 Deprecation] Remove V0 Output Processor ([#25320](https://github.com/vllm-project/vllm/pull/25320)) by @WoosukKwon
* Handle triton kernel import exception ([#25319](https://github.com/vllm-project/vllm/pull/25319)) by @minosfuture
* Make `mypy` behave like a proper pre-commit hook ([#25313](https://github.com/vllm-project/vllm/pull/25313)) by @hmellor
* [V0 Deprecation] Enable the remaining multimodal tests in V1 ([#25307](https://github.com/vllm-project/vllm/pull/25307)) by @DarkLight1337
* [CI Failure] Disable FlashInfer RoPE to unblock CI ([#25299](https://github.com/vllm-project/vllm/pull/25299)) by @mgoin
* [BUG FIX][NON-CUDA]quick fix to avoid call cudagraph_unsafe in attention ([#25298](https://github.com/vllm-project/vllm/pull/25298)) by @xuechendi
* [Misc] Support more collective_rpc return types ([#25294](https://github.com/vllm-project/vllm/pull/25294)) by @njhill
* test: Remove vestigial skip for prompt embeds tests after landing v1 Prompt Embeds support ([#25291](https://github.com/vllm-project/vllm/pull/25291)) by @qthequartermasterman
* Improve weight loading for encoder models in Transformers backend ([#25289](https://github.com/vllm-project/vllm/pull/25289)) by @hmellor
* Multimodal - audio tests ([#25285](https://github.com/vllm-project/vllm/pull/25285)) by @debroy-rh
* Don't skip special tokens with hermes-style tool calling ([#25281](https://github.com/vllm-project/vllm/pull/25281)) by @maxdebayser
* allow disable flashinfer prefill ([#25276](https://github.com/vllm-project/vllm/pull/25276)) by @luccafong
* [CLI env var] Add VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH in env variables ([#25274](https://github.com/vllm-project/vllm/pull/25274)) by @Daisy-Ma-coder
* Specify platform in `pip-compile` `pre-commit` hook so it runs on MacOS ([#25273](https://github.com/vllm-project/vllm/pull/25273)) by @hmellor
* Update CODEOWNERS ([#25269](https://github.com/vllm-project/vllm/pull/25269)) by @hmellor
* [Optimization] Avoid repeated model architecture conversion for pooling models ([#25261](https://github.com/vllm-project/vllm/pull/25261)) by @DarkLight1337
* Move `ModelConfig` from `config/__init__.py` to `config/model.py` ([#25252](https://github.com/vllm-project/vllm/pull/25252)) by @hmellor
* Enable Eagle3 speculative decoding for GPT-OSS model ([#25246](https://github.com/vllm-project/vllm/pull/25246)) by @eldarkurtic
* [Qwen] Remove cuda hard-code in qwen3 next ([#25243](https://github.com/vllm-project/vllm/pull/25243)) by @wxsIcey
* [V0 Deprecation] Remove V0 logic from `get_input_embeddings` interface ([#25242](https://github.com/vllm-project/vllm/pull/25242)) by @DarkLight1337
* [Misc] Cleanup test conftest for deprecated encoder-decoder models ([#25231](https://github.com/vllm-project/vllm/pull/25231)) by @Isotr0py
* [Misc] Clean up MM profiling warnings ([#25222](https://github.com/vllm-project/vllm/pull/25222)) by @ywang96
* [Kernels] Support blocked fp8 quantization for compressed tensors MoE ([#25219](https://github.com/vllm-project/vllm/pull/25219)) by @bnellnm
* [CI/Build] add nightly prime-rl integration tests ([#25207](https://github.com/vllm-project/vllm/pull/25207)) by @Jackmin801
* [Log] Optimize kv cache memory log from Bytes to GiB ([#25204](https://github.com/vllm-project/vllm/pull/25204)) by @yewentao256
* [Spec Decode] Enable FlashInfer Spec Decoding ([#25196](https://github.com/vllm-project/vllm/pull/25196)) by @benchislett
* [Compile] Fix Compile Warning for Ignoring `MIN_BLOCK_PER_SM` ([#25193](https://github.com/vllm-project/vllm/pull/25193)) by @yewentao256
* Move `PoolerConfig` from `config/__init__.py` to `config/pooler.py` ([#25181](https://github.com/vllm-project/vllm/pull/25181)) by @hmellor
* Encoder model support for the Transformers backend ([#25174](https://github.com/vllm-project/vllm/pull/25174)) by @hmellor
* [CPU] Disable oneDNN linear on non-x86 platforms ([#25166](https://github.com/vllm-project/vllm/pull/25166)) by @bigPYJ1151
* [OOT] Support sync_model_loading for OOT ([#25126](https://github.com/vllm-project/vllm/pull/25126)) by @xuechendi
* [NIXL][OOT platform] support nixl_connector with oot platform and other nixl_backend ([#25121](https://github.com/vllm-project/vllm/pull/25121)) by @xuechendi
* [Hybrid Allocator] Support full attention with different hidden size  ([#25101](https://github.com/vllm-project/vllm/pull/25101)) by @heheda12345
* [CI/Build] fix test function_calling ([#25072](https://github.com/vllm-project/vllm/pull/25072)) by @chaunceyjiang
* Enable symmetric memory all reduce by default only enabling for TP ([#25070](https://github.com/vllm-project/vllm/pull/25070)) by @ilmarkov
* [V0 Deprecation] Remove LLMEngine ([#25033](https://github.com/vllm-project/vllm/pull/25033)) by @WoosukKwon
* [Multi Modal][Performance] Fused Q,K's apply_rope in more models ([#25005](https://github.com/vllm-project/vllm/pull/25005)) by @wwl2755
* [Spec Decode] Add Batch Parallel Ngram. Upto 8x lower overhead. ([#24986](https://github.com/vllm-project/vllm/pull/24986)) by @ekagra-ranjan
* [torch.compile] Make Query Quantization Fusable ([#24914](https://github.com/vllm-project/vllm/pull/24914)) by @jmkuebler
* Improve `--help` for enhanced user experience ([#24903](https://github.com/vllm-project/vllm/pull/24903)) by @hmellor
* [DP] support torchrun external launcher with Data Parallelism ([#24899](https://github.com/vllm-project/vllm/pull/24899)) by @luccafong
* [Core/DBO][2/N] Dual-Batch Overlap add DeepEP High Throughput support and Prefill support ([#24845](https://github.com/vllm-project/vllm/pull/24845)) by @LucasWilkinson
* [V1][Attention] Split triton_attn in triton-only and rocm specific backends  ([#24648](https://github.com/vllm-project/vllm/pull/24648)) by @bringlein
* [DP/EP][GPTOSS] Use triton matmul-ogs kernels for GPTOSS DP/EP ([#24588](https://github.com/vllm-project/vllm/pull/24588)) by @varun-sundar-rabindranath
* [EPLB] Reduce EPLB Inference Overhead ([#24573](https://github.com/vllm-project/vllm/pull/24573)) by @abmfy
* [torch.compile] Cleanup compilation tests and custom passes, add debug utils, fix DCE bug (#23091), fix test (#24376), and prep for custom op matching (#24604) ([#24542](https://github.com/vllm-project/vllm/pull/24542)) by @ProExpertProg
* [Spec Decode][CI] Add e2e test for `examples/spec_decode.py` and prevent breaking Acceptance Length ([#24531](https://github.com/vllm-project/vllm/pull/24531)) by @ekagra-ranjan
* [V1][Kernel] Add triton implementation for `reshape_and_cache_flash` ([#24503](https://github.com/vllm-project/vllm/pull/24503)) by @bringlein
* [P/D] Support NIXL connector to disconnect during a clean shutdown ([#24423](https://github.com/vllm-project/vllm/pull/24423)) by @chaunceyjiang
* [KV sharing] Re-land Gemma3n model changes from #22628 ([#24357](https://github.com/vllm-project/vllm/pull/24357)) by @sarckk
* [torch.compile] CUDAGraph Inductor partition integration ([#24281](https://github.com/vllm-project/vllm/pull/24281)) by @BoyuanFeng
* [KV offload][5/N] Add `CPUOffloadingSpec` ([#24251](https://github.com/vllm-project/vllm/pull/24251)) by @orozery
* [test/doc] make NixlConnector example more clear ([#24249](https://github.com/vllm-project/vllm/pull/24249)) by @panpan0000
* [V1] Add sliding window support to Flex Attention backend ([#24089](https://github.com/vllm-project/vllm/pull/24089)) by @Isotr0py
* [V1][Metrics] Add per-request TPOT histogram ([#24015](https://github.com/vllm-project/vllm/pull/24015)) by @baxingpiaochong
* [ux] Switch a warning to debug about a pytorch fallback ([#23750](https://github.com/vllm-project/vllm/pull/23750)) by @russellb
* [Misc] Reduce initialization time of auto_tune ([#23682](https://github.com/vllm-project/vllm/pull/23682)) by @wdhongtw
*  Generate _ModelInfo properties file when loading to improve loading speed ([#23558](https://github.com/vllm-project/vllm/pull/23558)) by @manoelmarques
* MI-300X triton moe configs ([#23445](https://github.com/vllm-project/vllm/pull/23445)) by @Sara-KS
* Enable modelopt gemma3 nvfp4/fp8, make workflow more robust ([#22771](https://github.com/vllm-project/vllm/pull/22771)) by @Edwardf0t1
* [KV offload][4/N] Offloading KV connector ([#22595](https://github.com/vllm-project/vllm/pull/22595)) by @orozery
* [P/D][Nixl] Introduce `KVTransferMetrics` and aggregation strategy ([#22188](https://github.com/vllm-project/vllm/pull/22188)) by @NickLucche
* [Hardware][RISC-V] Add riscv64 support for vLLM with scalar ([#22112](https://github.com/vllm-project/vllm/pull/22112)) by @langc23
* [KV offload][3/N] Add worker-side CPU support ([#21448](https://github.com/vllm-project/vllm/pull/21448)) by @orozery
* [KV offload][2/N] Introduce LRU-based CPU offloading management ([#20075](https://github.com/vllm-project/vllm/pull/20075)) by @orozery
* [V1] Support `LLM.apply_model` ([#18465](https://github.com/vllm-project/vllm/pull/18465)) by @DarkLight1337

## Contributors

@842974287, @AlonKejzman, @Amir-19, @BoyuanFeng, @Csrayz, @Daisy-Ma-coder, @DarkLight1337, @David-Wen2025, @Edwardf0t1, @ElizaWszola, @Isotr0py, @JJJYmmm, @Jackmin801, @JartX, @Jialin, @KKSK-DON, @LJH-LBJ, @LuYanFCP, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @NickLucche, @OftenDream, @ProExpertProg, @SageMoore, @Sara-KS, @WoosukKwon, @Zhikaiiii, @abmfy, @adobrzyn, @ahao-anyscale, @ahartel, @alecsolder, @alexm-redhat, @amd-hhashemi, @baxingpiaochong, @bbrowning, @benchislett, @bigPYJ1151, @bnellnm, @bringlein, @candyzone, @chaunceyjiang, @chenxi-yang, @coreylowman, @courage17340, @david6666666, @debroy-rh, @djmmoss, @dougbtv, @ekagra-ranjan, @eldarkurtic, @faaany, @fadara01, @frank-wei, @gshtras, @heheda12345, @hl475, @hmellor, @ilmarkov, @jacobkahn, @jcyang43, @jeejeelee, @jiahanc, @jikunshang, @jmkuebler, @jpvillam-amd, @kouroshHakha, @kylesayrs, @langc23, @lengrongfu, @lirong-lirong, @luccafong, @maleksan85, @manoelmarques, @maxdebayser, @mgoin, @minosfuture, @nicole-lihui, @nikhil-arm, @njhill, @noooop, @nvjullin, @orozery, @panpan0000, @qandrew, @qthequartermasterman, @rahul-tuli, @rivos-shreeasish, @rouchenzi, @russellb, @samanamp, @samzong, @sarckk, @simon-mo, @simondanielsson, @taohui, @tdoublep, @tlrmchlsmth, @tomeras91, @varun-sundar-rabindranath, @vllmellm, @wdhongtw, @wenscarl, @windsonsea, @wuxibin89, @wwl2755, @wxsIcey, @xuechendi, @yaochengji, @yewentao256, @yiz-liu, @yma11, @youkaichao, @ywang96, @yyzxw, @zhuohan123, @zixi-qi

