# Weekly Release Notes for vllm-project/vllm (2026-01-02)

## What's Changed

### ‚ú® Features & Enhancements

* Add get_expert_mapping to NemotronHModel (for LoRA support) ([#31539](https://github.com/vllm-project/vllm/pull/31539)) by @danisereb
* Add docker buildx bake configuration ([#31477](https://github.com/vllm-project/vllm/pull/31477)) by @amrmahdi
* Add GLM-ASR multimodal support  ([#31436](https://github.com/vllm-project/vllm/pull/31436)) by @baonudesifeizhai
* Add Loraconfig parameter to  get_punica_wrapper function ([#31408](https://github.com/vllm-project/vllm/pull/31408)) by @ZT-AIA
* Add Fused MoE Triton kernels for GLM-4.5-Air, GLM-4.5v, GLM-4.6v on 2x RTX Pro 6000 ([#31407](https://github.com/vllm-project/vllm/pull/31407)) by @mratsim
* add tip for VLLM_USE_PRECOMPILED arg to reduce docker build time ([#31385](https://github.com/vllm-project/vllm/pull/31385)) by @yitingdc
* [Feature] Add offline FastAPI documentation support for air-gapped environments ([#30184](https://github.com/vllm-project/vllm/pull/30184)) by @rickychen-infinirc
* Add Multimodal Processor Benchmark  ([#29105](https://github.com/vllm-project/vllm/pull/29105)) by @reaganjlee

### üêõ Bug Fixes

* [Bugfix] Replace BaseException with specific exceptions in FLA utils ([#31590](https://github.com/vllm-project/vllm/pull/31590)) by @c0de128
* [BugFix] Fix async scheduling for pooling models ([#31584](https://github.com/vllm-project/vllm/pull/31584)) by @njhill
* [Bugfix] Fix BAGEL online serving for text and image understanding ([#31546](https://github.com/vllm-project/vllm/pull/31546)) by @Dylan1229
* [Fix] Align fused moe lora_b shape with peft ([#31534](https://github.com/vllm-project/vllm/pull/31534)) by @linitra24
* [BugFix] Fix NUMA node validation in CPU platform ([#31520](https://github.com/vllm-project/vllm/pull/31520)) by @SameerAsal
* [Bugfix]Fix pooling model always disabled due to incorrect PP rank check ([#31505](https://github.com/vllm-project/vllm/pull/31505)) by @vintipandey
* [Bugfix][ROCm] Fix Static Quant Issue ([#31502](https://github.com/vllm-project/vllm/pull/31502)) by @robertgshaw2-redhat
* [BugFix]  add select_gemm_impl on CompressedTensorsWNA16MoEMethod to support LoRA ([#31453](https://github.com/vllm-project/vllm/pull/31453)) by @JartX
* [Bugfix][Frontend] Fix Jina reranker multimodal input compatibility ([#31445](https://github.com/vllm-project/vllm/pull/31445)) by @twjww
* [Bugfix] Preserve tool call id/type/name in streaming finish chunk ([#31438](https://github.com/vllm-project/vllm/pull/31438)) by @amittell
* [BugFix] register quant scale tensors as buffer ([#31395](https://github.com/vllm-project/vllm/pull/31395)) by @BoyuanFeng
* [Bug] Fix log issue with `\n` ([#31390](https://github.com/vllm-project/vllm/pull/31390)) by @yewentao256
* [BugFix] Fix cache issue in compilation_config ([#31376](https://github.com/vllm-project/vllm/pull/31376)) by @BoyuanFeng
* [BugFix] Re-fix async multimodal cpu tensor race condition ([#31373](https://github.com/vllm-project/vllm/pull/31373)) by @njhill
* [Bugfix] Support LoRA and GPTQModel for PLaMo 2/3  ([#31322](https://github.com/vllm-project/vllm/pull/31322)) by @Alnusjaponica
* fix: update kimi k2 tool parser logic ([#31207](https://github.com/vllm-project/vllm/pull/31207)) by @wangln19

### ‚ö°Ô∏è Performance

* Optimize QKNorm for MiniMax-M2/M2.1 ([#31493](https://github.com/vllm-project/vllm/pull/31493)) by @rogeryoungh

### ü§ñ Model Support

* [Model] Add tuned triton fused_moe configs for Qwen3Moe on B200 ([#31448](https://github.com/vllm-project/vllm/pull/31448)) by @Jzz1943
* [Model] Add support for openPangu moe model ([#28775](https://github.com/vllm-project/vllm/pull/28775)) by @yt0428

### üîå Hardware & Backend

* [ROCm][CI] Update MiniCPM model test: MiniCPM3-4B to MiniCPM4.1-8B and simplify attention backend testing ([#31551](https://github.com/vllm-project/vllm/pull/31551)) by @AndreasKaratzas
* [ROCm][Bugfix] Fix accuracy issue on fmoe when `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` enabled ([#31523](https://github.com/vllm-project/vllm/pull/31523)) by @ganyi1996ppo
* [xpu] [bugfix] upgrade to latest oneccl in dockerfile ([#31522](https://github.com/vllm-project/vllm/pull/31522)) by @rogerxfeng8
* [ROCm][CI] Skip DeepGemm-dependent test on ROCm platform ([#31462](https://github.com/vllm-project/vllm/pull/31462)) by @AndreasKaratzas
* [ROCm][CI] Added perceptron lib in requirements for isaac multi-modal test ([#31441](https://github.com/vllm-project/vllm/pull/31441)) by @AndreasKaratzas
* [XPU][CI]skip test_preprocess_error_handling due to fork/spawn issue ([#31381](https://github.com/vllm-project/vllm/pull/31381)) by @jikunshang
* [ROCm] Migrate xgrammar to upstream release ([#31327](https://github.com/vllm-project/vllm/pull/31327)) by @AndreasKaratzas
* [ROCm][CI] Add TorchCodec source build for transcription tests ([#31323](https://github.com/vllm-project/vllm/pull/31323)) by @AndreasKaratzas
* [ROCm][GPTQ][Bugfix] Fix GPTQ GEMM kernel output zeroing race condition ([#30719](https://github.com/vllm-project/vllm/pull/30719)) by @AndreasKaratzas

### ‚öôÔ∏è Refactoring & Core

* [Core] Remove unused `num_tokens` parameter from `_init_model_kwargs` ([#31517](https://github.com/vllm-project/vllm/pull/31517)) by @maang-h
* [Core] Deduplicate generate/encode logic in `AsyncLLM` ([#31510](https://github.com/vllm-project/vllm/pull/31510)) by @njhill
* [Frontend] add continue_final_message parameter to /embeddings endpoint ([#31497](https://github.com/vllm-project/vllm/pull/31497)) by @kevin-pw
* [Core][Hybrid allocator + connector] Support hybrid allocator + kv cache connector ([#30166](https://github.com/vllm-project/vllm/pull/30166)) by @ivanium
* [Core] Enable async scheduling by default ([#27614](https://github.com/vllm-project/vllm/pull/27614)) by @njhill
* [Core] Initialize LoRA support for tower and connector in multi-modal models ([#26674](https://github.com/vllm-project/vllm/pull/26674)) by @jeejeelee

### üîß Build, CI & Testing

* [CI][Bugfix] Fix token counting in chunked prefill streaming test ([#31565](https://github.com/vllm-project/vllm/pull/31565)) by @AndreasKaratzas
* [CI] [Critical] [CUDA] Fix duplicated test name ([#31562](https://github.com/vllm-project/vllm/pull/31562)) by @tjtanaa
* [CI][NIXL] Split DPEP tests ([#31491](https://github.com/vllm-project/vllm/pull/31491)) by @NickLucche
* [CI] fix test_chat_truncation_content_not_null test ([#31488](https://github.com/vllm-project/vllm/pull/31488)) by @chaunceyjiang
* [CI]Test Group 'NixlConnector PD accuracy tests' is fixed ([#31460](https://github.com/vllm-project/vllm/pull/31460)) by @qli88
* [CI] Fix flaky vision beam search test with flexible semantic validation ([#31324](https://github.com/vllm-project/vllm/pull/31324)) by @AndreasKaratzas

### üìö Documentation

* [Docs] Use relative `md` links instead of absolute `html` links for cross referencing ([#31494](https://github.com/vllm-project/vllm/pull/31494)) by @hmellor
* [Docs] Fix some snippets ([#31378](https://github.com/vllm-project/vllm/pull/31378)) by @hmellor
* [Docs] Add profiler user docs for http request ([#31370](https://github.com/vllm-project/vllm/pull/31370)) by @lengrongfu

### üì¶ Miscellaneous

* [CPU] Disable async schedule on CPU ([#31525](https://github.com/vllm-project/vllm/pull/31525)) by @bigPYJ1151
* [Minor] Various small code cleanups/simplifications ([#31508](https://github.com/vllm-project/vllm/pull/31508)) by @njhill
* Migrate meetups & sponsors [2/N] ([#31500](https://github.com/vllm-project/vllm/pull/31500)) by @esmeetu
* [MoE Refactor][12/N] Marlin Fp8 MoE Pure Function ([#31499](https://github.com/vllm-project/vllm/pull/31499)) by @robertgshaw2-redhat
* Replace `nn.ConvNd` with vLLM's `ConvNdLayer` for Transformers modeling backend ([#31498](https://github.com/vllm-project/vllm/pull/31498)) by @hmellor
* Migrate doc to website: Hardware Plugins (1/N) ([#31496](https://github.com/vllm-project/vllm/pull/31496)) by @esmeetu
* [CI/Build][CPU] Update CPU CI test cases ([#31466](https://github.com/vllm-project/vllm/pull/31466)) by @bigPYJ1151
* [Chore]: Remove HF format Phi4-MM examples ([#31405](https://github.com/vllm-project/vllm/pull/31405)) by @Isotr0py
* [CI/Build] Ignore max transformers version for more common tests ([#31401](https://github.com/vllm-project/vllm/pull/31401)) by @Isotr0py
* implements register kv caches in lmcache connector ([#31397](https://github.com/vllm-project/vllm/pull/31397)) by @chunxiaozheng
* feat(frontend): add --default-chat-template-kwargs CLI argument ([#31343](https://github.com/vllm-project/vllm/pull/31343)) by @effortprogrammer
* [Misc] Fix Qwen2-MoE shared_expert_gate ([#31339](https://github.com/vllm-project/vllm/pull/31339)) by @jeejeelee
* [benchmark] use model card root instead of id ([#31329](https://github.com/vllm-project/vllm/pull/31329)) by @andyxning
* CustomOp: Unify aiter impl into GroupedTopk ([#31221](https://github.com/vllm-project/vllm/pull/31221)) by @xinyu-intel
* [CI/ROCm] Fixing "V1 Test attention (H100)" test group. ([#31187](https://github.com/vllm-project/vllm/pull/31187)) by @Alexei-V-Ivanov-AMD
* [MoE Refactor][10/N] Cleanup Fp8 Process Weights After Loading ([#31169](https://github.com/vllm-project/vllm/pull/31169)) by @robertgshaw2-redhat
* [Mistral common] Ensure all functions are imported from the top & only use public methods ([#31138](https://github.com/vllm-project/vllm/pull/31138)) by @patrickvonplaten
* [Mics] add pcp basic support to MoE model ([#31003](https://github.com/vllm-project/vllm/pull/31003)) by @pisceskkk
* Fix/get raw stream patch #30905 ([#30912](https://github.com/vllm-project/vllm/pull/30912)) by @baonudesifeizhai
* [CI/Build] Ignore max transformers version skipping for initialization tests ([#30619](https://github.com/vllm-project/vllm/pull/30619)) by @Isotr0py
* [Audio] Improve Audio Inference Scripts (offline/online) ([#29279](https://github.com/vllm-project/vllm/pull/29279)) by @ekagra-ranjan
* Feature/isaac 0.1 ([#28367](https://github.com/vllm-project/vllm/pull/28367)) by @oscardev256
* [Prefix Cache] Include lora_name in BlockStored event for deterministic KV-cache reconstruction ([#27577](https://github.com/vllm-project/vllm/pull/27577)) by @sagearc

## Contributors

@Alexei-V-Ivanov-AMD, @Alnusjaponica, @AndreasKaratzas, @BoyuanFeng, @Dylan1229, @Isotr0py, @JartX, @Jzz1943, @NickLucche, @SameerAsal, @ZT-AIA, @amittell, @amrmahdi, @andyxning, @baonudesifeizhai, @bigPYJ1151, @c0de128, @chaunceyjiang, @chunxiaozheng, @danisereb, @effortprogrammer, @ekagra-ranjan, @esmeetu, @ganyi1996ppo, @hmellor, @ivanium, @jeejeelee, @jikunshang, @kevin-pw, @lengrongfu, @linitra24, @maang-h, @mratsim, @njhill, @oscardev256, @patrickvonplaten, @pisceskkk, @qli88, @reaganjlee, @rickychen-infinirc, @robertgshaw2-redhat, @rogerxfeng8, @rogeryoungh, @sagearc, @tjtanaa, @twjww, @vintipandey, @wangln19, @xinyu-intel, @yewentao256, @yitingdc, @yt0428

