# Weekly Release Notes for vllm-project/vllm (2025-10-17)

## What's Changed

### ‚ú® Features & Enhancements

* Support `set` in the CLI generation ([#27031](https://github.com/vllm-project/vllm/pull/27031)) by @hmellor
* [Feature] Migrate DeepGEMM API from `get_m_alignment_for_contiguous_layout` to `get_mk_alignment_for_contiguous_layout` ([#26935](https://github.com/vllm-project/vllm/pull/26935)) by @yewentao256
* Support block size of 256 used by Intel HPU ([#26883](https://github.com/vllm-project/vllm/pull/26883)) by @mandy-li
* [Feature] Add process_weights_after_loading to AttentionImpl ([#26870](https://github.com/vllm-project/vllm/pull/26870)) by @lengrongfu
* [Feature] default --extra-body param to disable thinking in vllm bench serve ([#26784](https://github.com/vllm-project/vllm/pull/26784)) by @lengrongfu
* [Feature] Change vllm.py with pydantic validation ([#26726](https://github.com/vllm-project/vllm/pull/26726)) by @VladOS95-cyber
* support flashinfer_fp4 moe for 5090 gpu ([#26669](https://github.com/vllm-project/vllm/pull/26669)) by @XiaobingSuper
* Add @noooop to codeowner for pooling models ([#26652](https://github.com/vllm-project/vllm/pull/26652)) by @noooop
* [Feature]: Use pydantic validation in observability.py config ([#26637](https://github.com/vllm-project/vllm/pull/26637)) by @cern1710
* [FEATURE]: Use pydantic validation in `multimodal.py` config ([#26629](https://github.com/vllm-project/vllm/pull/26629)) by @andycandy
* Add support for the /rerank endpoint in vllm bench serve ([#26602](https://github.com/vllm-project/vllm/pull/26602)) by @maxdebayser
* Add tests for chunked prefill and prefix cache with causal pooling models ([#26526](https://github.com/vllm-project/vllm/pull/26526)) by @maxdebayser
* Add EAGLE-3 Speculative Decoding Support for Qwen3 MoE ([#26485](https://github.com/vllm-project/vllm/pull/26485)) by @rahul-tuli
* [Feature] Add support for naver/splade-v3 (BERT-based sparse embedding model) ([#26339](https://github.com/vllm-project/vllm/pull/26339)) by @gjgjos
* Add Qwen3-Omni moe thinker ([#25550](https://github.com/vllm-project/vllm/pull/25550)) by @wangxiongts
* [Feature][Responses API] Stream Function Call - harmony ([#24317](https://github.com/vllm-project/vllm/pull/24317)) by @chaunceyjiang
* [Feature][Quantization] auto_round format add support for regex ([#24024](https://github.com/vllm-project/vllm/pull/24024)) by @n1ck-guo

### üêõ Bug Fixes

* Fix Qwen2.5 VL image grid docstring ([#27033](https://github.com/vllm-project/vllm/pull/27033)) by @skyloevil
* [Bug] Fix batch invariant test `has` to `is` ([#27032](https://github.com/vllm-project/vllm/pull/27032)) by @yewentao256
* [Bugfix] Correct LayerNorm epsilon parameter in modernbert.py ([#27008](https://github.com/vllm-project/vllm/pull/27008)) by @bogdan01m
* [BUG] Allow runai_streamer_sharded in config check ([#26958](https://github.com/vllm-project/vllm/pull/26958)) by @ahao-anyscale
* [BugFix] Work around graph partition x torch.compile cache issue ([#26956](https://github.com/vllm-project/vllm/pull/26956)) by @zou3519
* [bugfix] Fix SP + PP without specifying compile size ([#26955](https://github.com/vllm-project/vllm/pull/26955)) by @angelayi
* [Bug] Temporally Disable `VLLM_ALLREDUCE_USE_SYMM_MEM` by Default ([#26925](https://github.com/vllm-project/vllm/pull/26925)) by @yewentao256
* [BugFix] Patch inductor memory plan logic ([#26878](https://github.com/vllm-project/vllm/pull/26878)) by @BoyuanFeng
* [Fix] Remove divisibility requirement between num_kv_heads and tp_size in bailing_moe ([#26876](https://github.com/vllm-project/vllm/pull/26876)) by @ant-yy
* [Bugfix][Multi Modal] Fix incorrect Molmo token processing ([#26873](https://github.com/vllm-project/vllm/pull/26873)) by @sangho-vision
* [bugfix] Lazy import cv2 ([#26869](https://github.com/vllm-project/vllm/pull/26869)) by @angelayi
* [BUGFIX][NIXL] quick fix for 'assert self.connector_worker is not None' in get_kv_connector_stats ([#26851](https://github.com/vllm-project/vllm/pull/26851)) by @xuechendi
* [Bug] Add Assertion for `random-input-len` / `random-output-len` ([#26834](https://github.com/vllm-project/vllm/pull/26834)) by @yewentao256
* [Bugfix] Fixes prefix-repetition benchmark script ([#26828](https://github.com/vllm-project/vllm/pull/26828)) by @kouroshHakha
* [Bugfix] Fix qwen3-omni audio truncation issue ([#26815](https://github.com/vllm-project/vllm/pull/26815)) by @Isotr0py
* [Bugfix] Standardize merging multimodal embeddings ([#26771](https://github.com/vllm-project/vllm/pull/26771)) by @DarkLight1337
* [BugFix] Patch inductor partitioning logic ([#26735](https://github.com/vllm-project/vllm/pull/26735)) by @angelayi
* Fix lora tests failure in TPU CI due to the removal of LoRA bias ([#26723](https://github.com/vllm-project/vllm/pull/26723)) by @vanbasten23
* [Bugfix] Fix out of bound index issue for Jina-embedding-v3 RoPE with cuda graph ([#26687](https://github.com/vllm-project/vllm/pull/26687)) by @Isotr0py
* [Bugfix][Core]Fix block table out-of-range issue in priority scheduling ([#26661](https://github.com/vllm-project/vllm/pull/26661)) by @quanliu1991
* [Bugfix][CI/Build] Fix failing Mteb CI ([#26638](https://github.com/vllm-project/vllm/pull/26638)) by @Isotr0py
* [Bugfix] Fix qwen-moe packed_modules_mapping ([#26634](https://github.com/vllm-project/vllm/pull/26634)) by @jeejeelee
* [Bugfix][Qwen3VL] fix deepstack in qwen3vl ([#26626](https://github.com/vllm-project/vllm/pull/26626)) by @JJJYmmm
* Fix some typing issues found by `mypy==1.18.2` ([#26596](https://github.com/vllm-project/vllm/pull/26596)) by @hmellor
* [Bugfix][Speculative Decoding] Extend Eagle quantization config fix to llama_eagle.py ([#26590](https://github.com/vllm-project/vllm/pull/26590)) by @rahul-tuli
* [Bugfix][DCP] Set default CUDAGraphMode to PIECEWISE for DCP ([#26574](https://github.com/vllm-project/vllm/pull/26574)) by @FENP
* [BUG] Qwen3-next MTP. Fix attn metadata build bug ([#26564](https://github.com/vllm-project/vllm/pull/26564)) by @vadiklyutiy
* [Bugfix][Multi Modal] Fix incorrect Molmo image processing ([#26563](https://github.com/vllm-project/vllm/pull/26563)) by @sangho-vision
* [Bugfix] Convert untraceable GroupShape to list for AMD impl ([#26535](https://github.com/vllm-project/vllm/pull/26535)) by @Lucaskabela
* [Bug] Fix Assertion error DeepEP/csrc/kernels/intranode.cu:928: 'false and Unsupported type' ([#26532](https://github.com/vllm-project/vllm/pull/26532)) by @yewentao256
* fix test_simple_inductor_graph_partition ([#26522](https://github.com/vllm-project/vllm/pull/26522)) by @BoyuanFeng
* [Bugfix] fixed top_logprobs: -1 does not appear to work as intended ([#26470](https://github.com/vllm-project/vllm/pull/26470)) by @chaunceyjiang
* [BugFix] Make penalties and bad_words work with async scheduling ([#26467](https://github.com/vllm-project/vllm/pull/26467)) by @njhill
* [BugFix] Fix noop elimination edge case ([#26394](https://github.com/vllm-project/vllm/pull/26394)) by @andylolu2
* [BugFix] Fix async scheduling + request preemption ([#26385](https://github.com/vllm-project/vllm/pull/26385)) by @njhill
* [Bugfix] Make DP padding optional in coordinate_batch_across_dp ([#26375](https://github.com/vllm-project/vllm/pull/26375)) by @SageMoore
* [Bugfix]fix Qwen3 xml tool parser ([#26345](https://github.com/vllm-project/vllm/pull/26345)) by @Zhikaiiii
* [bugfix][DCP] fix block_size of hash in DCP prefix caching ([#26296](https://github.com/vllm-project/vllm/pull/26296)) by @heheda12345
* [Bugfix] reasoning_parser parameter handling in run_batch.py ([#26225](https://github.com/vllm-project/vllm/pull/26225)) by @inc-jeong
* [BugFix][torch.compile] Fix fused_scaled_matmul_reduce_scatter signature for PyTorch 2.8 ([#26038](https://github.com/vllm-project/vllm/pull/26038)) by @jasonlizhengjian
* [Bugfix][Rocm] fix qr error when different inp shape ([#25892](https://github.com/vllm-project/vllm/pull/25892)) by @haoyangli-amd
* [FIX] Throwing an exception when the model does not support pool tasks (#25840) ([#25855](https://github.com/vllm-project/vllm/pull/25855)) by @yyzxw
* fix: response_format for completion ([#23212](https://github.com/vllm-project/vllm/pull/23212)) by @Nan2018

### ‚ö°Ô∏è Performance

* [PERF] Qwen3-next MTP speedup (change bool mask indexing to index_select / index_copy to reduce d2h) ([#26437](https://github.com/vllm-project/vllm/pull/26437)) by @vadiklyutiy
* [PERF] [Qwen3-next] Speed up gated RMSNorm ([#26207](https://github.com/vllm-project/vllm/pull/26207)) by @vadiklyutiy
* [Perf] Cache vllm.env.__getattr__ result to avoid recomputation ([#26146](https://github.com/vllm-project/vllm/pull/26146)) by @Jialin

### ü§ñ Model Support

* [Model] Fix Qwen3VL mm mapping ([#27027](https://github.com/vllm-project/vllm/pull/27027)) by @jeejeelee
* [gpt-oss][1/N] EZ: refactor serving_responses for modularity ([#26948](https://github.com/vllm-project/vllm/pull/26948)) by @qandrew
* [Model][Bugfix] fix ernie45 vl run failed from shared experts optimization ([#26885](https://github.com/vllm-project/vllm/pull/26885)) by @CSWYF3634076
* llama4_vision_rope: add HIP override to accept (q, k) and avoid (positions, q, k) mismatch ([#26790](https://github.com/vllm-project/vllm/pull/26790)) by @hl475
* [Model] Use merge_by_field_config for MM models (O-P) ([#26776](https://github.com/vllm-project/vllm/pull/26776)) by @DarkLight1337
* [Model] Use merge_by_field_config for MM models (M-N) ([#26710](https://github.com/vllm-project/vllm/pull/26710)) by @DarkLight1337
* [Model][Bugfix]fix ernie45 load failed due to ernie45 eplb code ([#26684](https://github.com/vllm-project/vllm/pull/26684)) by @CSWYF3634076
* [Model] Fix  Skywork R1V mlp ([#26673](https://github.com/vllm-project/vllm/pull/26673)) by @jeejeelee
* [Model][Qwen3VL] Compute `cu_seqlens` on CPU to remove  ([#26496](https://github.com/vllm-project/vllm/pull/26496)) by @lgeiger
* [Model][0/N] Improve all pooling task | clean up ([#25817](https://github.com/vllm-project/vllm/pull/25817)) by @noooop
* [GPT-OSS] Add support for arrays  at tool message content ([#25593](https://github.com/vllm-project/vllm/pull/25593)) by @luis5tb
* [Model] Add DeepSeek-V3.1 reasoning parser (split from PR #24972) ([#25589](https://github.com/vllm-project/vllm/pull/25589)) by @taohui
* [Model][2/N] Improve all pooling task | Support multi-vector retrieval ([#25370](https://github.com/vllm-project/vllm/pull/25370)) by @noooop
* [Model] Add reasoning_parser and tool_parser for Ernie45 thinking ([#25027](https://github.com/vllm-project/vllm/pull/25027)) by @CSWYF3634076
* [Model] Add FlexOlmo model implementation ([#24923](https://github.com/vllm-project/vllm/pull/24923)) by @2015aroras

### üîå Hardware & Backend

* [NVIDIA] [Perf] Update to leverage flashinfer trtllm FP4 MOE throughput kernel ([#26714](https://github.com/vllm-project/vllm/pull/26714)) by @jiahanc
* [XPU] Upgrade NIXL to remove CUDA dependency ([#26570](https://github.com/vllm-project/vllm/pull/26570)) by @zhenwei-intel
* [NVIDIA] Add support for cudnn fp4 gemm via flashinfer ([#26107](https://github.com/vllm-project/vllm/pull/26107)) by @kaixih
* [ROCm][FEAT] Fuse DeepSeek shared experts into AITER fused_moe ops ([#24097](https://github.com/vllm-project/vllm/pull/24097)) by @kliuae

### ‚öôÔ∏è Refactoring & Core

* Remove unused imports ([#26972](https://github.com/vllm-project/vllm/pull/26972)) by @lgeiger
* Refactor Transformers backend to use mixins ([#26906](https://github.com/vllm-project/vllm/pull/26906)) by @hmellor
* [Kernel][MoE] Add MoE tunings for GLM 4.6-FP8 and GLM 4.5 Air on NVidia B200 ([#26818](https://github.com/vllm-project/vllm/pull/26818)) by @zklapow
* [Core][Easy] Use envs.__getattr__ for all Unify to environment variable access ([#26810](https://github.com/vllm-project/vllm/pull/26810)) by @Jialin
* [Core] Streamline some structured output related code ([#26737](https://github.com/vllm-project/vllm/pull/26737)) by @njhill
* remove attn output view kernel ([#26680](https://github.com/vllm-project/vllm/pull/26680)) by @BoyuanFeng
* [Refactor]Reduce duplicate code in serving_chat ([#26627](https://github.com/vllm-project/vllm/pull/26627)) by @chaunceyjiang
* [Core] Small simplification in `GPUModelRunner._update_states()` ([#26508](https://github.com/vllm-project/vllm/pull/26508)) by @njhill
* [FrontEnd] UNREVERT CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops  ([#26502](https://github.com/vllm-project/vllm/pull/26502)) by @morrison-turnansky
* [Frontend][1/N] Improve all pooling task | Support FP16 Embedding Base64 (Still uses fp32 by default). ([#26414](https://github.com/vllm-project/vllm/pull/26414)) by @noooop
* [Frontend][torch.compile] CompilationConfig Overhaul (#20283): name change  compilation level to compilation mode, deprecation compilation level ([#26355](https://github.com/vllm-project/vllm/pull/26355)) by @morrison-turnansky
* Remove LoRA bias support ([#25807](https://github.com/vllm-project/vllm/pull/25807)) by @ashwin-phadke
* [frontend][gptoss] Add per turn stats into Harmony Context ([#25061](https://github.com/vllm-project/vllm/pull/25061)) by @lacora
* [Core] Reuse empty block lists whenever possible in KVCacheBlocks to mitigate GC costs ([#24964](https://github.com/vllm-project/vllm/pull/24964)) by @Jialin
* [Refactor]: Use M-RoPE interface directly while defining model class instead of maintaining model specific M-RoPE implementation in mrope.py ([#24172](https://github.com/vllm-project/vllm/pull/24172)) by @divyanshsinghvi

### üîß Build, CI & Testing

* [CI] Prune Quantization Tests and skip compilation ([#27038](https://github.com/vllm-project/vllm/pull/27038)) by @mgoin
* [CI] Fix mypy for `vllm/executor` ([#26845](https://github.com/vllm-project/vllm/pull/26845)) by @yewentao256
* [CI] Fix test_tool_id_kimi_k2 ([#26787](https://github.com/vllm-project/vllm/pull/26787)) by @chaunceyjiang
* [CI] [ROCm] Automate CC list for ROCm related issue ([#26753](https://github.com/vllm-project/vllm/pull/26753)) by @vllmellm
* [CI] Enable Blackwell Llama4 MoE tests ([#26731](https://github.com/vllm-project/vllm/pull/26731)) by @mgoin
* [CI] Raise VLLM_MAX_SIZE_MB to 500 due to failing Build wheel - CUDA 12.9 ([#26722](https://github.com/vllm-project/vllm/pull/26722)) by @mgoin
* [build][torch.compile] upgrade depyf version ([#26702](https://github.com/vllm-project/vllm/pull/26702)) by @youkaichao
* [CI][Release][Arm64]: Build arm64 release for gpu arch 8.9 ([#26698](https://github.com/vllm-project/vllm/pull/26698)) by @cyb70289
* [CI] Fix mypy for `vllm/distributed` ([#26593](https://github.com/vllm-project/vllm/pull/26593)) by @yewentao256
* [CI] fix ruff format ([#26579](https://github.com/vllm-project/vllm/pull/26579)) by @chaunceyjiang
* [CI] fix test_run_batch.py::test_completions - AssertionError ([#26578](https://github.com/vllm-project/vllm/pull/26578)) by @chaunceyjiang
* [TEST][BUG FIX] Fix DP GPU_ID issue ([#26442](https://github.com/vllm-project/vllm/pull/26442)) by @xuechendi
* [CI] Replace large models with tiny alternatives in tests ([#24057](https://github.com/vllm-project/vllm/pull/24057)) by @tahsintunan

### üìö Documentation

* [docs] standardize Hugging Face env var to `HF_TOKEN` (deprecates `HUGGING_FACE_HUB_TOKEN`) ([#27020](https://github.com/vllm-project/vllm/pull/27020)) by @yankay
* [DOC][XPU]update feature parity with Intel GPU ([#26954](https://github.com/vllm-project/vllm/pull/26954)) by @xuechendi
* [doc] add Context Parallel Deployment doc ([#26877](https://github.com/vllm-project/vllm/pull/26877)) by @youkaichao
* [Docs] Move build.inc into arm.inc ([#26862](https://github.com/vllm-project/vllm/pull/26862)) by @windsonsea
* [Doc] ruff format remaining Python examples ([#26795](https://github.com/vllm-project/vllm/pull/26795)) by @DarkLight1337
* [Doc] ruff format some Python examples ([#26767](https://github.com/vllm-project/vllm/pull/26767)) by @DarkLight1337
* [Docs] Add a start tag to build.inc.md ([#26747](https://github.com/vllm-project/vllm/pull/26747)) by @windsonsea
* docs: wrong command in structured_outputs README ([#26677](https://github.com/vllm-project/vllm/pull/26677)) by @yihong0618

### üì¶ Miscellaneous

* [torch.compile] fix simple inductor graph partition test ([#27050](https://github.com/vllm-project/vllm/pull/27050)) by @BoyuanFeng
* [torch.compile] Passing only necessary compilation config to inductor pass config ([#27041](https://github.com/vllm-project/vllm/pull/27041)) by @luccafong
* [Chore] Separate out `vllm.utils.import_utils` ([#27022](https://github.com/vllm-project/vllm/pull/27022)) by @DarkLight1337
* [Benchmark] Show E2EL by default for pooling models ([#27014](https://github.com/vllm-project/vllm/pull/27014)) by @DarkLight1337
* [Benchmark] Use truncation by default for pooling benchmarks ([#26992](https://github.com/vllm-project/vllm/pull/26992)) by @DarkLight1337
* [Chore] Separate out `vllm.utils.collections` ([#26990](https://github.com/vllm-project/vllm/pull/26990)) by @DarkLight1337
* [Hardware][CPU][PowerPC]Disable torch.compile() in toptopk sampling ([#26987](https://github.com/vllm-project/vllm/pull/26987)) by @Akashcodes732
* [CI/Build] Update expected beam search output for Phi3V ([#26978](https://github.com/vllm-project/vllm/pull/26978)) by @DarkLight1337
* Adding Warmup to Benchmark Serving ([#26943](https://github.com/vllm-project/vllm/pull/26943)) by @kimbochen
* [Chore] Clean up CODEOWNERS ([#26923](https://github.com/vllm-project/vllm/pull/26923)) by @WoosukKwon
* [Chore] Rename `utils` submodules ([#26920](https://github.com/vllm-project/vllm/pull/26920)) by @DarkLight1337
* Lower severity of log when model info cache misses due to exception ([#26917](https://github.com/vllm-project/vllm/pull/26917)) by @hmellor
* [Chore] Separate out `vllm.utils.async_utils` ([#26913](https://github.com/vllm-project/vllm/pull/26913)) by @DarkLight1337
* [Chore] Separate out `vllm.utils.func` ([#26904](https://github.com/vllm-project/vllm/pull/26904)) by @DarkLight1337
* [ModelOpt] Remove NVFP4 MoE K%16==0 constraint ([#26891](https://github.com/vllm-project/vllm/pull/26891)) by @XiaobingSuper
* chore: remove unused marker ([#26890](https://github.com/vllm-project/vllm/pull/26890)) by @max-wittig
* [Misc] Remove `isort` and `yapf` ignores ([#26888](https://github.com/vllm-project/vllm/pull/26888)) by @DarkLight1337
* [Qwen3-Next] Add tuned MoE config for Qwen3-Next FP8 on H100 tp2 ([#26887](https://github.com/vllm-project/vllm/pull/26887)) by @felixzhu555
* [Misc] Use helper function to generate dummy messages in OpenAI MM tests ([#26875](https://github.com/vllm-project/vllm/pull/26875)) by @DarkLight1337
* Disable FlashInfer sampler by default ([#26859](https://github.com/vllm-project/vllm/pull/26859)) by @mgoin
* [small][batch invariance] Rename the env and internal flags to simplify usage ([#26855](https://github.com/vllm-project/vllm/pull/26855)) by @bwasti
* [Misc] Update TritonLanguagePlaceholder to have attributes that are used by Flash Linear Attention ops. ([#26853](https://github.com/vllm-project/vllm/pull/26853)) by @madongfly
* Adjusting AMD test composition 2025-10-14 ([#26852](https://github.com/vllm-project/vllm/pull/26852)) by @Alexei-V-Ivanov-AMD
* [Compressed Tensors] Always clone output for compile robustness ([#26849](https://github.com/vllm-project/vllm/pull/26849)) by @kylesayrs
* [Attention] Tune CUTLASS MLA num_splits ([#26846](https://github.com/vllm-project/vllm/pull/26846)) by @MatthewBonanni
* [Easy] Get rid of unnecessary paraenthesis in kv_cache_manager ([#26842](https://github.com/vllm-project/vllm/pull/26842)) by @Jialin
* [CI/Build] Fix AMD import failures in CI ([#26841](https://github.com/vllm-project/vllm/pull/26841)) by @zhewenl
* Added MoE configs for llama 4, H200 device with tp=4/8 tuning ([#26837](https://github.com/vllm-project/vllm/pull/26837)) by @Dhruvilbhatt
* [WideEP][P/D] Add usage stats for DP+EP and KV Connector ([#26836](https://github.com/vllm-project/vllm/pull/26836)) by @tlrmchlsmth
* [Graph Partition] pass tests for decorator ([#26831](https://github.com/vllm-project/vllm/pull/26831)) by @BoyuanFeng
* [CI Failure] Fix torchao dep failure for Quantization Test ([#26824](https://github.com/vllm-project/vllm/pull/26824)) by @mgoin
* Notice for deprecation of AutoAWQ ([#26820](https://github.com/vllm-project/vllm/pull/26820)) by @HDCharles
* [CI Failure] Fix tests with missing TinyLlama-1.1B-Chat-v1.0-FP8-e2e ([#26816](https://github.com/vllm-project/vllm/pull/26816)) by @mgoin
* Revert "[issues template] Encourage the author implement their own ideas" ([#26814](https://github.com/vllm-project/vllm/pull/26814)) by @noooop
* Adjusted the model order of the model registration file ([#26798](https://github.com/vllm-project/vllm/pull/26798)) by @princepride
* [Chore] Use `max_transformers_version` for Qwen-VL test ([#26792](https://github.com/vllm-project/vllm/pull/26792)) by @DarkLight1337
* Don't allow `typos` to fix by default ([#26785](https://github.com/vllm-project/vllm/pull/26785)) by @hmellor
* [Chore] Remove `SupportsV0Only` interface and update supported models docs ([#26783](https://github.com/vllm-project/vllm/pull/26783)) by @DarkLight1337
* [CI/Build][Bugfix] fix qutlass cmake error when set QUTLASS_SRC_DIR ([#26773](https://github.com/vllm-project/vllm/pull/26773)) by @izhuhaoran
* scheduler.py: Update the name of the default scheduler. ([#26758](https://github.com/vllm-project/vllm/pull/26758)) by @ryanli
* [Plugin] Make plugin group clear ([#26757](https://github.com/vllm-project/vllm/pull/26757)) by @wangxiyuan
* [CI/Build] Cleanup LoRA test ([#26752](https://github.com/vllm-project/vllm/pull/26752)) by @jeejeelee
* [CI/Build] Use 127.0.0.1 instead of localhost in utils ([#26750](https://github.com/vllm-project/vllm/pull/26750)) by @yeqcharlotte
* [Config] Remove Unused Environment Variable `VLLM_DISABLE_PAD_FOR_CUDAGRAPH` ([#26743](https://github.com/vllm-project/vllm/pull/26743)) by @yewentao256
* [Easy] Fix env type check errors from VLLM_DEBUG_LOG_API_SERVER_RESPONSE ([#26742](https://github.com/vllm-project/vllm/pull/26742)) by @Jialin
* [torch.compile] Unwrap fused_marlin_moe custom op ([#26739](https://github.com/vllm-project/vllm/pull/26739)) by @varun-sundar-rabindranath
* [Minor] Group async_scheduling related fields in model runner init ([#26736](https://github.com/vllm-project/vllm/pull/26736)) by @njhill
* [UX] Replace VLLM_ALL2ALL_BACKEND with --all2all-backend ([#26732](https://github.com/vllm-project/vllm/pull/26732)) by @mgoin
* [ResponseAPI] Further polish message serialization and unit tests ([#26728](https://github.com/vllm-project/vllm/pull/26728)) by @Jialin
* Pruning kernel Core Tests ([#26727](https://github.com/vllm-project/vllm/pull/26727)) by @kfhfar
* Adding the test-amd.yaml for test definitions for the AMD backend. (alternative PR) ([#26718](https://github.com/vllm-project/vllm/pull/26718)) by @Alexei-V-Ivanov-AMD
* [Misc] Separate prompt logging to debug ([#26713](https://github.com/vllm-project/vllm/pull/26713)) by @aitsvet
* [Misc] rename torch_dtype to dtype ([#26695](https://github.com/vllm-project/vllm/pull/26695)) by @wangxiyuan
* [Hardware][CPU] Disable torch.compile for RISC-V to prevent APIError ([#26693](https://github.com/vllm-project/vllm/pull/26693)) by @ihb2032
* Ignore large reformatting PRs in `git blame` ([#26690](https://github.com/vllm-project/vllm/pull/26690)) by @hmellor
* use combo kernel to fuse qk-norm and qk-rope ([#26682](https://github.com/vllm-project/vllm/pull/26682)) by @BoyuanFeng
* [compile] Enable sequence parallelism for full cuda graph without specifying compile sizes ([#26681](https://github.com/vllm-project/vllm/pull/26681)) by @angelayi
* [issues template] Encourage the author implement their own ideas ([#26671](https://github.com/vllm-project/vllm/pull/26671)) by @noooop
* [Misc] cache result of disable_inplace ([#26666](https://github.com/vllm-project/vllm/pull/26666)) by @bnellnm
* [easy] fix pre commit error on trunk ([#26665](https://github.com/vllm-project/vllm/pull/26665)) by @hl475
* [DSA][MLA] Tiny refactor on DeepSeek to make it reusable for different backends ([#26656](https://github.com/vllm-project/vllm/pull/26656)) by @MengqingCao
* [MISC] fix import violations for re and triton modules ([#26654](https://github.com/vllm-project/vllm/pull/26654)) by @llsj14
* [Models][Qwen3VL] Speedup `fast_pos_embed_interpolate` ([#26647](https://github.com/vllm-project/vllm/pull/26647)) by @lgeiger
* [compile] Fix inductor partition config ([#26645](https://github.com/vllm-project/vllm/pull/26645)) by @angelayi
* [Benchmark] Support Infinity API ([#26641](https://github.com/vllm-project/vllm/pull/26641)) by @DarkLight1337
* Update `Optional[x]` -> `x | None` and `Union[x, y]` to `x | y` ([#26633](https://github.com/vllm-project/vllm/pull/26633)) by @hmellor
* [ResponseAPI] Simplify input/output message serialization ([#26620](https://github.com/vllm-project/vllm/pull/26620)) by @Jialin
* Deepseek-v3 Batch Invariant on 8xH100 ([#26609](https://github.com/vllm-project/vllm/pull/26609)) by @bwasti
* [MM] Move Qwen3Omni MRoPE impl to model file ([#26608](https://github.com/vllm-project/vllm/pull/26608)) by @ywang96
* [compile] Add patched_fused_scaled_matmul_reduce_scatter ([#26604](https://github.com/vllm-project/vllm/pull/26604)) by @angelayi
* [Log] Optimize Startup Log ([#26601](https://github.com/vllm-project/vllm/pull/26601)) by @yewentao256
* Update CUDA architecture list in build pipeline for 12.9.1 wheels ([#26592](https://github.com/vllm-project/vllm/pull/26592)) by @wseaton
* Update `pre-commit` hook versions ([#26591](https://github.com/vllm-project/vllm/pull/26591)) by @hmellor
* [Metrics] Add test for multi-modal cache stats logging ([#26588](https://github.com/vllm-project/vllm/pull/26588)) by @markmc
* Added test_top_k_per_row to test-pipeline.yaml. ([#26569](https://github.com/vllm-project/vllm/pull/26569)) by @dcampora
* [CPU] fix the issue when the node is '-' cause json decode error. ([#26562](https://github.com/vllm-project/vllm/pull/26562)) by @muzian666
* [deepseek] kernel block size for UniformTypeKVCacheSpecs ([#26559](https://github.com/vllm-project/vllm/pull/26559)) by @heheda12345
* [NIXL][HeteroTP]Enable KV transfer from HND prefill to NHD decode ([#26556](https://github.com/vllm-project/vllm/pull/26556)) by @xuechendi
* [Attention][Spec Decode] FlashMLA spec decode support ([#26541](https://github.com/vllm-project/vllm/pull/26541)) by @MatthewBonanni
* [CI Perf]Prune Tests in kernel/mamba ([#26538](https://github.com/vllm-project/vllm/pull/26538)) by @kfhfar
* Move query quantization to attention layer for Flashinfer & Triton. ([#26534](https://github.com/vllm-project/vllm/pull/26534)) by @adabeyta
* Cleanup code after Python 3.10 upgrade ([#26520](https://github.com/vllm-project/vllm/pull/26520)) by @lgeiger
* [Chore]: One pythonic tool parser test uses the wrong parser ([#26515](https://github.com/vllm-project/vllm/pull/26515)) by @bbrowning
* Cache the environment variable check for batch invariance ([#26510](https://github.com/vllm-project/vllm/pull/26510)) by @bwasti
* CP: make correct_attn_out robust to 4‚ÄëD views and fix Triton arg binding ([#26509](https://github.com/vllm-project/vllm/pull/26509)) by @hl475
* [CI/Build] upgrade compressed-tensors to 0.12.2 to address LGPLv3 ([#26501](https://github.com/vllm-project/vllm/pull/26501)) by @csy1204
* vllm bench serve shows num of failed requests ([#26478](https://github.com/vllm-project/vllm/pull/26478)) by @tomasruizt
* [Deepseek-V3.2][Kernel] Integrate cuda indexer k cache gather ([#26456](https://github.com/vllm-project/vllm/pull/26456)) by @zyongye
* Update coveragerc and add codecov.yml for path fixes ([#26435](https://github.com/vllm-project/vllm/pull/26435)) by @rzabarazesh
* fix(nix): Allow local oneDNN path to fix vLLM CPU build failure ([#26401](https://github.com/vllm-project/vllm/pull/26401)) by @ihb2032
* [unrevert] Add batch invariant kernel override for FlashInfer backend [2/n] ([#26373](https://github.com/vllm-project/vllm/pull/26373)) by @bwasti
* [Lora]Load tuned multi-lora kernel configs from json files ([#26319](https://github.com/vllm-project/vllm/pull/26319)) by @li2haipeng
* [Metrics] Log multi-modal cache stats and fix reset ([#26285](https://github.com/vllm-project/vllm/pull/26285)) by @DarkLight1337
* Vectorize RMS norm variance using vectorize_read_with_alignment ([#26234](https://github.com/vllm-project/vllm/pull/26234)) by @bbeckca
* [P/D] [NixlConnector] kv load recovery integration ([#26171](https://github.com/vllm-project/vllm/pull/26171)) by @wseaton
* Olmo 3 tool parser and tests ([#26143](https://github.com/vllm-project/vllm/pull/26143)) by @pdasigi
* [torch.compile] Fix tests for torch==2.9 inductor partition ([#26116](https://github.com/vllm-project/vllm/pull/26116)) by @ProExpertProg
* [Quantization] [Performance] Enable Marlin GEMM kernels for the calibration-free RTN-based quantization ([#26051](https://github.com/vllm-project/vllm/pull/26051)) by @sakogan
* [KVConnector][Metrics] Aggregate scheduler-side KVConnectorStats ([#26046](https://github.com/vllm-project/vllm/pull/26046)) by @QierLi
* [GPTOSS][DP/EP][Marlin] Enable GPTOSS Batched DP/EP using Marlin kernels ([#25997](https://github.com/vllm-project/vllm/pull/25997)) by @varun-sundar-rabindranath
* [MISC] Rename the torch profiler filename as instance_id+rank_id for merging the Profiler results of each Rank ([#25867](https://github.com/vllm-project/vllm/pull/25867)) by @noooop
* [torch.compile] Make inductor partition rules respect splitting_ops #25691 ([#25845](https://github.com/vllm-project/vllm/pull/25845)) by @baonudesifeizhai
*  [Frontend] Improve the performance of `is_reasoning_end` ([#25735](https://github.com/vllm-project/vllm/pull/25735)) by @chaunceyjiang
* [NIXL] Improve request_finished() debug logs ([#25665](https://github.com/vllm-project/vllm/pull/25665)) by @markmc
* [UX] Speedup DeepGEMM warmup with heuristics ([#25619](https://github.com/vllm-project/vllm/pull/25619)) by @mgoin
* [Spec-Decode] Support piecewise cudagraphs for Eagle head ([#25109](https://github.com/vllm-project/vllm/pull/25109)) by @LucasWilkinson
* Silu v2 ([#25074](https://github.com/vllm-project/vllm/pull/25074)) by @elvircrn
* [NIXL] Ignore abort on already-finished request ([#25067](https://github.com/vllm-project/vllm/pull/25067)) by @markmc
* [DCP] Support Decode Context Parallel (DCP) for GQA with FlashAttention ([#24864](https://github.com/vllm-project/vllm/pull/24864)) by @FENP
* [Transform] [Quantization] Add QuTLASS support to vLLM ([#24440](https://github.com/vllm-project/vllm/pull/24440)) by @LopezCastroRoberto
* [Misc][DP] support customized aggregated logger for dp ([#24354](https://github.com/vllm-project/vllm/pull/24354)) by @luccafong
* AOT Compilation for torch.compile (Bundled) ([#24274](https://github.com/vllm-project/vllm/pull/24274)) by @zhxchen17
* [DP][ray] Support different VLLM_RAY_DP_PACK_STRATEGY ([#23849](https://github.com/vllm-project/vllm/pull/23849)) by @ruisearch42
* [CI/Build] Fix ppc64le CPU build and tests ([#22443](https://github.com/vllm-project/vllm/pull/22443)) by @npanpaliya
* [Platform] allow platform to init dp group ([#22243](https://github.com/vllm-project/vllm/pull/22243)) by @wangxiyuan
* [EPLB] Support ernie4.5-moe ([#22100](https://github.com/vllm-project/vllm/pull/22100)) by @HsChen-sys
* [CI/Build] Add Qwen2.5-VL-7B-Instruct ChartQA Accuracy Tests in CI ([#21810](https://github.com/vllm-project/vllm/pull/21810)) by @zhewenl
* fix(frontend): always include usage, when configured to do so ([#20983](https://github.com/vllm-project/vllm/pull/20983)) by @max-wittig
* [CI/Build] Add tool to build vllm-tpu wheel ([#19165](https://github.com/vllm-project/vllm/pull/19165)) by @mgoin

## Contributors

@2015aroras, @Akashcodes732, @Alexei-V-Ivanov-AMD, @BoyuanFeng, @CSWYF3634076, @DarkLight1337, @Dhruvilbhatt, @FENP, @HDCharles, @HsChen-sys, @Isotr0py, @JJJYmmm, @Jialin, @LopezCastroRoberto, @LucasWilkinson, @Lucaskabela, @MatthewBonanni, @MengqingCao, @Nan2018, @ProExpertProg, @QierLi, @SageMoore, @VladOS95-cyber, @WoosukKwon, @XiaobingSuper, @Zhikaiiii, @adabeyta, @ahao-anyscale, @aitsvet, @andycandy, @andylolu2, @angelayi, @ant-yy, @ashwin-phadke, @baonudesifeizhai, @bbeckca, @bbrowning, @bnellnm, @bogdan01m, @bwasti, @cern1710, @chaunceyjiang, @csy1204, @cyb70289, @dcampora, @divyanshsinghvi, @elvircrn, @felixzhu555, @gjgjos, @haoyangli-amd, @heheda12345, @hl475, @hmellor, @ihb2032, @inc-jeong, @izhuhaoran, @jasonlizhengjian, @jeejeelee, @jiahanc, @kaixih, @kfhfar, @kimbochen, @kliuae, @kouroshHakha, @kylesayrs, @lacora, @lengrongfu, @lgeiger, @li2haipeng, @llsj14, @luccafong, @luis5tb, @madongfly, @mandy-li, @markmc, @max-wittig, @maxdebayser, @mgoin, @morrison-turnansky, @muzian666, @n1ck-guo, @njhill, @noooop, @npanpaliya, @pdasigi, @princepride, @qandrew, @quanliu1991, @rahul-tuli, @ruisearch42, @ryanli, @rzabarazesh, @sakogan, @sangho-vision, @skyloevil, @tahsintunan, @taohui, @tlrmchlsmth, @tomasruizt, @vadiklyutiy, @vanbasten23, @varun-sundar-rabindranath, @vllmellm, @wangxiongts, @wangxiyuan, @windsonsea, @wseaton, @xuechendi, @yankay, @yeqcharlotte, @yewentao256, @yihong0618, @youkaichao, @ywang96, @yyzxw, @zhenwei-intel, @zhewenl, @zhxchen17, @zklapow, @zou3519, @zyongye

