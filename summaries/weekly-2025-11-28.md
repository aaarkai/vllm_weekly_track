# Weekly Release Notes for vllm-project/vllm (2025-11-28)

## What's Changed

### âœ¨ Features & Enhancements

* add skip_reading_prefix_cache in repr for PoolingParams ([#29620](https://github.com/vllm-project/vllm/pull/29620)) by @guodongxiaren
* add xpu supported model and model id for cpu ([#29380](https://github.com/vllm-project/vllm/pull/29380)) by @louie-tsai
* Add TP CLI argument to multimodal inference examples ([#29301](https://github.com/vllm-project/vllm/pull/29301)) by @faaany
* Add fused MoE config for H200 E160 N192 fp8 ([#29182](https://github.com/vllm-project/vllm/pull/29182)) by @FlintyLemming
* [Feature]: Improve GGUF loading from HuggingFace user experience like repo_id:quant_type ([#29137](https://github.com/vllm-project/vllm/pull/29137)) by @sts07142
* [Feature][Benchmark] add --link-vars can filter when serve_param equal bench_param ([#28909](https://github.com/vllm-project/vllm/pull/28909)) by @lengrongfu
* Add TRTLLM MoE NVFP4 kernel to CompressedTensorsW4A4MoeMethod ([#28892](https://github.com/vllm-project/vllm/pull/28892)) by @Victor49152
* [Feature] Shared Experts Overlap with FI deepgemm swap kernel, 2.2% throughput improvement and 3.6% TTFT improvement ([#28879](https://github.com/vllm-project/vllm/pull/28879)) by @yewentao256
* Add option to use unbacked, and backed size obl dynamic shapes for more sounds compilation. ([#26199](https://github.com/vllm-project/vllm/pull/26199)) by @laithsakka

### ðŸ› Bug Fixes

* [Bugfix] Fix doc build on main ([#29619](https://github.com/vllm-project/vllm/pull/29619)) by @DarkLight1337
* [Bugfix][MM encoder] Fix ViT attention backend resolving for Turing GPU ([#29614](https://github.com/vllm-project/vllm/pull/29614)) by @Isotr0py
* [Bugfix] Fix pre-commit ([#29601](https://github.com/vllm-project/vllm/pull/29601)) by @DarkLight1337
* [Bugfix] Fix HunyuanVL XD-RoPE ([#29593](https://github.com/vllm-project/vllm/pull/29593)) by @ywang96
* [Bugfix] Update Ultravox  compatibility ([#29588](https://github.com/vllm-project/vllm/pull/29588)) by @DarkLight1337
* [BugFix] Optional tokenizer argument when loading GGUF models ([#29582](https://github.com/vllm-project/vllm/pull/29582)) by @sts07142
* [BugFix] Fix new nightly failures ([#29578](https://github.com/vllm-project/vllm/pull/29578)) by @LucasWilkinson
* Fix tpu-inference platform path ([#29554](https://github.com/vllm-project/vllm/pull/29554)) by @jcyang43
* [Bugfix] Fix handling of image embeds in models ([#29480](https://github.com/vllm-project/vllm/pull/29480)) by @DarkLight1337
* [Bugfix] Fix getting device for MoE LoRA ([#29475](https://github.com/vllm-project/vllm/pull/29475)) by @jeejeelee
* Fix TeleChatForCausalLM not register issue ([#29473](https://github.com/vllm-project/vllm/pull/29473)) by @Yejing-Lai
* [BugFix] Fix assertion for single world use case (uni) ([#29429](https://github.com/vllm-project/vllm/pull/29429)) by @luccafong
* [BugFix] Fix `plan` API Mismatch when using latest FlashInfer ([#29426](https://github.com/vllm-project/vllm/pull/29426)) by @askliar
* [Bugfix] Fix logic for choosing default prefix caching setting ([#29393](https://github.com/vllm-project/vllm/pull/29393)) by @tdoublep
* [Bugfix] Fix overallocation in MM profiling  ([#29386](https://github.com/vllm-project/vllm/pull/29386)) by @ywang96
* [BugFix] Use unique ids for different transcription prompts ([#29372](https://github.com/vllm-project/vllm/pull/29372)) by @njhill
* [Bugfix] [ROCm] [UX]: revert Flex attention backend ([#29371](https://github.com/vllm-project/vllm/pull/29371)) by @vllmellm
* Fix PoolingParams.skip_reading_prefix_cache type ([#29364](https://github.com/vllm-project/vllm/pull/29364)) by @kflu
* [BugFix] Fix duplicate req id tool-call race condition ([#29355](https://github.com/vllm-project/vllm/pull/29355)) by @njhill
* [Bugfix] Only use triton_kernels for MXFP4 on SM90 and SM100 ([#29339](https://github.com/vllm-project/vllm/pull/29339)) by @mgoin
* Fix RoPE related failures in Transformers nightly tests ([#29333](https://github.com/vllm-project/vllm/pull/29333)) by @hmellor
* [BugFix] Fix initialization of draft model.  ([#29319](https://github.com/vllm-project/vllm/pull/29319)) by @halyavin
* [BugFix] bad_words filtering ineffective when n > 1 ([#29313](https://github.com/vllm-project/vllm/pull/29313)) by @GOavi101
* [Bugfix] Make deprecated `--task embedding` consistent with `--runnerâ€¦ ([#29312](https://github.com/vllm-project/vllm/pull/29312)) by @maryamtahhan
* [BugFix] Fix R-VL model loading error ([#29299](https://github.com/vllm-project/vllm/pull/29299)) by @faaany
* [fix][cpu] Use a SwigluOAI impl which supports interleaved gate-up wei ([#29273](https://github.com/vllm-project/vllm/pull/29273)) by @fadara01
* [Bugfix] Update Gradio OpenAI Chatbot Webserver example to new Gradio message history format ([#29249](https://github.com/vllm-project/vllm/pull/29249)) by @joshiemoore
* [Bugfix] Use HF config fields as fallback when loading Mistral config ([#29239](https://github.com/vllm-project/vllm/pull/29239)) by @DarkLight1337
* Fix EVS crash when using `video_embeds` inputs in Qwen2.5-VL ([#29232](https://github.com/vllm-project/vllm/pull/29232)) by @skyloevil
* [BugFix] Fix returned logprobs with spec decode + prefill chunking ([#29216](https://github.com/vllm-project/vllm/pull/29216)) by @njhill
* Fix: Resolve circular import in model_loader/utils.py ([#29189](https://github.com/vllm-project/vllm/pull/29189)) by @nandan2003
* Fix mistral config ([#29172](https://github.com/vllm-project/vllm/pull/29172)) by @juliendenize
* [BugFix] EPLB + B200 + DeepGEMM : Handle column-major scales tensor ([#29162](https://github.com/vllm-project/vllm/pull/29162)) by @varun-sundar-rabindranath
* [Fix] Add SM check to flashinfer MOE backend ([#29144](https://github.com/vllm-project/vllm/pull/29144)) by @jiahanc
* [Bugfix] Fix default MM LoRA alignment for single str prompts ([#29140](https://github.com/vllm-project/vllm/pull/29140)) by @alex-jw-brooks
* [BugFix] skip combo kernel on cpu ([#29129](https://github.com/vllm-project/vllm/pull/29129)) by @BoyuanFeng
* [Bug] Fix torch warning of tf32 usage ([#29112](https://github.com/vllm-project/vllm/pull/29112)) by @yewentao256
* [BugFix] Fix missing symbol triggering FA2 fallback on Hopper ([#29107](https://github.com/vllm-project/vllm/pull/29107)) by @LucasWilkinson
* [BugFix] Fix Eagle `IndexError: list index out of range` for even `num_speculative_tokens` ([#29102](https://github.com/vllm-project/vllm/pull/29102)) by @LucasWilkinson
* [Bugfix] Fix block size in block_table with PCP ([#29094](https://github.com/vllm-project/vllm/pull/29094)) by @Livinfly
* [Bugfix] Fix Plamo3 rope handling ([#29092](https://github.com/vllm-project/vllm/pull/29092)) by @DarkLight1337
* [BugFix] Fix chunked prompt logprobs + preemption ([#29071](https://github.com/vllm-project/vllm/pull/29071)) by @njhill
* fix: clean up function never use in setup.py ([#29061](https://github.com/vllm-project/vllm/pull/29061)) by @yihong0618
* [Bugfix] Use lazy string reference for DeepseekV3Config in config registry ([#28958](https://github.com/vllm-project/vllm/pull/28958)) by @yongming-qin
* [Bugfix] If chunked_prefill is disabled, end the scheduling early. ([#28911](https://github.com/vllm-project/vllm/pull/28911)) by @noooop
* [Bugfix] Make compressed-tensors MoEs respect ignored layers ([#28878](https://github.com/vllm-project/vllm/pull/28878)) by @HDCharles
* [Bugfix] Fix GPT-OSS AR+NORM fusion ([#28841](https://github.com/vllm-project/vllm/pull/28841)) by @elvischenv
* [Bugfix]Fix a conditional to not check zero value ([#28754](https://github.com/vllm-project/vllm/pull/28754)) by @gmagogsfm
* [Bugfix] fix IMA issue in certain cases of the moe marlin kernel ([#28619](https://github.com/vllm-project/vllm/pull/28619)) by @jinzhen-lin
* fix cross attention ([#28346](https://github.com/vllm-project/vllm/pull/28346)) by @fsx950223
* [bugfix] avoid NIXL_ERR_REMOTE_DISCONNECT in nixl_connector when Prefill dies ([#28120](https://github.com/vllm-project/vllm/pull/28120)) by @hasB4K
* [Bugfix] properly handle nested json with llama3 tool parser ([#27701](https://github.com/vllm-project/vllm/pull/27701)) by @Aydin-ab
* [Bugfix][Rocm] Fix shared expert weight loading failure in DeepSeek-MTP ([#27563](https://github.com/vllm-project/vllm/pull/27563)) by @zhyajie
* [BugFix] Make sure to allocate worst case MoE workspace during profile run in the DP + EP case ([#27426](https://github.com/vllm-project/vllm/pull/27426)) by @LucasWilkinson
* [Bugfix] [ROCm] [UX] Reorganize ROCm Backend Selection Logic ([#26980](https://github.com/vllm-project/vllm/pull/26980)) by @vllmellm

### âš¡ï¸ Performance

* Optimize the wording of the document and unify the terminology and thâ€¦ ([#29491](https://github.com/vllm-project/vllm/pull/29491)) by @Adityayxt
* [Perf] Disable DeepGEMM MoE by default when TP=8 is used ([#29346](https://github.com/vllm-project/vllm/pull/29346)) by @mgoin
* [Perf] Optimize batch invariant BMM, 18.1% Throughput improvement, 10.7% TTFT improvement ([#29345](https://github.com/vllm-project/vllm/pull/29345)) by @yewentao256
* [Perf] use cpu all reduce to avoid sync when async_scheduling & dp > 1 ([#29311](https://github.com/vllm-project/vllm/pull/29311)) by @izhuhaoran
* [perf][cpu] Accelerate paged attention GEMMs (QK, PV) on Arm CPUs with NEON ([#29193](https://github.com/vllm-project/vllm/pull/29193)) by @fadara01
* [Perf][Deepseek] optimize gather_and_maybe_dequant_cache kernel's perf for extremely long sequence ([#28029](https://github.com/vllm-project/vllm/pull/28029)) by @ganyi1996ppo
* [Performance][MLA][ROCm] Remove redundant D2D copy in deepseek ([#27457](https://github.com/vllm-project/vllm/pull/27457)) by @ganyi1996ppo
* [Perf] These changes enhance the NUMA functionality of vllm for systems with more than one NUMA nodes per socket ([#25559](https://github.com/vllm-project/vllm/pull/25559)) by @skaraban3807

### ðŸ¤– Model Support

* [Model] Add HunyuanOCR support ([#29327](https://github.com/vllm-project/vllm/pull/29327)) by @Isotr0py
* [Model][Qwen3VL] Tune Triton w8a8 block fp8 kernel for L40s ([#29217](https://github.com/vllm-project/vllm/pull/29217)) by @lgeiger
* [Model] Add OpenCUA-7B support ([#29068](https://github.com/vllm-project/vllm/pull/29068)) by @lim4349

### ðŸ”Œ Hardware & Backend

* [ROCm][CI] Fix test_cpu_offloading for ROCm ([#29548](https://github.com/vllm-project/vllm/pull/29548)) by @micah-wil
* [ROCm][CI] Fix test_cudagraph_mode failure in AMD CI ([#29367](https://github.com/vllm-project/vllm/pull/29367)) by @micah-wil
* [XPU]fix Kimi-VL-A3B-thinking on xpu ([#29309](https://github.com/vllm-project/vllm/pull/29309)) by @yma11
* [XPU] upgrade torch & ipex 2.9 on XPU platform ([#29307](https://github.com/vllm-project/vllm/pull/29307)) by @jikunshang
* [ROCm][CI] Fix config/test_config_generation.py ([#29142](https://github.com/vllm-project/vllm/pull/29142)) by @charlifu
* [Rocm][CI] Fix DeekSeek V2-Lite Accuracy CI ([#29135](https://github.com/vllm-project/vllm/pull/29135)) by @charlifu
* [ROCm] Fix for import when building with upstream triton for gfx1100 for gpt-oss serving ([#29127](https://github.com/vllm-project/vllm/pull/29127)) by @hongxiayang
* [ROCm][CI] Fix "Cannot re-initialize CUDA in forked subprocess" in test_pynccl.py  ([#29119](https://github.com/vllm-project/vllm/pull/29119)) by @micah-wil
* [ROCm] Support for Whisper v1 with Aiter Unified Attention and Aiter Flash Attention ([#28376](https://github.com/vllm-project/vllm/pull/28376)) by @apinge
* [ROCm][MLA] enable fp8 MLA decode on ROCm ([#28032](https://github.com/vllm-project/vllm/pull/28032)) by @gbyu-amd
* [TPU] add tpu_inference ([#27277](https://github.com/vllm-project/vllm/pull/27277)) by @jcyang43

### âš™ï¸ Refactoring & Core

* Remove VLLM_SKIP_WARMUP tip ([#29331](https://github.com/vllm-project/vllm/pull/29331)) by @tlrmchlsmth
* [Core] Generalize Encoder-Decoder `seq_lens` computation to avoid Whisper hardcoded logic   ([#29268](https://github.com/vllm-project/vllm/pull/29268)) by @NickLucche
* [Core] Deprecate `xformers` ([#29262](https://github.com/vllm-project/vllm/pull/29262)) by @ywang96
* [Kernel] Add NVFP4 MoE CUTLASS support for SM120 ([#29242](https://github.com/vllm-project/vllm/pull/29242)) by @mgoin
* [Core] Support logprobs with spec decode + async scheduling  ([#29223](https://github.com/vllm-project/vllm/pull/29223)) by @njhill
* [Kernel] Use pre-allocated output buffer for triton kernel fused_experts ([#29219](https://github.com/vllm-project/vllm/pull/29219)) by @xyang16
* [Frontend][Responses API] Multi-turn (with type: "output_text") support for non-harmony requests ([#29175](https://github.com/vllm-project/vllm/pull/29175)) by @madskildegaard
* [Core] Add audio_embeds support to chat completions ([#29059](https://github.com/vllm-project/vllm/pull/29059)) by @jeremyteboul
* Simplify `from_blob` usage in `get_cuda_view_from_cpu_tensor` ([#29027](https://github.com/vllm-project/vllm/pull/29027)) by @janeyx99
* [refactor] CTConfig methods to static/class methods ([#28870](https://github.com/vllm-project/vllm/pull/28870)) by @HDCharles
* [Core] Refactor padding logic and pad for CUDA graphs before attention metadata building  ([#28579](https://github.com/vllm-project/vllm/pull/28579)) by @LucasWilkinson
* [kernel][perf] support uncontiguous input for rms_norm kernel ([#28103](https://github.com/vllm-project/vllm/pull/28103)) by @izhuhaoran
* Refactor: Move CUDA graph dispatch logic earlier ([#27382](https://github.com/vllm-project/vllm/pull/27382)) by @yiz-liu
* [Core] Align whisper closer to other multimodal models ([#27292](https://github.com/vllm-project/vllm/pull/27292)) by @russellb
* [Frontend][torch.compile] CompilationConfig Overhaul (#20283): Set up -O infrastructure ([#26847](https://github.com/vllm-project/vllm/pull/26847)) by @morrison-turnansky
* [Frontend] Respect Chat Completion parallel_tool_calls param ([#26233](https://github.com/vllm-project/vllm/pull/26233)) by @bbrowning

### ðŸ”§ Build, CI & Testing

* [CI] Auto label CPU related issues ([#29602](https://github.com/vllm-project/vllm/pull/29602)) by @bigPYJ1151
* Update Transformers pin in CI to 4.57.3 ([#29418](https://github.com/vllm-project/vllm/pull/29418)) by @hmellor
* [CI] Resettle pooling entrypoints tests.  ([#29370](https://github.com/vllm-project/vllm/pull/29370)) by @noooop
* [CI][ROCm] Install arctic-inference on ROCm tests ([#29344](https://github.com/vllm-project/vllm/pull/29344)) by @divakar-amd
* [CI] Add batched audios Whisper test ([#29308](https://github.com/vllm-project/vllm/pull/29308)) by @NickLucche
* [CI] Bug: Fix triton import issue ([#29202](https://github.com/vllm-project/vllm/pull/29202)) by @yewentao256
* [CI] Fix mypy for `vllm/v1/worker` ([#29037](https://github.com/vllm-project/vllm/pull/29037)) by @yewentao256
* [CI] Add batch invariant test to ci ([#27842](https://github.com/vllm-project/vllm/pull/27842)) by @yewentao256

### ðŸ“š Documentation

* [Docs] Improve `priority` parameter documentation ([#29572](https://github.com/vllm-project/vllm/pull/29572)) by @maang-h
* [DOC] Add vLLM Bangkok Meetup info ([#29561](https://github.com/vllm-project/vllm/pull/29561)) by @tjtanaa
* [Doc]: fixing typos in diverse files ([#29492](https://github.com/vllm-project/vllm/pull/29492)) by @didier-durand
* [Docs] Update supported models for Olmo 3 in tool calling documentation ([#29411](https://github.com/vllm-project/vllm/pull/29411)) by @wilsonwu
* [Doc]: fix typos in various files ([#29230](https://github.com/vllm-project/vllm/pull/29230)) by @didier-durand
* [Doc] Update more docs with respect to V1 ([#29188](https://github.com/vllm-project/vllm/pull/29188)) by @DarkLight1337
* docs: fixes distributed executor backend config for multi-node vllm ([#29173](https://github.com/vllm-project/vllm/pull/29173)) by @michaelact
* [docs] Fix cudagraph mode config ([#29170](https://github.com/vllm-project/vllm/pull/29170)) by @angelayi
* [Doc] cleanup TPU documentation and remove outdated examples ([#29048](https://github.com/vllm-project/vllm/pull/29048)) by @RobMulla
* [Doc] update installation guide regarding aarch64+cuda pytorch build ([#28875](https://github.com/vllm-project/vllm/pull/28875)) by @soodoshll
* [Doc] Update plugin doc ([#28532](https://github.com/vllm-project/vllm/pull/28532)) by @wangxiyuan

### ðŸ“¦ Miscellaneous

* [Model Runner V2][BugFix] Keep reference to GPU tensors in AsyncOutput ([#29623](https://github.com/vllm-project/vllm/pull/29623)) by @WoosukKwon
* [Deprecation] Advance deprecation status ([#29617](https://github.com/vllm-project/vllm/pull/29617)) by @DarkLight1337
* [Misc] Remove unused code from `protocol.py` ([#29616](https://github.com/vllm-project/vllm/pull/29616)) by @DarkLight1337
* [CI/Build][Bugfix] Fix auto label issues for CPU ([#29610](https://github.com/vllm-project/vllm/pull/29610)) by @bigPYJ1151
* [CPU]Update CPU PyTorch to 2.9.0 ([#29589](https://github.com/vllm-project/vllm/pull/29589)) by @scydas
* [Model Runner V2] Refactor CudaGraphManager ([#29583](https://github.com/vllm-project/vllm/pull/29583)) by @WoosukKwon
* [Model Runner V2] Minor cleanup for build_attn_metadata ([#29576](https://github.com/vllm-project/vllm/pull/29576)) by @WoosukKwon
* [Model Runner V2] Minor code cleanup ([#29570](https://github.com/vllm-project/vllm/pull/29570)) by @WoosukKwon
* [Model Runner V2] Implement multi-step Eagle with CUDA graph ([#29559](https://github.com/vllm-project/vllm/pull/29559)) by @WoosukKwon
* [CI/Build] Skip ray tests on ROCm ([#29556](https://github.com/vllm-project/vllm/pull/29556)) by @rjrock
* [cpu][fix] Fix Arm CI tests ([#29552](https://github.com/vllm-project/vllm/pull/29552)) by @fadara01
* [Attention] Update attention imports ([#29540](https://github.com/vllm-project/vllm/pull/29540)) by @MatthewBonanni
* Revert "[Bugfix] Fix GPT-OSS AR+NORM fusion (#28841)" ([#29483](https://github.com/vllm-project/vllm/pull/29483)) by @hl475
* [Misc] Allow LM only loading for Pixtral ([#29451](https://github.com/vllm-project/vllm/pull/29451)) by @ywang96
* [Attention][Async] Eliminate `seq_lens_cpu` in FlashAttention metadata building with DCP > 1 ([#29449](https://github.com/vllm-project/vllm/pull/29449)) by @MatthewBonanni
* Change warning logs to debug for unimplemented MXFP4 Linear/Attention ([#29441](https://github.com/vllm-project/vllm/pull/29441)) by @mgoin
* [caching] Add enable_prompt_embeds and cpu_offload_gb to compile hashes. ([#29435](https://github.com/vllm-project/vllm/pull/29435)) by @zhxchen17
* dummy run corner case ([#29433](https://github.com/vllm-project/vllm/pull/29433)) by @xieyangxu
* Attempt to fix GPU OOM in a spec-decoding test ([#29419](https://github.com/vllm-project/vllm/pull/29419)) by @eldarkurtic
* Scheduled removal of `override_pooler_config` and `disable_log_requests` ([#29402](https://github.com/vllm-project/vllm/pull/29402)) by @hmellor
* Make Transformers Nightly tests soft-fail and enable all tests ([#29401](https://github.com/vllm-project/vllm/pull/29401)) by @hmellor
* [responsesAPI][2] parse ResponseFunctionToolCallOutputItem ([#29383](https://github.com/vllm-project/vllm/pull/29383)) by @qandrew
* [Misc] Streamline unique id generation ([#29375](https://github.com/vllm-project/vllm/pull/29375)) by @njhill
* [responsesAPI][1] refactor construct_input_messages ([#29359](https://github.com/vllm-project/vllm/pull/29359)) by @qandrew
* [CI/Build] Pin torchgeo dependency for AMD ([#29353](https://github.com/vllm-project/vllm/pull/29353)) by @rjrock
* [UX] Raise error for attn backend of batch invariant ([#29348](https://github.com/vllm-project/vllm/pull/29348)) by @yewentao256
* [Model Runner V2] Simplify Eagle bookkeeping with num_rejected ([#29347](https://github.com/vllm-project/vllm/pull/29347)) by @WoosukKwon
* [Attention] Remove imports from `vllm/attention/__init__.py` ([#29342](https://github.com/vllm-project/vllm/pull/29342)) by @MatthewBonanni
* [Compile] Refactor. Move PostGradPassManager out of Compilation config ([#29340](https://github.com/vllm-project/vllm/pull/29340)) by @ilmarkov
* [CI/Test Fix] Fix CP tests on Blackwell ([#29338](https://github.com/vllm-project/vllm/pull/29338)) by @LucasWilkinson
* [UX] Put CUDA attention backend selection log into one line ([#29337](https://github.com/vllm-project/vllm/pull/29337)) by @mgoin
* [Tests] Verify gpt_oss package is installed in harmony tests ([#29336](https://github.com/vllm-project/vllm/pull/29336)) by @njhill
* [Model Runner V2] Add minor clarification comments for Eagle ([#29332](https://github.com/vllm-project/vllm/pull/29332)) by @WoosukKwon
* [Metrics] Scheduled removal of deprecated metrics ([#29330](https://github.com/vllm-project/vllm/pull/29330)) by @markmc
* [Model Runner V2] Change Numba AoT to JIT ([#29328](https://github.com/vllm-project/vllm/pull/29328)) by @WoosukKwon
* Scheduled removal of `guided_*` config fields ([#29326](https://github.com/vllm-project/vllm/pull/29326)) by @hmellor
* Scheduled removal of `ParallelConfig`'s direct child EPLB fields ([#29324](https://github.com/vllm-project/vllm/pull/29324)) by @hmellor
* Scheduled removal of `CompilationConfig.use_inductor` ([#29323](https://github.com/vllm-project/vllm/pull/29323)) by @hmellor
* [LoRA] Continue optimizing MoE LoRA weight loading ([#29322](https://github.com/vllm-project/vllm/pull/29322)) by @jeejeelee
* [NIXL] Use config to enable telemetry + NIXL version bump ([#29305](https://github.com/vllm-project/vllm/pull/29305)) by @NickLucche
* [Hybrid Allocator] Better layer padding strategy for gpt-oss eagle ([#29303](https://github.com/vllm-project/vllm/pull/29303)) by @heheda12345
* [Model Runner V2] Implement Single-step Eagle 1 ([#29300](https://github.com/vllm-project/vllm/pull/29300)) by @WoosukKwon
* Bump actions/checkout from 4 to 6 ([#29293](https://github.com/vllm-project/vllm/pull/29293)) by @app/dependabot
* [Misc] Suppress log outputs when constructing the default vllm config. ([#29291](https://github.com/vllm-project/vllm/pull/29291)) by @noooop
* [Model Runner V2] Add apply_temperature option to gumbel_sample ([#29276](https://github.com/vllm-project/vllm/pull/29276)) by @WoosukKwon
* [Model Runner V2] Optimize CUDA graph capture time ([#29275](https://github.com/vllm-project/vllm/pull/29275)) by @WoosukKwon
* [Model Runner V2] Support spec decoding [1/N] ([#29274](https://github.com/vllm-project/vllm/pull/29274)) by @WoosukKwon
* [CI/Build] Moves to cuda-base runtime image while retaining minimal JIT dependencies ([#29270](https://github.com/vllm-project/vllm/pull/29270)) by @bbartels
* Update KServe guide link in documentation ([#29258](https://github.com/vllm-project/vllm/pull/29258)) by @terrytangyuan
* [Model Runner V2] Minor fix for cudagraph_utils ([#29256](https://github.com/vllm-project/vllm/pull/29256)) by @WoosukKwon
* [CI/Build][AMD] Skip test_multi_shared_storage_connector_consistency  in test_multi_connector.py due to hipErrorLaunchFailure  when calling .cpu() ([#29253](https://github.com/vllm-project/vllm/pull/29253)) by @rasmith
* [CI/Build][AMD] Add check for flash_att_varlen_func to test_tree_attention.py ([#29252](https://github.com/vllm-project/vllm/pull/29252)) by @rasmith
* [UX] Suppress gloo log spam ([#29250](https://github.com/vllm-project/vllm/pull/29250)) by @mgoin
* [Chore] Update batch invariant code owner ([#29246](https://github.com/vllm-project/vllm/pull/29246)) by @yewentao256
* chore: add RTX_PRO_6000 GLM4.6-FP8 kernel tuning ([#29240](https://github.com/vllm-project/vllm/pull/29240)) by @coval3nte
* [Misc] Fix pre-commit ([#29238](https://github.com/vllm-project/vllm/pull/29238)) by @DarkLight1337
* [CI/Build Don't add FLASHINFER backend in test_cpu_offloading.py ([#29229](https://github.com/vllm-project/vllm/pull/29229)) by @rasmith
* [CI/Build] Skip tests that require libcudart in test_lmcache_integration.py ([#29228](https://github.com/vllm-project/vllm/pull/29228)) by @rasmith
* [LoRA] Optimize 3D MoE logic ([#29222](https://github.com/vllm-project/vllm/pull/29222)) by @jeejeelee
* [Model Runner V2] Limit cudagraph size to max decode batch size ([#29221](https://github.com/vllm-project/vllm/pull/29221)) by @WoosukKwon
* [CI/Build][AMD] Enable Entrypoints Integration Test (Pooling) to run without error on ROCm ([#29212](https://github.com/vllm-project/vllm/pull/29212)) by @rasmith
* [Model Runner V2] Optimize Gumbel Sampling Kernel ([#29210](https://github.com/vllm-project/vllm/pull/29210)) by @WoosukKwon
* [CI/Build] Add terratorch for AMD ([#29205](https://github.com/vllm-project/vllm/pull/29205)) by @rjrock
* [CI/Build] Disable test_gptoss_tp.py in 'LoRA TP Test' group for ROCm platform ([#29204](https://github.com/vllm-project/vllm/pull/29204)) by @qli88
* Display warning only when ROCm version is less than Pytorch required version ([#29200](https://github.com/vllm-project/vllm/pull/29200)) by @Inokinoki
* [Build/CI][DP/EP] Add QWen/Qwen3-30B-A3B-FP8 + EPLB tests to Nightly H100 and B200 ([#29195](https://github.com/vllm-project/vllm/pull/29195)) by @varun-sundar-rabindranath
* [Model Runner V2] Change bookkeeping logic in preparation for spec decoding ([#29194](https://github.com/vllm-project/vllm/pull/29194)) by @WoosukKwon
* [Chore] Fix pre-commit error after #25266 ([#29190](https://github.com/vllm-project/vllm/pull/29190)) by @WoosukKwon
* [LoRA] Cleanup FusedMoEWithLoRA ([#29187](https://github.com/vllm-project/vllm/pull/29187)) by @jeejeelee
* [Misc] Further clean up chunked prefill and prefix caching init ([#29186](https://github.com/vllm-project/vllm/pull/29186)) by @DarkLight1337
* [Deprecation] Deprecate `seed=None` ([#29185](https://github.com/vllm-project/vllm/pull/29185)) by @DarkLight1337
* Revert "Revert #28875 (#29159)" ([#29179](https://github.com/vllm-project/vllm/pull/29179)) by @DarkLight1337
* [Misc] Move dynamic seed initialization to `EngineArgs` ([#29165](https://github.com/vllm-project/vllm/pull/29165)) by @DarkLight1337
* [Misc] remove useless v1 env ([#29164](https://github.com/vllm-project/vllm/pull/29164)) by @david6666666
* Tool Call Parser logs should not contain user input / model output except on DEBUG ([#29160](https://github.com/vllm-project/vllm/pull/29160)) by @sfbemerk
* Revert #28875 ([#29159](https://github.com/vllm-project/vllm/pull/29159)) by @DarkLight1337
* Patch DeepEP when building docker image with CUDA 13 ([#29154](https://github.com/vllm-project/vllm/pull/29154)) by @soodoshll
* [Minor][Clean] Remove the legacy assertion in video ([#29150](https://github.com/vllm-project/vllm/pull/29150)) by @gcanlin
* [CI/Build] Only use supported types and features on ROCm in MoE kernel tests ([#29149](https://github.com/vllm-project/vllm/pull/29149)) by @rasmith
* [Hybrid Allocator] Support KV cache groups with different block_size ([#29143](https://github.com/vllm-project/vllm/pull/29143)) by @ivanium
* [CI/Build][Kernel][AMD] Move extra dim to after load in _fwd_kv_parallel in lighting_attn.py ([#29132](https://github.com/vllm-project/vllm/pull/29132)) by @rasmith
* [CI/Build] allow user modify pplx and deepep ref by ENV or command line ([#29131](https://github.com/vllm-project/vllm/pull/29131)) by @alec-flowers
* [AITER] [ROCm] Fix crash when loading llama4 model with old aiter version installed, fallback to forward_native implementation ([#29124](https://github.com/vllm-project/vllm/pull/29124)) by @xli
* [NIXL] Fix after virtual block_size for host_buffer with heter kv_layout ([#29122](https://github.com/vllm-project/vllm/pull/29122)) by @xuechendi
* Revert "[Redo] #26368 (#28771)" ([#29121](https://github.com/vllm-project/vllm/pull/29121)) by @Jialin
* [CI/Build] Fix illegal memory access and unsupported test in kernels/attention/test_cache.py ([#29118](https://github.com/vllm-project/vllm/pull/29118)) by @rasmith
* [CI Failure] Fix Gemma3 RoPE configuration for sliding attention layers ([#29111](https://github.com/vllm-project/vllm/pull/29111)) by @hl475
* [CI Bugfix] Fix Kernels DeepGEMM Test (H100) ([#29106](https://github.com/vllm-project/vllm/pull/29106)) by @mgoin
* [Attention] Add ROCM_AITER_MLA_SPARSE to attention backend registry ([#29103](https://github.com/vllm-project/vllm/pull/29103)) by @MatthewBonanni
* Update model references for OLMo3 ([#29099](https://github.com/vllm-project/vllm/pull/29099)) by @mgoin
* [V0 Deprecation] Remove `best_of` ([#29090](https://github.com/vllm-project/vllm/pull/29090)) by @DarkLight1337
* [V0 deprecation] Remove more V0 references ([#29088](https://github.com/vllm-project/vllm/pull/29088)) by @DarkLight1337
* Revert back to torch.equal over torch.allclose from #28819  ([#29086](https://github.com/vllm-project/vllm/pull/29086)) by @eldarkurtic
* [Attention] Refactor FA `block_size` limitations to hybrid models only  ([#29084](https://github.com/vllm-project/vllm/pull/29084)) by @NickLucche
* [chore][LMCache connector] Remove useless logs from lmcache connector ([#29069](https://github.com/vllm-project/vllm/pull/29069)) by @ApostaC
* [MoE][Refactor] Make select_experts a non-static method ([#29067](https://github.com/vllm-project/vllm/pull/29067)) by @bnellnm
* Handle triton kernel import exception  ([#29062](https://github.com/vllm-project/vllm/pull/29062)) by @hjh0119
* [Docker] Optimize Dockerfile: consolidate apt-get and reduce image size by ~200MB ([#29060](https://github.com/vllm-project/vllm/pull/29060)) by @princepride
* [Small] Capture AttributeError when checking ray dependency.  ([#29024](https://github.com/vllm-project/vllm/pull/29024)) by @huachenheli
* [LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 ([#28971](https://github.com/vllm-project/vllm/pull/28971)) by @xyang16
* Update Dockerfile to use gcc-toolset-14 and fix test case failures on power (ppc64le) ([#28957](https://github.com/vllm-project/vllm/pull/28957)) by @bhagyashrigai
* [Log] Optimize startup log ([#28948](https://github.com/vllm-project/vllm/pull/28948)) by @yewentao256
* [Attention] add `_cudagraph_support` for linear attention ([#28934](https://github.com/vllm-project/vllm/pull/28934)) by @ZJY0516
* [CPU][IBM Z] Fix BF16 support and vectorize math operations for s390x ([#28926](https://github.com/vllm-project/vllm/pull/28926)) by @R3hankhan123
* Upstream triton fp4 weight preshuffle ([#28888](https://github.com/vllm-project/vllm/pull/28888)) by @maleksan85
* [tiny] Remove unsupported TRITON_MLA backend from batch invariance ([#28832](https://github.com/vllm-project/vllm/pull/28832)) by @bwasti
* [Misc] Add backup hash algorithm for FIPS constrained environments ([#28795](https://github.com/vllm-project/vllm/pull/28795)) by @geodavic
* Default model load/config/tokenizer to `mistral` format if relevant files exist ([#28659](https://github.com/vllm-project/vllm/pull/28659)) by @juliendenize
* [log] add weights loading time log to sharded_state loader ([#28628](https://github.com/vllm-project/vllm/pull/28628)) by @andyxning
* Allow oot custom compiler extension via CompilerInterface ([#28623](https://github.com/vllm-project/vllm/pull/28623)) by @wxsIcey
* [KV Connector] Fix async connector prefix cache metrics ([#28585](https://github.com/vllm-project/vllm/pull/28585)) by @markmc
* [LoRA][2/2]Remove LoRA extra vocab  ([#28545](https://github.com/vllm-project/vllm/pull/28545)) by @jeejeelee
* [responsesAPI] parse reasoning item input ([#28248](https://github.com/vllm-project/vllm/pull/28248)) by @qandrew
* [Multimodal][Qwen3 Omni] Make Qwen3 Omni work with audio-in-video inputs in V1 engine.   ([#27721](https://github.com/vllm-project/vllm/pull/27721)) by @huachenheli
* [Spec Decode] Add support for EAGLE3 heads that do not use_aux_hidden_states ([#27688](https://github.com/vllm-project/vllm/pull/27688)) by @hjjq
* [CI/build] Removes source compilation from runtime image ([#26966](https://github.com/vllm-project/vllm/pull/26966)) by @bbartels
* [Spec-Decode][DP] EAGLE Support DP>1 ([#26086](https://github.com/vllm-project/vllm/pull/26086)) by @Flechman
* GPU Model Runner V2 ([#25266](https://github.com/vllm-project/vllm/pull/25266)) by @WoosukKwon
* [EPLB] Optimize EPLB for Async Rearrange Experts  ([#22179](https://github.com/vllm-project/vllm/pull/22179)) by @david6666666

## Contributors

@Adityayxt, @ApostaC, @Aydin-ab, @BoyuanFeng, @DarkLight1337, @Flechman, @FlintyLemming, @GOavi101, @HDCharles, @Inokinoki, @Isotr0py, @Jialin, @Livinfly, @LucasWilkinson, @MatthewBonanni, @NickLucche, @R3hankhan123, @RobMulla, @Victor49152, @WoosukKwon, @Yejing-Lai, @ZJY0516, @alec-flowers, @alex-jw-brooks, @andyxning, @angelayi, @apinge, @app/dependabot, @askliar, @bbartels, @bbrowning, @bhagyashrigai, @bigPYJ1151, @bnellnm, @bwasti, @charlifu, @coval3nte, @david6666666, @didier-durand, @divakar-amd, @eldarkurtic, @elvischenv, @faaany, @fadara01, @fsx950223, @ganyi1996ppo, @gbyu-amd, @gcanlin, @geodavic, @gmagogsfm, @guodongxiaren, @halyavin, @hasB4K, @heheda12345, @hjh0119, @hjjq, @hl475, @hmellor, @hongxiayang, @huachenheli, @ilmarkov, @ivanium, @izhuhaoran, @janeyx99, @jcyang43, @jeejeelee, @jeremyteboul, @jiahanc, @jikunshang, @jinzhen-lin, @joshiemoore, @juliendenize, @kflu, @laithsakka, @lengrongfu, @lgeiger, @lim4349, @louie-tsai, @luccafong, @maang-h, @madskildegaard, @maleksan85, @markmc, @maryamtahhan, @mgoin, @micah-wil, @michaelact, @morrison-turnansky, @nandan2003, @njhill, @noooop, @princepride, @qandrew, @qli88, @rasmith, @rjrock, @russellb, @scydas, @sfbemerk, @skaraban3807, @skyloevil, @soodoshll, @sts07142, @tdoublep, @terrytangyuan, @tjtanaa, @tlrmchlsmth, @varun-sundar-rabindranath, @vllmellm, @wangxiyuan, @wilsonwu, @wxsIcey, @xieyangxu, @xli, @xuechendi, @xyang16, @yewentao256, @yihong0618, @yiz-liu, @yma11, @yongming-qin, @ywang96, @zhxchen17, @zhyajie

