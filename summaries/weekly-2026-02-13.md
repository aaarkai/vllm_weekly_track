# Weekly Release Notes for vllm-project/vllm (2026-02-13)

## What's Changed

### ‚ú® Features & Enhancements

* Add config file for fused MoE for Nemotron (TP4, B200) ([#34411](https://github.com/vllm-project/vllm/pull/34411)) by @danisereb
* Add new sections to CODEOWNERS ([#34309](https://github.com/vllm-project/vllm/pull/34309)) by @DarkLight1337
* Add flagos in MiniCPM-o ([#34126](https://github.com/vllm-project/vllm/pull/34126)) by @tc-mb
* add --insecure arg to the vllm bench to skip TLS ([#34026](https://github.com/vllm-project/vllm/pull/34026)) by @fanyang-real
* Support benchmarking of Geospatial models  ([#33922](https://github.com/vllm-project/vllm/pull/33922)) by @mgazz
* support view_from_cpu_tensor on XPU ([#33868](https://github.com/vllm-project/vllm/pull/33868)) by @xinyu-intel
* Add support for ModelOpt MXFP8 dense models ([#33786](https://github.com/vllm-project/vllm/pull/33786)) by @danisereb
* [Feature] Warn about unrecognized environment variables ([#33581](https://github.com/vllm-project/vllm/pull/33581)) by @gshtras
* Add embedding input functionality for disabled modalities [remake] ([#32493](https://github.com/vllm-project/vllm/pull/32493)) by @reaganjlee
* Add NUMA Core binding in nixl_connector for CPU xPyD ([#32365](https://github.com/vllm-project/vllm/pull/32365)) by @ZhengHongming888
* [Feat][RL] Pause and Resume with keep requests for single engine ([#32351](https://github.com/vllm-project/vllm/pull/32351)) by @hao-aaron
* [Feature] OTEL tracing during loading ([#31162](https://github.com/vllm-project/vllm/pull/31162)) by @emricksini-h

### üêõ Bug Fixes

* Fix num_logprobs parameter description in sampler.py ([#34451](https://github.com/vllm-project/vllm/pull/34451)) by @zhuohan123
* [BugFix] Add block_size validation for mamba cache align mode ([#34445](https://github.com/vllm-project/vllm/pull/34445)) by @peakcrosser7
* [Bugfix] Remove assert that's no longer valid ([#34443](https://github.com/vllm-project/vllm/pull/34443)) by @bnellnm
* Fix MoE for the Transformers modelling backend ([#34436](https://github.com/vllm-project/vllm/pull/34436)) by @hmellor
* [Bugfix] Remove broken raw url GGUF model loading support ([#34433](https://github.com/vllm-project/vllm/pull/34433)) by @Isotr0py
* [Bugfix] Delete unused redundant code in Kimi-K2.5 ([#34427](https://github.com/vllm-project/vllm/pull/34427)) by @LoganJane
* [bugfix] refactor FunASR's _get_data_parser  ([#34397](https://github.com/vllm-project/vllm/pull/34397)) by @AllenDou
* [Bugfix] Fix MTP accuracy for GLM-5 ([#34385](https://github.com/vllm-project/vllm/pull/34385)) by @mgoin
* [BUG] Reset running requests when clearing cache for pause/resume ([#34382](https://github.com/vllm-project/vllm/pull/34382)) by @hao-aaron
* [BugFix] Fix DP chunking  ([#34379](https://github.com/vllm-project/vllm/pull/34379)) by @LucasWilkinson
* [Bugfix] Enforce DeepGEMM when using sparse_attn_indexer on CUDA ([#34374](https://github.com/vllm-project/vllm/pull/34374)) by @mgoin
* [Bugfix] Fix some issues with MoERunner PR #32344 ([#34371](https://github.com/vllm-project/vllm/pull/34371)) by @bnellnm
* [Bugfix] fix default is_neox_style to be True for deepseekv3.2 ([#34353](https://github.com/vllm-project/vllm/pull/34353)) by @xyDong0223
* [Bugfix] Fix more multimodal tests for transformers V5 ([#34334](https://github.com/vllm-project/vllm/pull/34334)) by @zucchini-nlp
* [Bugfix][CPU] Fix llama4 inference on CPU ([#34321](https://github.com/vllm-project/vllm/pull/34321)) by @bigPYJ1151
* Fix CI failure - Flashinfer Kernel tests ([#34316](https://github.com/vllm-project/vllm/pull/34316)) by @wzhao18
* [Bugfix] Fix weight naming in Qwen3.5 ([#34313](https://github.com/vllm-project/vllm/pull/34313)) by @ywang96
* [Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides ([#34279](https://github.com/vllm-project/vllm/pull/34279)) by @tlrmchlsmth
* [Bugfix] Enable attn quantization of Llama-4 by correctly permuting scales for rope (int8, fp8) ([#34243](https://github.com/vllm-project/vllm/pull/34243)) by @eldarkurtic
* [Bugfix] Fix FI kernel`chunk_gated_delta_rule` output shape for Qwen3.5 ([#34219](https://github.com/vllm-project/vllm/pull/34219)) by @ywang96
* [Bugfix] Fix `--trust-remote-code` conflict ([#34218](https://github.com/vllm-project/vllm/pull/34218)) by @DarkLight1337
* [Bugfix] Add `--trust-remote-code` to dataset bench args ([#34208](https://github.com/vllm-project/vllm/pull/34208)) by @DarkLight1337
* [Bugfix] Fix mamba cache dtype for Qwen3.5 ([#34200](https://github.com/vllm-project/vllm/pull/34200)) by @ywang96
* [Bugfix] Adopt `ChunkGatedDeltaRule` for Qwen3.5 ([#34198](https://github.com/vllm-project/vllm/pull/34198)) by @ywang96
* [Bugfix] Sort hf_weights_files in fastsafetensors_weights_iterator to match #33491 ([#34190](https://github.com/vllm-project/vllm/pull/34190)) by @jaim12005
* [Bugfix] Fix DP Attention Padding in Dummy Run ([#34187](https://github.com/vllm-project/vllm/pull/34187)) by @LucasWilkinson
* [Bugfix][Core] Fix CPU memory leak from Request reference cycle in prefix caching ([#34183](https://github.com/vllm-project/vllm/pull/34183)) by @ywang96
* [Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementation on ROCm if the new API is not available ([#34153](https://github.com/vllm-project/vllm/pull/34153)) by @gshtras
* [Bugfix] Fix benchmark_moe.py inplace assertion with torch >= 2.9 ([#34149](https://github.com/vllm-project/vllm/pull/34149)) by @mgehre-amd
* [Bugfix] Avoid duplicate k-proj weight emission in helper ([#34142](https://github.com/vllm-project/vllm/pull/34142)) by @artuskg
* [Bugfix] Voxtral prompt/audio placeholder alignment ([#34140](https://github.com/vllm-project/vllm/pull/34140)) by @artuskg
* Fix Mistral config remap to accept compressed-tensors quantization #34028 ([#34104](https://github.com/vllm-project/vllm/pull/34104)) by @baonudesifeizhai
* [BugFix] Change support no act and mul for marlin ([#34088](https://github.com/vllm-project/vllm/pull/34088)) by @TomerBN-Nvidia
* [Bugfix] Fix shared expert input for latent MoE in EP+DP (Nemotron-H) ([#34087](https://github.com/vllm-project/vllm/pull/34087)) by @TomerBN-Nvidia
* Fix DeepSeek-OCR tensor validation for all size variants ([#34085](https://github.com/vllm-project/vllm/pull/34085)) by @yichuan-w
* [BUGFIX] Fix accuracy bugs in Qwen3-Next MTP ([#34077](https://github.com/vllm-project/vllm/pull/34077)) by @vadiklyutiy
* [BugFix] Fix `fastsafetensors` TP all procs using all GPUs ([#34070](https://github.com/vllm-project/vllm/pull/34070)) by @njhill
* [Bugfix] Fix Worker.load_model context-manager composition for sleep mode ([#34021](https://github.com/vllm-project/vllm/pull/34021)) by @tianshu-Michael-yu
* [Bugfix] Fix Whisper tokenization ([#34011](https://github.com/vllm-project/vllm/pull/34011)) by @NickLucche
* [Fix] Fix `logprobs=0` handling for `/inference/v1/generate` endpoint ([#34010](https://github.com/vllm-project/vllm/pull/34010)) by @SumanthRH
* fix description in plugin_system.md ([#33999](https://github.com/vllm-project/vllm/pull/33999)) by @guodongxiaren
* [Bugfix] Fix no attribute error of SharedFusedMoE (DeepSeek-V3.1 as test model) ([#33993](https://github.com/vllm-project/vllm/pull/33993)) by @xuebwang-amd
* Fix spelling errors ([#33978](https://github.com/vllm-project/vllm/pull/33978)) by @sleepcoo
* [Bugfix] Fix models and tests for transformers v5 ([#33977](https://github.com/vllm-project/vllm/pull/33977)) by @zucchini-nlp
* Fix `main` pre-commit ([#33975](https://github.com/vllm-project/vllm/pull/33975)) by @hmellor
* [Bugfix] Fix QK Norm+RoPE fusion pattern matching on B200+FP8 ([#33967](https://github.com/vllm-project/vllm/pull/33967)) by @ikchifo
* [Bugfix] Fix the issue where tool calling does not work when using fast detokenization with dsv32 ([#33964](https://github.com/vllm-project/vllm/pull/33964)) by @chaunceyjiang
* [Bugfix] send None sentinel on final commit so server properly sends transcription.done ([#33963](https://github.com/vllm-project/vllm/pull/33963)) by @pjs102793
* [Bugfix]: Fix ROCm fusion attn test; use AttentionBackend utils to create kv cache ([#33948](https://github.com/vllm-project/vllm/pull/33948)) by @Rohan138
* [bugfix] [ROCm] Fix premature CUDA initialization in platform detection ([#33941](https://github.com/vllm-project/vllm/pull/33941)) by @kouroshHakha
* Fix RoutingMethodType logic ([#33919](https://github.com/vllm-project/vllm/pull/33919)) by @dbari
* [Bugfix] Fix Random Dataset Prefix Length Inaccuracy ([#33907](https://github.com/vllm-project/vllm/pull/33907)) by @frankwang28
* [Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN matmul ([#33901](https://github.com/vllm-project/vllm/pull/33901)) by @nikhil-arm
* [Bugfix][DeepSeek-V3.2] fix fp8 kvcache type cast ([#33884](https://github.com/vllm-project/vllm/pull/33884)) by @kebe7jun
* [Bugfix] Fix _fused_moe_lora_expand signature mismatch ([#33821](https://github.com/vllm-project/vllm/pull/33821)) by @xyang16
* [FIX] guidance: use max(vocab_size, len(tokenizer)) for n_vocab ([#33509](https://github.com/vllm-project/vllm/pull/33509)) by @FredericOdermatt
* [Bugfix] Fix Sparse24 Compressed Tensors models ([#33446](https://github.com/vllm-project/vllm/pull/33446)) by @kylesayrs
* [Bugfix] Fix weights offloading for sleep mode ([#32947](https://github.com/vllm-project/vllm/pull/32947)) by @jseppanen
* [BugFix] Fix async EPLB hang with DeepEP LL all2all backend ([#32860](https://github.com/vllm-project/vllm/pull/32860)) by @ilmarkov
* [Bugfix] Fix memory inconsistency in cross-process shared memory ([#32022](https://github.com/vllm-project/vllm/pull/32022)) by @slippersss
* [Bugfix][Model] Support LoRA on Qwen3 Output Embedding ([#29816](https://github.com/vllm-project/vllm/pull/29816)) by @klshuster
* [BugFix] Avoid prefix cache hit in the same schedule step for mamba layers ([#29387](https://github.com/vllm-project/vllm/pull/29387)) by @heheda12345

### ‚ö°Ô∏è Performance

* [Perf] Simplify DeepseekV32 tokenizer, ensure fast detokenization used ([#33855](https://github.com/vllm-project/vllm/pull/33855)) by @njhill
* [Perf][Kernel] Add faster topKperRow decode kernel for DeepSeek-V3.2 sparse attention ([#33680](https://github.com/vllm-project/vllm/pull/33680)) by @LopezCastroRoberto
* [Perf] Disable clean_logits in deepgemm fp8_mqa_logits kernel ([#33568](https://github.com/vllm-project/vllm/pull/33568)) by @xyang16
* [Perf] Optimize detokenizer python logic ([#32975](https://github.com/vllm-project/vllm/pull/32975)) by @yewentao256
* [Perf] Move eplb rebalance algo to async thread ([#30888](https://github.com/vllm-project/vllm/pull/30888)) by @ilmarkov

### ü§ñ Model Support

* [GPT-OSS] Remove unnecessary contiguous ([#34337](https://github.com/vllm-project/vllm/pull/34337)) by @elvischenv
* [Model] GLM adaptation ([#34124](https://github.com/vllm-project/vllm/pull/34124)) by @jeejeelee
* [MODEL] Adding Support for Qwen3.5 Models ([#34110](https://github.com/vllm-project/vllm/pull/34110)) by @JJJYmmm
* [Model] Enable Step3p5ForCausalLM testing ([#33755](https://github.com/vllm-project/vllm/pull/33755)) by @jeejeelee
* [Model] Support MiniCPM-o 4.5 ([#33431](https://github.com/vllm-project/vllm/pull/33431)) by @tc-mb
* [model] support FunASR model ([#33247](https://github.com/vllm-project/vllm/pull/33247)) by @AllenDou
* glm 4.6 fused tuned inference config for B200 ([#32958](https://github.com/vllm-project/vllm/pull/32958)) by @navmarri14

### üîå Hardware & Backend

* [ROCm][CI] Pin TorchCodec to v0.10.0 for ROCm compatibility ([#34447](https://github.com/vllm-project/vllm/pull/34447)) by @AndreasKaratzas
* [ROCm][quantization] improve OCP weight quant parser robust ([#34431](https://github.com/vllm-project/vllm/pull/34431)) by @xuebwang-amd
* [ROCm][CI] Revert Test Groups From mi325_8 to mi325_1 Agent Pool In AMD CI ([#34384](https://github.com/vllm-project/vllm/pull/34384)) by @micah-wil
* [ROCm] [CI] fix test_unrecognized_env ([#34350](https://github.com/vllm-project/vllm/pull/34350)) by @tjtanaa
* [ROCm][CI] Fix test_sequence_parallel.py location in AMD CI pipeline ([#34280](https://github.com/vllm-project/vllm/pull/34280)) by @micah-wil
* [XPU][7/N] enable xpu fp8 moe ([#34202](https://github.com/vllm-project/vllm/pull/34202)) by @zufangzhu
* [ROCm] Enable MXFP4 MoE weight pre-shuffling on gfx950 and update aiter ([#34192](https://github.com/vllm-project/vllm/pull/34192)) by @dllehr-amd
* [XPU][6/N] add xpu scaled_mm kernel ([#34117](https://github.com/vllm-project/vllm/pull/34117)) by @zufangzhu
* [XPU][9/N] clean up existing ipex code/doc ([#34111](https://github.com/vllm-project/vllm/pull/34111)) by @jikunshang
* [ROCm][Bugfix] Resolve Dynamo tracing crash from amdsmi calls in on_gfx* arch detection ([#34108](https://github.com/vllm-project/vllm/pull/34108)) by @AndreasKaratzas
* [ROCm][Bugfix] fix act_quant_fusion module import error ([#34069](https://github.com/vllm-project/vllm/pull/34069)) by @AndreasKaratzas
* [ROCm] [CI] Reduce Resource of two test groups ([#34059](https://github.com/vllm-project/vllm/pull/34059)) by @tjtanaa
* [ROCm][CI] Fix serving tokens test failures ([#34047](https://github.com/vllm-project/vllm/pull/34047)) by @AndreasKaratzas
* [ROCm][CI] Pinning lm-eval version to resolve multi-modal small eval bug ([#34038](https://github.com/vllm-project/vllm/pull/34038)) by @AndreasKaratzas
* [ROCm] update triton branch to support gpt-oss models for gfx11xx devices ([#34032](https://github.com/vllm-project/vllm/pull/34032)) by @hongxiayang
* [XPU][5/N] add wna16 xpu kernel ([#33973](https://github.com/vllm-project/vllm/pull/33973)) by @zufangzhu
* [ROCm][AITER] Fix AITER import regression for explicit backend selection ([#33749](https://github.com/vllm-project/vllm/pull/33749)) by @AndreasKaratzas
* [Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op ([#33734](https://github.com/vllm-project/vllm/pull/33734)) by @charlifu
* [NVIDIA][test] Tests for flashinfer TRTLLM BF16 MoE ([#33715](https://github.com/vllm-project/vllm/pull/33715)) by @Linda-Stadter
* [ROCm] [aiter] Split KV cache update for AiterFlashAttention ([#33681](https://github.com/vllm-project/vllm/pull/33681)) by @kliuae
* [XPU][4/N] add mxfp4 moe model support ([#33679](https://github.com/vllm-project/vllm/pull/33679)) by @jikunshang
* [XPU]Replace pip in docker.xpu with uv pip ([#31112](https://github.com/vllm-project/vllm/pull/31112)) by @1643661061leo
* [ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations  ([#29008](https://github.com/vllm-project/vllm/pull/29008)) by @xuebwang-amd

### ‚öôÔ∏è Refactoring & Core

* [Refactor] Simplify BOS/EOS token handling ([#34435](https://github.com/vllm-project/vllm/pull/34435)) by @DarkLight1337
* [Refactor] Move validation to params definitions ([#34362](https://github.com/vllm-project/vllm/pull/34362)) by @DarkLight1337
* [Refactor] Pass Renderer to Input Processor ([#34329](https://github.com/vllm-project/vllm/pull/34329)) by @DarkLight1337
* [Frontend] Exploit tokenizers "new stream" in FastIncrementalDetokenizer ([#34217](https://github.com/vllm-project/vllm/pull/34217)) by @njhill
* [Frontend][CI]  Consolidate instrumentator entrypoints ([#34123](https://github.com/vllm-project/vllm/pull/34123)) by @noooop
* [Kernel] [Helion] [5/N] Add Helion Autotuning infrastructure ([#34025](https://github.com/vllm-project/vllm/pull/34025)) by @gmagogsfm
* [Kernel] Add KernelConfig flag to enable/disable FlashInfer autotune ([#34006](https://github.com/vllm-project/vllm/pull/34006)) by @mmangkad
* [Kernel] FlashInfer: switch allreduce fusion to unified API ([#33985](https://github.com/vllm-project/vllm/pull/33985)) by @mmangkad
* [Frontend]Add support for transcriptions and translations to run_batch ([#33934](https://github.com/vllm-project/vllm/pull/33934)) by @pooyadavoodi
* [Refactor] Consolidate sequence normalization and enc-dec parsing ([#33928](https://github.com/vllm-project/vllm/pull/33928)) by @DarkLight1337
* [Refactor] Replace `activation: str` with `MoEActivation` enum ([#33843](https://github.com/vllm-project/vllm/pull/33843)) by @mgoin
* [Frontend] Enable generic structured_outputs for responses API ([#33709](https://github.com/vllm-project/vllm/pull/33709)) by @alecsolder
* [Core][BugFix] Fix PP KV cache sharding memory validation ([#33698](https://github.com/vllm-project/vllm/pull/33698)) by @junuxyz
* [Kernel] Add enable_sm120_or_later for SM121 (DGX Spark) CUTLASS support ([#33517](https://github.com/vllm-project/vllm/pull/33517)) by @Code4me2
* [Kernel] Support Flashinfer trtllm fused MoE non gated FP8 & NVFP4 ([#33506](https://github.com/vllm-project/vllm/pull/33506)) by @amitz-nv
* [Refactor] Remove align block size logic in `moe_permute` ([#33449](https://github.com/vllm-project/vllm/pull/33449)) by @yewentao256
* [Kernel] [Helion] [4/N] Add silu_mul_fp8 Helion kernel  ([#33373](https://github.com/vllm-project/vllm/pull/33373)) by @gmagogsfm
* [Core] Profiler improvements and lazy initialization ([#33198](https://github.com/vllm-project/vllm/pull/33198)) by @jaewonlee-fb
* [Core] Add sleep level 0 mode with enqueue/wait pattern ([#33195](https://github.com/vllm-project/vllm/pull/33195)) by @jaewonlee-fb
* [Kernel] Apply 256bit LDG/STG To Activation Kernels ([#33022](https://github.com/vllm-project/vllm/pull/33022)) by @AstroVoyager7
* [Kernel] use flashinfer for gdn prefill ([#32846](https://github.com/vllm-project/vllm/pull/32846)) by @ZJY0516
* [Frontend][last/5] Make pooling entrypoints request schema consensus.  ([#31127](https://github.com/vllm-project/vllm/pull/31127)) by @noooop

### üîß Build, CI & Testing

* [CI] Remove empty image_size_factors for fuyu, glm4_1v, glm_ocr ([#34107](https://github.com/vllm-project/vllm/pull/34107)) by @AndreasKaratzas
* [CI][Build]  Pin grpcio-tools==1.78.0 ([#34048](https://github.com/vllm-project/vllm/pull/34048)) by @noooop
* [CI][torch.compile] Fix incorrect filtering for E2E fusion tests on B200 ([#34031](https://github.com/vllm-project/vllm/pull/34031)) by @ProExpertProg
* [CI][AMD]Bugfix] Check that model_config is not None in enable_norm_pad_fusion ([#34007](https://github.com/vllm-project/vllm/pull/34007)) by @rasmith
* [ci] Integrate AMD tests into CI ([#33626](https://github.com/vllm-project/vllm/pull/33626)) by @khluu
* [CI] Add pip caching to cleanup_pr_body workflow ([#32979](https://github.com/vllm-project/vllm/pull/32979)) by @sjhddh
* [CI][BugFix] Fix silent failure in shellcheck hook and baseline exist‚Ä¶ ([#32458](https://github.com/vllm-project/vllm/pull/32458)) by @junuxyz

### üìö Documentation

* [Docs] Spec decoding docs warning removal ([#34439](https://github.com/vllm-project/vllm/pull/34439)) by @NickLucche
* [Docs] Fix typo ("defult") and double spacing ([#34348](https://github.com/vllm-project/vllm/pull/34348)) by @SorenDreano
* [Doc] Update Marlin support matrix for Turing ([#34319](https://github.com/vllm-project/vllm/pull/34319)) by @iori2333
* [Docs] Reduce time spent generating API docs ([#34255](https://github.com/vllm-project/vllm/pull/34255)) by @hmellor
* [Docs] Speed up build environment set-up  ([#34240](https://github.com/vllm-project/vllm/pull/34240)) by @hmellor
* [Doc] Update usage of `--limit-mm-per-prompt` ([#34148](https://github.com/vllm-project/vllm/pull/34148)) by @DarkLight1337
* [Docs] Fix format error in KV load failure recovery doc ([#34137](https://github.com/vllm-project/vllm/pull/34137)) by @zzaebok
* [Doc] Fix run_batch docs ([#34056](https://github.com/vllm-project/vllm/pull/34056)) by @DarkLight1337
* [DOC] [ROCm] Update docker deployment doc ([#33971](https://github.com/vllm-project/vllm/pull/33971)) by @vllmellm
* [Docs] Add reo analytics ([#33957](https://github.com/vllm-project/vllm/pull/33957)) by @simon-mo
* [Docs] Add sections on process architecture and minimum CPU resources ([#33940](https://github.com/vllm-project/vllm/pull/33940)) by @mgoin
* [Doc] Add DCP support to attention backend doc ([#33936](https://github.com/vllm-project/vllm/pull/33936)) by @mgoin
* [Docs] Improve documentation ([#33799](https://github.com/vllm-project/vllm/pull/33799)) by @SorenDreano
* [Docs] Update link to Benchmark CLI documentation ([#33254](https://github.com/vllm-project/vllm/pull/33254)) by @eldarkurtic

### üì¶ Miscellaneous

* [CI/Build] Update video URLs for testing ([#34446](https://github.com/vllm-project/vllm/pull/34446)) by @DarkLight1337
* [Voxtral Realtime] Refactor & Improve buffering logic ([#34428](https://github.com/vllm-project/vllm/pull/34428)) by @patrickvonplaten
* small adjustment to wvSplitKrc ([#34410](https://github.com/vllm-project/vllm/pull/34410)) by @amd-hhashemi
* [V0 Deprecation] Remove code related to per-request logits processors ([#34400](https://github.com/vllm-project/vllm/pull/34400)) by @DarkLight1337
* Use paged_attention_v1 for sliding window decode in rocm_aiter_fa ([#34378](https://github.com/vllm-project/vllm/pull/34378)) by @iseeyuan
* Don't try and run GLM-ASR with remote code ([#34352](https://github.com/vllm-project/vllm/pull/34352)) by @hmellor
* [Benchmarks] Reduce ready checker log verbosity ([#34349](https://github.com/vllm-project/vllm/pull/34349)) by @tomasruizt
* [Multimodal] Expose `mm_processor_kwargs` for `DummyInputsBuilder` ([#34330](https://github.com/vllm-project/vllm/pull/34330)) by @Isotr0py
* [Chore] Move `BaseRenderer` to `base.py` ([#34308](https://github.com/vllm-project/vllm/pull/34308)) by @DarkLight1337
* [torch.compile] Enable AR+rms fusion by default available for `-O2` ([#34299](https://github.com/vllm-project/vllm/pull/34299)) by @ProExpertProg
* [ModelBash][DSR1 NVFp4] Avoid Bf16 Bias Cast ([#34298](https://github.com/vllm-project/vllm/pull/34298)) by @robertgshaw2-redhat
* [Misc] Bump `fastsafetensors` version for latest fixes ([#34273](https://github.com/vllm-project/vllm/pull/34273)) by @njhill
* [Misc] Add pre-commit hook to catch boolean ops in with-statements ([#34271](https://github.com/vllm-project/vllm/pull/34271)) by @tlrmchlsmth
* [Benchmarks] Fix attention benchmark smoke test ([#34269](https://github.com/vllm-project/vllm/pull/34269)) by @MatthewBonanni
* Responses harmony system message structured ([#34268](https://github.com/vllm-project/vllm/pull/34268)) by @Kimahriman
* Make JAIS compatible with Transformers v5 ([#34264](https://github.com/vllm-project/vllm/pull/34264)) by @hmellor
* Make Qwen3VL compatible with Transformers v5 ([#34262](https://github.com/vllm-project/vllm/pull/34262)) by @hmellor
* Patch protobuf for CVE-2026-0994 ([#34253](https://github.com/vllm-project/vllm/pull/34253)) by @eicherseiji
* [Redo] Add `--trust-remote-code` to dataset bench args ([#34251](https://github.com/vllm-project/vllm/pull/34251)) by @DarkLight1337
* Minor cleanup for Voxtral ([#34247](https://github.com/vllm-project/vllm/pull/34247)) by @andylolu2
* [Plugin] Simplify IO Processor Plugin interface ([#34236](https://github.com/vllm-project/vllm/pull/34236)) by @DarkLight1337
* Stop testing for slow tokenizers as they will not exist soon ([#34235](https://github.com/vllm-project/vllm/pull/34235)) by @hmellor
* Bump `mamba-ssm` version in CI for Transformers v5 compatibility ([#34233](https://github.com/vllm-project/vllm/pull/34233)) by @hmellor
* [Model Runner V2] Use pinned memory for write_contents ([#34222](https://github.com/vllm-project/vllm/pull/34222)) by @WoosukKwon
* [V1][BugFix] Fix EAGLE3 encoder cache miss with disable_chunked_mm_input ([#34220](https://github.com/vllm-project/vllm/pull/34220)) by @KrxGu
* Revert #34208 ([#34216](https://github.com/vllm-project/vllm/pull/34216)) by @DarkLight1337
* [Misc] allow specify is_mm_prefix_lm in hf_config ([#34215](https://github.com/vllm-project/vllm/pull/34215)) by @lkhphuc
* [CI/Build] Relax `test_mcp_tool_call` ([#34204](https://github.com/vllm-project/vllm/pull/34204)) by @DarkLight1337
* [responsesAPI] fix simpleContext streaming output_messages ([#34188](https://github.com/vllm-project/vllm/pull/34188)) by @qandrew
* [Misc] Introduce ec_both role EC (encoder cache) connector ([#34182](https://github.com/vllm-project/vllm/pull/34182)) by @furionw
* [LMCache] Token Base IPC API ([#34175](https://github.com/vllm-project/vllm/pull/34175)) by @Oasis-Git
* [ModelRunner V2][BugFix] Fix `max_query_len` calculation ([#34167](https://github.com/vllm-project/vllm/pull/34167)) by @njhill
* [compile] Enable AOT compile with 2.10 in trunk. ([#34155](https://github.com/vllm-project/vllm/pull/34155)) by @zhxchen17
* [UX nit] Fix non-default api_server_count message ([#34152](https://github.com/vllm-project/vllm/pull/34152)) by @mgoin
* [Misc] Clean up validation logic in input processor ([#34144](https://github.com/vllm-project/vllm/pull/34144)) by @DarkLight1337
* Vllm CPU benchmark suite improvement ([#34128](https://github.com/vllm-project/vllm/pull/34128)) by @louie-tsai
* [UX] Add `--language-model-only` for hybrid models ([#34120](https://github.com/vllm-project/vllm/pull/34120)) by @ywang96
* [CPU] Enable FP16 (Half dtype) support for s390x ([#34116](https://github.com/vllm-project/vllm/pull/34116)) by @R3hankhan123
* [Tiny] Rename encoder budget file to more specific name  ([#34103](https://github.com/vllm-project/vllm/pull/34103)) by @reaganjlee
* [torch.compile] Stop doing unnecessary FakeTensorProp in PiecewiseCompileInterpreter ([#34093](https://github.com/vllm-project/vllm/pull/34093)) by @zou3519
* [torch.compile] Disable recursive pre_grad_passes ([#34092](https://github.com/vllm-project/vllm/pull/34092)) by @zou3519
* Convert online APIs to use Renderer  ([#34084](https://github.com/vllm-project/vllm/pull/34084)) by @reaganjlee
* [CI/Build] Skip GCS test ([#34057](https://github.com/vllm-project/vllm/pull/34057)) by @DarkLight1337
* fix(cpu): fix mla_decode compilation on x86 without AVX512 ([#34052](https://github.com/vllm-project/vllm/pull/34052)) by @ihb2032
* Reapply [Attention][FA3] Update FA3 to include new swizzle optimization ([#34043](https://github.com/vllm-project/vllm/pull/34043)) by @LucasWilkinson
* [Renderer] Define `render_cmpl` and `render_chat` ([#34039](https://github.com/vllm-project/vllm/pull/34039)) by @DarkLight1337
* [Misc] Simplify `get_max_tokens` ([#34036](https://github.com/vllm-project/vllm/pull/34036)) by @DarkLight1337
* [Misc] Make `PlaceholderRange.get_num_embeds` a method ([#34035](https://github.com/vllm-project/vllm/pull/34035)) by @DarkLight1337
* [bug-fix] supported_tasks is breaking backward compatibility at init_app_state ([#34027](https://github.com/vllm-project/vllm/pull/34027)) by @kouroshHakha
* [Misc][Spec Decode] support different load config for draft model ([#34022](https://github.com/vllm-project/vllm/pull/34022)) by @ZhengkaiZ
* [ModelRunner V2] Revert token rank comparison difference for now ([#34017](https://github.com/vllm-project/vllm/pull/34017)) by @njhill
* [Misc] Add backward-compatible import aliases for renamed translations module ([#34015](https://github.com/vllm-project/vllm/pull/34015)) by @kouroshHakha
* Threshold fix wvSplitk for occasional CI fails ([#34013](https://github.com/vllm-project/vllm/pull/34013)) by @amd-hhashemi
* [torch.compile] Stop compiling identical artifacts ([#34003](https://github.com/vllm-project/vllm/pull/34003)) by @zou3519
* [Revert] Add util `handle_deprecated` back ([#33998](https://github.com/vllm-project/vllm/pull/33998)) by @yewentao256
* Update `WeightTransferConfig` to be more standard like the others ([#33989](https://github.com/vllm-project/vllm/pull/33989)) by @hmellor
* Bump HF Hub client to get bug fix ([#33984](https://github.com/vllm-project/vllm/pull/33984)) by @hmellor
* Consolidate and fix forbidden import `pre-commit` checks ([#33982](https://github.com/vllm-project/vllm/pull/33982)) by @hmellor
* [PaddleOCR-VL] Add BC for transformers 5.0 config ([#33976](https://github.com/vllm-project/vllm/pull/33976)) by @zhang-prog
* [torch.compile][Fusion] Fix attention fusion pass removing kv_udpate op. ([#33945](https://github.com/vllm-project/vllm/pull/33945)) by @charlifu
* [Log] Optimize duplicate startup log ([#33944](https://github.com/vllm-project/vllm/pull/33944)) by @yewentao256
* move checks out of `unified_kv_cache_update` custom op ([#33943](https://github.com/vllm-project/vllm/pull/33943)) by @Rohan138
* Enable Eagle3 speculative decoding for Mistral3ForConditionalGeneration to support eagle3 ([#33939](https://github.com/vllm-project/vllm/pull/33939)) by @TundeAtSN
* Update DeepGEMM version pin in Dockerfile to match #32479 ([#33935](https://github.com/vllm-project/vllm/pull/33935)) by @zifeitong
* [Misc] Update code for encoder-decoder models ([#33900](https://github.com/vllm-project/vllm/pull/33900)) by @DarkLight1337
* [Bug Fix] Fix `naive_block_assignment` always defaulting to False due to arg misalignment ([#33848](https://github.com/vllm-project/vllm/pull/33848)) by @RunkaiTao
* [Misc] Fix up attention benchmarks ([#33810](https://github.com/vllm-project/vllm/pull/33810)) by @LucasWilkinson
* [Voxstral Realtime] Enable tests ([#33803](https://github.com/vllm-project/vllm/pull/33803)) by @patrickvonplaten
* [CPU] Add BF16 Kernel type for s390x ([#33788](https://github.com/vllm-project/vllm/pull/33788)) by @R3hankhan123
* [Revert] Fix performance regression for GLM-4.7-GPTQ decode and MTP acceptance rate ([#33771](https://github.com/vllm-project/vllm/pull/33771)) by @aabbccddwasd
* [WideEP] Fix nvfp4 DeepEP High Throughput All2All backend ([#33738](https://github.com/vllm-project/vllm/pull/33738)) by @tlrmchlsmth
* [torch.compile] Add an option to force-enable the MOE cold start optimization ([#33735](https://github.com/vllm-project/vllm/pull/33735)) by @zou3519
* [torch.compile] Reorganize vllm/compilation and tests/compile (0/N for vLLM IR) ([#33731](https://github.com/vllm-project/vllm/pull/33731)) by @ProExpertProg
* Onboard voyage-4-nano ([#33720](https://github.com/vllm-project/vllm/pull/33720)) by @chengchengpei
*  [Hybrid] Fix and optimize block-aligned splitting in mamba cache align mode ([#33706](https://github.com/vllm-project/vllm/pull/33706)) by @peakcrosser7
* [PluggableLayer][3/N] Apply PluggableLayer to mamba layers. ([#33660](https://github.com/vllm-project/vllm/pull/33660)) by @whx-sjtu
* Make directory exist ok for ray spinning up multiple replicas on a single instance ([#33604](https://github.com/vllm-project/vllm/pull/33604)) by @jiangwu300
* [CPU][BugFix] Fix loading of w4a8int models with bias ([#33582](https://github.com/vllm-project/vllm/pull/33582)) by @fadara01
* fix(ROCm): Make flash_attn import optional in MLA attention ([#33511](https://github.com/vllm-project/vllm/pull/33511)) by @rabi
* Perf tuning and expansion of cases covered for wvSplitKrc ([#33493](https://github.com/vllm-project/vllm/pull/33493)) by @amd-hhashemi
* [Attention] Add FlashInfer Sparse MLA backend ([#33451](https://github.com/vllm-project/vllm/pull/33451)) by @MatthewBonanni
* [KV Connector] Add missing method overrides to MultiConnector ([#33292](https://github.com/vllm-project/vllm/pull/33292)) by @eicherseiji
* [Model Runner V2] support apply penalty for spec decode ([#33251](https://github.com/vllm-project/vllm/pull/33251)) by @izhuhaoran
* [structured output] validate unsupported json features first ([#33233](https://github.com/vllm-project/vllm/pull/33233)) by @andyxning
* [Model Runner V2] Init cuda graph pool when necessary ([#33217](https://github.com/vllm-project/vllm/pull/33217)) by @xinyu-intel
* [Misc] Add run one batch script that supports profiling ([#32968](https://github.com/vllm-project/vllm/pull/32968)) by @LucasWilkinson
* [MoE Refactor] Introduce MoERunner abstraction and move execution logic from FusedMoE to DefaultMoERunner ([#32344](https://github.com/vllm-project/vllm/pull/32344)) by @bnellnm
* [ASR] Fix audio benchmark and add RTFx metric ([#32300](https://github.com/vllm-project/vllm/pull/32300)) by @ekagra-ranjan
* [cpu][performance] CPU Paged Attention NEON BFMMLA BF16 Implementation ([#32263](https://github.com/vllm-project/vllm/pull/32263)) by @gassan-arm
* feat(frontend): early-fail tokenization guard for user requests ([#31366](https://github.com/vllm-project/vllm/pull/31366)) by @scratch-ml
* [SM100] Resubmit FMHA FP8 prefill for MLA ([#31195](https://github.com/vllm-project/vllm/pull/31195)) by @pavanimajety
* [Release 2.10] Update to Torch 2.10 - final release ([#30525](https://github.com/vllm-project/vllm/pull/30525)) by @atalman

## Contributors

@1643661061leo, @AllenDou, @AndreasKaratzas, @AstroVoyager7, @Code4me2, @DarkLight1337, @FredericOdermatt, @Isotr0py, @JJJYmmm, @Kimahriman, @KrxGu, @Linda-Stadter, @LoganJane, @LopezCastroRoberto, @LucasWilkinson, @MatthewBonanni, @NickLucche, @Oasis-Git, @ProExpertProg, @R3hankhan123, @Rohan138, @RunkaiTao, @SorenDreano, @SumanthRH, @TomerBN-Nvidia, @TundeAtSN, @WoosukKwon, @ZJY0516, @ZhengHongming888, @ZhengkaiZ, @aabbccddwasd, @alecsolder, @amd-hhashemi, @amitz-nv, @andylolu2, @andyxning, @artuskg, @atalman, @baonudesifeizhai, @bigPYJ1151, @bnellnm, @charlifu, @chaunceyjiang, @chengchengpei, @danisereb, @dbari, @dllehr-amd, @eicherseiji, @ekagra-ranjan, @eldarkurtic, @elvischenv, @emricksini-h, @fadara01, @fanyang-real, @frankwang28, @furionw, @gassan-arm, @gmagogsfm, @gshtras, @guodongxiaren, @hao-aaron, @heheda12345, @hmellor, @hongxiayang, @ihb2032, @ikchifo, @ilmarkov, @iori2333, @iseeyuan, @izhuhaoran, @jaewonlee-fb, @jaim12005, @jeejeelee, @jiangwu300, @jikunshang, @jseppanen, @junuxyz, @kebe7jun, @khluu, @kliuae, @klshuster, @kouroshHakha, @kylesayrs, @lkhphuc, @louie-tsai, @mgazz, @mgehre-amd, @mgoin, @micah-wil, @mmangkad, @navmarri14, @nikhil-arm, @njhill, @noooop, @patrickvonplaten, @pavanimajety, @peakcrosser7, @pjs102793, @pooyadavoodi, @qandrew, @rabi, @rasmith, @reaganjlee, @robertgshaw2-redhat, @scratch-ml, @simon-mo, @sjhddh, @sleepcoo, @slippersss, @tc-mb, @tianshu-Michael-yu, @tjtanaa, @tlrmchlsmth, @tomasruizt, @vadiklyutiy, @vllmellm, @whx-sjtu, @wzhao18, @xinyu-intel, @xuebwang-amd, @xyDong0223, @xyang16, @yewentao256, @yichuan-w, @ywang96, @zhang-prog, @zhuohan123, @zhxchen17, @zifeitong, @zou3519, @zucchini-nlp, @zufangzhu, @zzaebok

