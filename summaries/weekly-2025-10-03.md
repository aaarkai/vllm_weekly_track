# Weekly Release Notes for vllm-project/vllm (2025-10-03)

## What's Changed

### âœ¨ Features & Enhancements

* Add Hugging Face Inference Endpoints guide to Deployment docs ([#25886](https://github.com/vllm-project/vllm/pull/25886)) by @sergiopaniego
* Add Phi4FlashForCausalLM to _PREVIOUSLY_SUPPORTED_MODELS ([#25832](https://github.com/vllm-project/vllm/pull/25832)) by @tdoublep
* Add filtering for chat template kwargs ([#25794](https://github.com/vllm-project/vllm/pull/25794)) by @russellb
* Add option to restrict media domains ([#25783](https://github.com/vllm-project/vllm/pull/25783)) by @russellb
* Add flashinfer-build.sh and register precompiled cu128 wheel in Dockerfile ([#25782](https://github.com/vllm-project/vllm/pull/25782)) by @mgoin
* Add explicit pooling classes for the Transformers backend ([#25322](https://github.com/vllm-project/vllm/pull/25322)) by @hmellor
* Support LongCat-Flash-Chat tool call ([#24083](https://github.com/vllm-project/vllm/pull/24083)) by @Xu-Wenqing
* Support RL online quantization with torchao ([#23014](https://github.com/vllm-project/vllm/pull/23014)) by @jerryzh168

### ðŸ› Bug Fixes

* [Bugfix] Disable cascade attention with FlashInfer ([#26130](https://github.com/vllm-project/vllm/pull/26130)) by @mgoin
* [BugFix] Fix FI accuracy issue when used for MLA prefill ([#26063](https://github.com/vllm-project/vllm/pull/26063)) by @LucasWilkinson
* [BugFix] ChunkedLocalAttention is currently not CG compatible ([#26034](https://github.com/vllm-project/vllm/pull/26034)) by @LucasWilkinson
* [BugFix][DP/EP] Fix CUTLASS MLA hang under load ([#26026](https://github.com/vllm-project/vllm/pull/26026)) by @LucasWilkinson
* [Bugfix] Apply same sampling parameters for both `n=1` and `n>1` ([#26005](https://github.com/vllm-project/vllm/pull/26005)) by @kmaehashi
* [BugFix][MM] Fix Nonetype error when video is cache in qwen2.5-omni-thinker ([#26004](https://github.com/vllm-project/vllm/pull/26004)) by @wwl2755
* [Bugfix] Fix `__syncwarp` on ROCM ([#25996](https://github.com/vllm-project/vllm/pull/25996)) by @zhewenl
* Fix test_mamba_ssm_ssd.py due to missing _query_start_loc_to_chunk_indices_offsets ([#25995](https://github.com/vllm-project/vllm/pull/25995)) by @hl475
* [BugFix] Fix default kv-cache-dtype default for DeepseekV3.2 ([#25988](https://github.com/vllm-project/vllm/pull/25988)) by @LucasWilkinson
* [Bug] Fix AttributeError: 'QKVParallelLinear' object has no attribute 'orig_dtype' ([#25958](https://github.com/vllm-project/vllm/pull/25958)) by @yewentao256
* [bugfix][deepseek] fix flashmla kernel selection ([#25956](https://github.com/vllm-project/vllm/pull/25956)) by @youkaichao
* [Bugfix][Model]fix ernie45 moe gate&bias dtype to float32 ([#25936](https://github.com/vllm-project/vllm/pull/25936)) by @CSWYF3634076
* Fix INT8 quantization error on Blackwell GPUs (SM100+) ([#25935](https://github.com/vllm-project/vllm/pull/25935)) by @padg9912
* [Bugfix] Token type and position embeddings fail to be applied to `inputs_embeds` ([#25922](https://github.com/vllm-project/vllm/pull/25922)) by @DarkLight1337
* [BugFix] Pass config_format via try_get_generation_config ([#25912](https://github.com/vllm-project/vllm/pull/25912)) by @acisseJZhong
* [Bug] Fix Weight Loading for Block FP8 Cutlass SM90 ([#25909](https://github.com/vllm-project/vllm/pull/25909)) by @yewentao256
* [BugFix] Fix DP/EP hang  ([#25906](https://github.com/vllm-project/vllm/pull/25906)) by @LucasWilkinson
* Fix MTP with deepep_low_latency ([#25904](https://github.com/vllm-project/vllm/pull/25904)) by @MatthewBonanni
* [Bugfix] Fix accuracy issue of TRTLLM FP8 MOE and improve logging ([#25895](https://github.com/vllm-project/vllm/pull/25895)) by @pavanimajety
* [Bugfix][Speculative Decoding] Fix Eagle3 quantization config issue ([#25883](https://github.com/vllm-project/vllm/pull/25883)) by @rahul-tuli
* [Bugfix] Fallback ViT attn backend to SDPA for blackwell ([#25851](https://github.com/vllm-project/vllm/pull/25851)) by @ywang96
* [Bugfix] fix Qwen3VLMoe load when pp > 1 ([#25838](https://github.com/vllm-project/vllm/pull/25838)) by @JJJYmmm
* [Bugfix] Fix requirements paths in install instructions ([#25827](https://github.com/vllm-project/vllm/pull/25827)) by @yingjun-mou
* [Fix] Improve CPU backend compatibility for RISC-V ([#25816](https://github.com/vllm-project/vllm/pull/25816)) by @ihb2032
* [Bugfix] Fix Qwen3-VL regression from #24982 ([#25814](https://github.com/vllm-project/vllm/pull/25814)) by @ywang96
* [Bugfix][NIXL] Fix Async Scheduler timeout issue ([#25808](https://github.com/vllm-project/vllm/pull/25808)) by @NickLucche
* [Bugfix] Fix triton import precommit failure ([#25803](https://github.com/vllm-project/vllm/pull/25803)) by @tlrmchlsmth
* [Bugfix] Add missing `image_size` for phi4_multimodal ([#25796](https://github.com/vllm-project/vllm/pull/25796)) by @Renovamen
* [Bugfix] Allow Only SDPA Backend for ViT on B200 for Qwen3-VL ([#25788](https://github.com/vllm-project/vllm/pull/25788)) by @yewentao256
* Fix GPTQ model loading in Transformers backend ([#25770](https://github.com/vllm-project/vllm/pull/25770)) by @hmellor
* [Bug]: Set LD_LIBRARY_PATH to include the 'standard' CUDA location ([#25766](https://github.com/vllm-project/vllm/pull/25766)) by @smarterclayton
* fix: print outputt offline_inference/base/chat.py example ([#25744](https://github.com/vllm-project/vllm/pull/25744)) by @Iceber
* fix: revert cast to cpu in `MsgpackEncoder._encode_tensor` to avoid hidden performance regressions ([#25738](https://github.com/vllm-project/vllm/pull/25738)) by @qthequartermasterman
* [Bugfix] Properly abort pooling request. ([#25734](https://github.com/vllm-project/vllm/pull/25734)) by @noooop
* [Fix][torch.compile] fix unique_filepath ([#25732](https://github.com/vllm-project/vllm/pull/25732)) by @ZJY0516
* [Bugfix] Use correct key "ignore" for config.json non-quantized layers ([#25706](https://github.com/vllm-project/vllm/pull/25706)) by @leejnau
* [Bugfix] Fix Shared Expert/Zero expert code in FusedMoE.process_chunk ([#25698](https://github.com/vllm-project/vllm/pull/25698)) by @SageMoore
* [Bug] Fix Negative Cuda Memory Usage ([#25683](https://github.com/vllm-project/vllm/pull/25683)) by @yewentao256
* [BugFix] Fix using `dbo_decode_token_threshold` always (and ignoring `dbo_prefill_token_threshold`) ([#25622](https://github.com/vllm-project/vllm/pull/25622)) by @LucasWilkinson
* [Bugfix][ROCm] Fixing trying to import non-existent symbols from libnccl.so ([#25605](https://github.com/vllm-project/vllm/pull/25605)) by @gshtras
* [BugFix][torch.compile] KV scale calculation issues with FP8 quantization (#21640) ([#25513](https://github.com/vllm-project/vllm/pull/25513)) by @adabeyta
* [Bugfix] Optimize CpuGpuBuffer initialization ([#25447](https://github.com/vllm-project/vllm/pull/25447)) by @namanlalitnyu
* [Bugfix] Improve GLM4 MoE Reasoning Parser's is_reasoning_end Condition ([#25355](https://github.com/vllm-project/vllm/pull/25355)) by @frankwang28
* [Bugfix][Model] Fix inference for Hunyuan dense models ([#25354](https://github.com/vllm-project/vllm/pull/25354)) by @Anionex
* [Bugfix]: Clean up chunked prefill logging when using whisper ([#25075](https://github.com/vllm-project/vllm/pull/25075)) by @simondanielsson
* [Bugfix][WideEP] Apply TP Attn + EP MoE fix to other models ([#24982](https://github.com/vllm-project/vllm/pull/24982)) by @tlrmchlsmth
* Fix random dataset mismatched token length with config. ([#24937](https://github.com/vllm-project/vllm/pull/24937)) by @weireweire
* [Bugfix] Merge MM embeddings by index instead of token IDs ([#16229](https://github.com/vllm-project/vllm/pull/16229)) by @DarkLight1337

### âš¡ï¸ Performance

* [perf] Use CPU tensor to reduce GPU->CPU sync ([#25884](https://github.com/vllm-project/vllm/pull/25884)) by @lhtin
* [Perf] Fix and reapply move apply w8a8 block fp8 linear to class ([#25696](https://github.com/vllm-project/vllm/pull/25696)) by @ElizaWszola

### ðŸ¤– Model Support

* [Model] Use `merge_by_field_config` for MM models (D-F) ([#26076](https://github.com/vllm-project/vllm/pull/26076)) by @DarkLight1337
* [Model] Use `merge_by_field_config` for MM models (A-C) ([#26073](https://github.com/vllm-project/vllm/pull/26073)) by @DarkLight1337
* [Model] MTP fallback to eager for DeepSeek v32 ([#25982](https://github.com/vllm-project/vllm/pull/25982)) by @luccafong
* [Model] Move `vision_feature_select_strategy` into `resolve_visual_encoder_outputs` ([#25938](https://github.com/vllm-project/vllm/pull/25938)) by @DarkLight1337
* [Model][Bugfix] Fix MiDashengLM audio encoder mask by removing incorrect `logical_not` ([#25925](https://github.com/vllm-project/vllm/pull/25925)) by @zhoukezi
* [Model] Remove MotifForCausalLM ([#25866](https://github.com/vllm-project/vllm/pull/25866)) by @jeejeelee
* [Model][Bugfix] Fix issues in MiDashengLM implementation for quantized models ([#25854](https://github.com/vllm-project/vllm/pull/25854)) by @zhoukezi
* [gpt-oss] use vLLM instead of openai types for streaming ([#25186](https://github.com/vllm-project/vllm/pull/25186)) by @qandrew
* Llamas 3.1 405B fp4 changes upstreaming from 355_wip ([#25135](https://github.com/vllm-project/vllm/pull/25135)) by @maleksan85
* [Model] Mamba2 varlen and metadata refactor  ([#21467](https://github.com/vllm-project/vllm/pull/21467)) by @cyang49

### ðŸ”Œ Hardware & Backend

* [ROCm][Bugfix] Add missing parameter to ROCm backend ([#26029](https://github.com/vllm-project/vllm/pull/26029)) by @gshtras
* [ROCm][Build] Add support for AMD Ryzen AI MAX / AI 300 Series ([#25908](https://github.com/vllm-project/vllm/pull/25908)) by @hyoon1
* [XPU]Fix xpu spec decoding UTs, avoid using cuda graph ([#25847](https://github.com/vllm-project/vllm/pull/25847)) by @jikunshang
* [NVIDIA] Blackwell Family ([#24673](https://github.com/vllm-project/vllm/pull/24673)) by @johnnynunez

### âš™ï¸ Refactoring & Core

* [Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 ([#25858](https://github.com/vllm-project/vllm/pull/25858)) by @Josephasafg
* Remove redundant cudagraph dispatcher warning ([#25841](https://github.com/vllm-project/vllm/pull/25841)) by @mgoin
* [Core] Don't count preempted tokens in prefix cache hit rate ([#25787](https://github.com/vllm-project/vllm/pull/25787)) by @zhuohan123
* [Refactor] Remove DeepGEMM OP Register ([#25710](https://github.com/vllm-project/vllm/pull/25710)) by @yewentao256
* [Core] Force PIECEWISE CUDAGraph mode for encoder-decoder ([#25701](https://github.com/vllm-project/vllm/pull/25701)) by @russellb
* Remove cuda hard-code in compute_causal_conv1d_metadata ([#25555](https://github.com/vllm-project/vllm/pull/25555)) by @wxsIcey
* [Core] Refactor self.model() to call a helper for subclassing. ([#25084](https://github.com/vllm-project/vllm/pull/25084)) by @patrick-toulme
* [Core] GC Debug callback ([#24829](https://github.com/vllm-project/vllm/pull/24829)) by @Jialin
* [Kernel] Chunk-aligned mamba2 ([#24683](https://github.com/vllm-project/vllm/pull/24683)) by @tdoublep

### ðŸ”§ Build, CI & Testing

* [CI] Add Blackwell DeepSeek FP8 FlashInfer MoE tests ([#26040](https://github.com/vllm-project/vllm/pull/26040)) by @mgoin
* [CI] Tweaks to GPT-OSS Eval (Blackwell) for stability ([#26030](https://github.com/vllm-project/vllm/pull/26030)) by @mgoin
* [CI] Only capture a single CUDA graph size in CI by default ([#25951](https://github.com/vllm-project/vllm/pull/25951)) by @hmellor
* [CI] Fix test_shared_storage_connector_hashes ([#25748](https://github.com/vllm-project/vllm/pull/25748)) by @chaunceyjiang
* [CI] Fix FlashInfer AOT in release docker image ([#25730](https://github.com/vllm-project/vllm/pull/25730)) by @mgoin
* [CI] Add E2E Blackwell Quantized MoE Test ([#25723](https://github.com/vllm-project/vllm/pull/25723)) by @mgoin
* [CI] Move applicable tests to CPU ([#24080](https://github.com/vllm-project/vllm/pull/24080)) by @rzabarazesh

### ðŸ“š Documentation

* [Doc] updating torch.compile doc link ([#25989](https://github.com/vllm-project/vllm/pull/25989)) by @nadathurv
* [Doc] Improve MM Pooling model documentation ([#25966](https://github.com/vllm-project/vllm/pull/25966)) by @DarkLight1337
* [Docs] Remove API Reference from search index ([#25949](https://github.com/vllm-project/vllm/pull/25949)) by @hmellor
* [Doc] Add Cambricon MLU support ([#25942](https://github.com/vllm-project/vllm/pull/25942)) by @a120092009
* [Doc] Polish example for torchrun dp ([#25899](https://github.com/vllm-project/vllm/pull/25899)) by @zhuohan123
* [Doc] Add documentation for vLLM continuous benchmarking and profiling ([#25819](https://github.com/vllm-project/vllm/pull/25819)) by @namanlalitnyu
* [Docs] Add Toronto Meetup ([#25773](https://github.com/vllm-project/vllm/pull/25773)) by @mgoin
* [Doc] Update Batch-level DP docs ([#25757](https://github.com/vllm-project/vllm/pull/25757)) by @DarkLight1337
* [Doc]: improve CPU(x86) build-wheel-from-source section ([#25617](https://github.com/vllm-project/vllm/pull/25617)) by @brokedba
* [docs] Resolve transcriptions API TODO ([#25446](https://github.com/vllm-project/vllm/pull/25446)) by @yyzxw
* [Docs] Add moe kernel features doc  ([#25297](https://github.com/vllm-project/vllm/pull/25297)) by @bnellnm

### ðŸ“¦ Miscellaneous

* [Log] Optimize DeepGEMM Missing Log ([#26106](https://github.com/vllm-project/vllm/pull/26106)) by @yewentao256
* Change size of single CUDA graph for CI to 4 ([#26089](https://github.com/vllm-project/vllm/pull/26089)) by @tdoublep
* Update base image to 22.04 (jammy) ([#26065](https://github.com/vllm-project/vllm/pull/26065)) by @huydhn
* [Small] Prevent bypassing media domain restriction via HTTP redirects ([#26035](https://github.com/vllm-project/vllm/pull/26035)) by @huachenheli
* [Misc] Make handling of SamplingParams clearer in n>1 case ([#26032](https://github.com/vllm-project/vllm/pull/26032)) by @njhill
* [Benchmark] Finish documented v0.11.0 deprecation of --endpoint-type ([#26007](https://github.com/vllm-project/vllm/pull/26007)) by @natoscott
* [Misc] Factor out common `_apply_feature_select_strategy` ([#26003](https://github.com/vllm-project/vllm/pull/26003)) by @DarkLight1337
* [MM] Add text-only mode for Qwen3-VL ([#26000](https://github.com/vllm-project/vllm/pull/26000)) by @ywang96
* [Deepseek v3.2] Support indexer prefill chunking ([#25999](https://github.com/vllm-project/vllm/pull/25999)) by @heheda12345
* [CI/Build] Replace `vllm.entrypoints.openai.api_server` entrypoint with `vllm serve` command ([#25967](https://github.com/vllm-project/vllm/pull/25967)) by @DarkLight1337
* [Bench] Add DeepSeekV32 to MoE benchmark ([#25962](https://github.com/vllm-project/vllm/pull/25962)) by @jeejeelee
* EAGLE 3: Fix preamble so that measured speedup over Eagle 1 becomes 32% instead of 5% on MTBench ([#25916](https://github.com/vllm-project/vllm/pull/25916)) by @ekagra-ranjan
* [Benchmark] Support benchmark throughput for external launcher DP ([#25913](https://github.com/vllm-project/vllm/pull/25913)) by @zhuohan123
* [NIXL] Add support for MLA caches with different latent dim ([#25902](https://github.com/vllm-project/vllm/pull/25902)) by @NickLucche
* [V0 Deprecation] Remove `vllm.worker` and update according imports ([#25901](https://github.com/vllm-project/vllm/pull/25901)) by @aarnphm
* [NIXL] Increase default KV block eviction timeout on P ([#25897](https://github.com/vllm-project/vllm/pull/25897)) by @NickLucche
* [New Model] DeepSeek-V3.2 (Rebased to Main) ([#25896](https://github.com/vllm-project/vllm/pull/25896)) by @zyongye
* [Llama4] [multimodal] Fix misplaced dtype cast of `cos_sin_cache` in `Llama4VisionRotaryEmbedding` ([#25889](https://github.com/vllm-project/vllm/pull/25889)) by @cjackal
* [CI/Build] Include Transformers backend test in nightly transformers test ([#25885](https://github.com/vllm-project/vllm/pull/25885)) by @Isotr0py
* update to latest deepgemm for dsv3.2 ([#25871](https://github.com/vllm-project/vllm/pull/25871)) by @youkaichao
* [torch.compile] serialize cudagraph_mode as its enum name instead of value ([#25868](https://github.com/vllm-project/vllm/pull/25868)) by @ZJY0516
* [Misc] Remove more `get_input_embeddings_v0` ([#25857](https://github.com/vllm-project/vllm/pull/25857)) by @DarkLight1337
* OffloadingConnector: Fix GPU block tracking bug ([#25856](https://github.com/vllm-project/vllm/pull/25856)) by @orozery
* [P/D] NIXL Updates ([#25844](https://github.com/vllm-project/vllm/pull/25844)) by @robertgshaw2-redhat
* Update launch_bounds_utils.h for correct compile on Multiple Cuda Arch - PTXAS out of range Warning ([#25843](https://github.com/vllm-project/vllm/pull/25843)) by @DrStone71
* Update GLM-4.5 Doc transformers version ([#25830](https://github.com/vllm-project/vllm/pull/25830)) by @zRzRzRzRzRzRzR
* [MISC] Fix misleading batch_size_capture_list when cuda_graph_sizes < 4 ([#25829](https://github.com/vllm-project/vllm/pull/25829)) by @billishyahao
* [Misc] fix tests failure by using current_platform ([#25825](https://github.com/vllm-project/vllm/pull/25825)) by @kingsmad
* [MM] Optimize memory profiling for scattered multimodal embeddings ([#25810](https://github.com/vllm-project/vllm/pull/25810)) by @ywang96
* [CI/Build] Add timing to Model Executor Test ([#25799](https://github.com/vllm-project/vllm/pull/25799)) by @22quinn
* [Misc] Update openai client example file for multimodal ([#25795](https://github.com/vllm-project/vllm/pull/25795)) by @ywang96
* [Misc] Make EP kernels install script support uv ([#25785](https://github.com/vllm-project/vllm/pull/25785)) by @LucasWilkinson
* Validate API tokens in constant time ([#25781](https://github.com/vllm-project/vllm/pull/25781)) by @russellb
* Reduce the Cuda Graph memory footprint when running with DBO ([#25779](https://github.com/vllm-project/vllm/pull/25779)) by @SageMoore
* [CI/Build] Reorganize root-level V1 tests ([#25767](https://github.com/vllm-project/vllm/pull/25767)) by @DarkLight1337
* [CI/Build] Consolidate model loader tests and requirements ([#25765](https://github.com/vllm-project/vllm/pull/25765)) by @DarkLight1337
* [amd_dev] branch rebase ([#25753](https://github.com/vllm-project/vllm/pull/25753)) by @HAIAI
* [Qwen3-Next][GDN] fixes cuda graph capturing bug in GDN metadata and a stride bug in causal_conv_1d. ([#25743](https://github.com/vllm-project/vllm/pull/25743)) by @sighingnow
* perf: Avoid copying inputs_embeds tensors to GPU unless prompt_embeds is enabled ([#25739](https://github.com/vllm-project/vllm/pull/25739)) by @qthequartermasterman
* [CI/Build] fix doc build warning: Failed to get 'name: description' pair ([#25733](https://github.com/vllm-project/vllm/pull/25733)) by @yitingdc
* [Misc] Remove unnecessary memoryviews in shm_broadcast.py ([#25721](https://github.com/vllm-project/vllm/pull/25721)) by @njhill
* [Misc] Don't log shm dequeue delay warning on worker side ([#25720](https://github.com/vllm-project/vllm/pull/25720)) by @njhill
* Test Prompt Embeds/LoRA compatibility and Enable LoRA Support for OPT Models  ([#25717](https://github.com/vllm-project/vllm/pull/25717)) by @qthequartermasterman
* [Log] Optimize Log for FP8MOE ([#25709](https://github.com/vllm-project/vllm/pull/25709)) by @yewentao256
* [Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI300X ([#25703](https://github.com/vllm-project/vllm/pull/25703)) by @xaguilar-amd
* Updated TRL integration docs ([#25684](https://github.com/vllm-project/vllm/pull/25684)) by @sergiopaniego
* [Spec decode] automatically disable mm for text-only draft models ([#25667](https://github.com/vllm-project/vllm/pull/25667)) by @jmkuebler
* [misc] refactor speculative config ([#25657](https://github.com/vllm-project/vllm/pull/25657)) by @yyzxw
* [torch.compile]: Add VLLM_DEBUG_DUMP_PATH environment variable ([#25651](https://github.com/vllm-project/vllm/pull/25651)) by @ZJY0516
* Kernel-override Determinism [1/n] ([#25603](https://github.com/vllm-project/vllm/pull/25603)) by @bwasti
* [CI/Build] Split up Distributed Tests ([#25572](https://github.com/vllm-project/vllm/pull/25572)) by @DarkLight1337
* [CI/Build] Fix some V1 tests not being run ([#25569](https://github.com/vllm-project/vllm/pull/25569)) by @DarkLight1337
* [VLM] Update Qwen3-VL max_num_video_tokens calculation for configurable video profiling ([#25557](https://github.com/vllm-project/vllm/pull/25557)) by @Isotr0py
* [FA/Chore] Bump vllm-flash-attention ([#25537](https://github.com/vllm-project/vllm/pull/25537)) by @LucasWilkinson
* [Platform][CI] Added OOT platform interface e2e test that running on Ascend NPU ([#25470](https://github.com/vllm-project/vllm/pull/25470)) by @leo-pony
* [Quantization] Add field to skip unquantized modules for GPTQ config ([#25455](https://github.com/vllm-project/vllm/pull/25455)) by @Isotr0py
* [ray][metrics] Replace ':' with '_' for OpenTelemetry compatibility in Ray ([#25439](https://github.com/vllm-project/vllm/pull/25439)) by @eicherseiji
* [Misc]allow disable pynccl ([#25421](https://github.com/vllm-project/vllm/pull/25421)) by @luccafong
* [V0 Deprecation][Models] Remove all V0 condition for mm embeddings merge ([#25331](https://github.com/vllm-project/vllm/pull/25331)) by @Isotr0py
* Move`VllmConfig` from `config/__init__.py` to `config/vllm.py` ([#25271](https://github.com/vllm-project/vllm/pull/25271)) by @hmellor
* [spec decode] Consolidate speculative decode method name for MTP ([#25232](https://github.com/vllm-project/vllm/pull/25232)) by @zixi-qi
* [Mamba][KVCacheManager] Simplify kv cache manage logic for mamba + MTP ([#25119](https://github.com/vllm-project/vllm/pull/25119)) by @heheda12345
* [env] default nixl side port conflicts  with kv-event zmq port ([#25056](https://github.com/vllm-project/vllm/pull/25056)) by @panpan0000
* [Misc] Fix codeowners override for v1 sample and attention ([#25037](https://github.com/vllm-project/vllm/pull/25037)) by @22quinn
* Run:ai model streamer add GCS package support ([#24909](https://github.com/vllm-project/vllm/pull/24909)) by @pwschuurman
* [Cuda2CPU][P/D] Add cuda2cpu support in NixlConnector ([#24690](https://github.com/vllm-project/vllm/pull/24690)) by @chenxi-yang
* [Qwen][ROCm] Flash Attention Rotary Embeddings ([#24642](https://github.com/vllm-project/vllm/pull/24642)) by @vllmellm
* Update to Transformers `v4.56.2` ([#24638](https://github.com/vllm-project/vllm/pull/24638)) by @hmellor
* Eagle3 that supports the Minicpm3 model ([#24243](https://github.com/vllm-project/vllm/pull/24243)) by @LDLINGLINGLING
* [V1] address post issues related to #20059 (part 1); cascade attention reenable by default ([#23046](https://github.com/vllm-project/vllm/pull/23046)) by @fhl2000
* EVS Support (Video tokens pruning) ([#22980](https://github.com/vllm-project/vllm/pull/22980)) by @BloodAxe
* [Multimodal][Speculative Decoding]Eagle Eagle3 mm support, enablement on qwen2.5vl ([#22872](https://github.com/vllm-project/vllm/pull/22872)) by @david6666666
* [V1] [P/D] Add Support for KV Load Failure Recovery ([#19330](https://github.com/vllm-project/vllm/pull/19330)) by @sdavidbd

## Contributors

@22quinn, @Anionex, @BloodAxe, @CSWYF3634076, @DarkLight1337, @DrStone71, @ElizaWszola, @HAIAI, @Iceber, @Isotr0py, @JJJYmmm, @Jialin, @Josephasafg, @LDLINGLINGLING, @LucasWilkinson, @MatthewBonanni, @NickLucche, @Renovamen, @SageMoore, @Xu-Wenqing, @ZJY0516, @a120092009, @aarnphm, @acisseJZhong, @adabeyta, @billishyahao, @bnellnm, @brokedba, @bwasti, @chaunceyjiang, @chenxi-yang, @cjackal, @cyang49, @david6666666, @eicherseiji, @ekagra-ranjan, @fhl2000, @frankwang28, @gshtras, @heheda12345, @hl475, @hmellor, @huachenheli, @huydhn, @hyoon1, @ihb2032, @jeejeelee, @jerryzh168, @jikunshang, @jmkuebler, @johnnynunez, @kingsmad, @kmaehashi, @leejnau, @leo-pony, @lhtin, @luccafong, @maleksan85, @mgoin, @nadathurv, @namanlalitnyu, @natoscott, @njhill, @noooop, @orozery, @padg9912, @panpan0000, @patrick-toulme, @pavanimajety, @pwschuurman, @qandrew, @qthequartermasterman, @rahul-tuli, @robertgshaw2-redhat, @russellb, @rzabarazesh, @sdavidbd, @sergiopaniego, @sighingnow, @simondanielsson, @smarterclayton, @tdoublep, @tlrmchlsmth, @vllmellm, @weireweire, @wwl2755, @wxsIcey, @xaguilar-amd, @yewentao256, @yingjun-mou, @yitingdc, @youkaichao, @ywang96, @yyzxw, @zRzRzRzRzRzRzR, @zhewenl, @zhoukezi, @zhuohan123, @zixi-qi, @zyongye

