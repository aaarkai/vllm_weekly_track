# Weekly Release Notes for vllm-project/vllm (2025-12-05)

## What's Changed

### âœ¨ Features & Enhancements

* support qwen3-vl handle requests with embeddings ([#30037](https://github.com/vllm-project/vllm/pull/30037)) by @taoyun951753
* Add DeepSeek-V3.2 tool parser. ([#29848](https://github.com/vllm-project/vllm/pull/29848)) by @Xu-Wenqing
* Add missing return in _check_vllm_model_embed_input_ids ([#29834](https://github.com/vllm-project/vllm/pull/29834)) by @jcyang43
* Add logging for cudagraph related info ([#29825](https://github.com/vllm-project/vllm/pull/29825)) by @sarckk
* Add Mistral Large 3 and Ministral 3 ([#29757](https://github.com/vllm-project/vllm/pull/29757)) by @juliendenize
* add add_truncate_prompt_tokens in repr for PoolingParams ([#29683](https://github.com/vllm-project/vllm/pull/29683)) by @guodongxiaren
* [Feature][Bench] Add pareto visualization ([#29477](https://github.com/vllm-project/vllm/pull/29477)) by @lengrongfu
* [Feat] Support non-gated activations in NVFP4 modelopt path ([#29004](https://github.com/vllm-project/vllm/pull/29004)) by @omera-nv
* Add gpu memory wait before test_async_tp ([#28893](https://github.com/vllm-project/vllm/pull/28893)) by @angelayi

### ðŸ› Bug Fixes

* [Bugfix] Fix the issue with interleaved thinking when using streaming ([#30033](https://github.com/vllm-project/vllm/pull/30033)) by @chaunceyjiang
* Fix broken multiline assert in `LoRAModelManager.register_module` ([#30032](https://github.com/vllm-project/vllm/pull/30032)) by @hyongtao-code
* [Bugfix] fixed deepseekv32 tool calling error ([#30025](https://github.com/vllm-project/vllm/pull/30025)) by @chaunceyjiang
* [Bugfix] Fix adapter_enabled IMA ([#29977](https://github.com/vllm-project/vllm/pull/29977)) by @jeejeelee
* [Bugfix] Fix flashinfer ar+norm kernel not available issue ([#29960](https://github.com/vllm-project/vllm/pull/29960)) by @elvischenv
* fix LoRA-related examples ([#29956](https://github.com/vllm-project/vllm/pull/29956)) by @Iceber
* Fix LLMEngine.del dp_group cleanup condition ([#29954](https://github.com/vllm-project/vllm/pull/29954)) by @hyongtao-code
* [Bugfix] Follow-up fix on MediaWithBytes ([#29951](https://github.com/vllm-project/vllm/pull/29951)) by @ywang96
* [Bugfix] Fix incorrect `image_grid_thw` rank for HunyuanOCR from missing `merge_by_field_config=True` ([#29950](https://github.com/vllm-project/vllm/pull/29950)) by @Isotr0py
* [Bugfix][Quantization] Support BF16 tensors on GGUF ([#29948](https://github.com/vllm-project/vllm/pull/29948)) by @a4lg
* [BugFix] Fix DBO assert `assert B_block_table == B_q` ([#29933](https://github.com/vllm-project/vllm/pull/29933)) by @LucasWilkinson
* [Bugfix] Fix regression on pooling models from PR#29621 ([#29921](https://github.com/vllm-project/vllm/pull/29921)) by @ywang96
* [BUGFIX] Fix regex pattern for Mistral Tool Call ([#29918](https://github.com/vllm-project/vllm/pull/29918)) by @juliendenize
* [Bugfix][EPLB] Prevent user-provided EPLB config from being overwritten with defaults ([#29911](https://github.com/vllm-project/vllm/pull/29911)) by @SageMoore
* [BUGFIX] llama_4_scaling wrongly passed to DeepseekAttention ([#29908](https://github.com/vllm-project/vllm/pull/29908)) by @juliendenize
* [BugFix] Fix assert in `build_for_cudagraph_capture` ([#29893](https://github.com/vllm-project/vllm/pull/29893)) by @LucasWilkinson
* [Bugfix] Fix FP8 MoE LoRA ([#29890](https://github.com/vllm-project/vllm/pull/29890)) by @jeejeelee
* [Bugfix] Fix incorrect channel order for idefics3 in edge case ([#29881](https://github.com/vllm-project/vllm/pull/29881)) by @Isotr0py
* [BugFix] fix imgs_pos in hunyuan_vl ([#29879](https://github.com/vllm-project/vllm/pull/29879)) by @wkcn
* Fix some more Transformers nightly tests ([#29872](https://github.com/vllm-project/vllm/pull/29872)) by @hmellor
* [BugFix] add max-num-batched-token to scheduler hash ([#29829](https://github.com/vllm-project/vllm/pull/29829)) by @BoyuanFeng
* Fix some Transformers nightly tests ([#29802](https://github.com/vllm-project/vllm/pull/29802)) by @hmellor
* Fix error while downloading dependencies for CPU backend ([#29797](https://github.com/vllm-project/vllm/pull/29797)) by @MaoJianwei
* [BugFix] Fix index error in ngram_proposer ([#29779](https://github.com/vllm-project/vllm/pull/29779)) by @usberkeley
* [Bugfix] fix --scheduling-policy=priority & n>1 crashes engine ([#29764](https://github.com/vllm-project/vllm/pull/29764)) by @chaunceyjiang
* [BugFix] respect VLLM_LOGGING_LEVEL in logger ([#29761](https://github.com/vllm-project/vllm/pull/29761)) by @BoyuanFeng
* [BugFix] Preserve spec decoding uniform decode when scheduling ([#29759](https://github.com/vllm-project/vllm/pull/29759)) by @njhill
* [Bugfix] Fix mismatched nvfp4 gemm output shape ([#29742](https://github.com/vllm-project/vllm/pull/29742)) by @Isotr0py
* Fix AttributeError about _use_fi_prefill ([#29734](https://github.com/vllm-project/vllm/pull/29734)) by @hl475
* [Bugfix] Revert test_tokenization.py ([#29729](https://github.com/vllm-project/vllm/pull/29729)) by @jeejeelee
* [Bugfix] Fix wrong mock attribute ([#29704](https://github.com/vllm-project/vllm/pull/29704)) by @DarkLight1337
* Fix RoPE failures in Transformers nightly ([#29700](https://github.com/vllm-project/vllm/pull/29700)) by @hmellor
* [BugFix] Fix DBO failing with TypeError: 'NoneType' object is not iterable ([#29698](https://github.com/vllm-project/vllm/pull/29698)) by @LucasWilkinson
* [Bugfix] fix dots.llm1.inst ([#29687](https://github.com/vllm-project/vllm/pull/29687)) by @ZJY0516
* [Bugfix] Fix O(nÂ²) multimodal string prompt processing ([#29667](https://github.com/vllm-project/vllm/pull/29667)) by @mertunsall
* [Bugfix] Defunctionalize TRTLLM AR+Norm op for avoiding extra clone kernel before it ([#29631](https://github.com/vllm-project/vllm/pull/29631)) by @elvischenv
* [BUGFIX] MistralTokenizer._call__ adds an invalid EOS token ([#29607](https://github.com/vllm-project/vllm/pull/29607)) by @juliendenize
* [Bugfix][CPU] Fix CPU KV cache fallback memory allocation ([#29604](https://github.com/vllm-project/vllm/pull/29604)) by @gausah01
* [Bugfix] Fix DeepSeek R1 MTP weight loading ([#29545](https://github.com/vllm-project/vllm/pull/29545)) by @MatthewBonanni
* [BugFix] Fix spec decoding max_tokens scheduling perf issue ([#29542](https://github.com/vllm-project/vllm/pull/29542)) by @njhill
* Fix parameter order in GPT-OSS weight loading function for non-MXFP4 weights  ([#29506](https://github.com/vllm-project/vllm/pull/29506)) by @qGentry
* [Bugfix] TypeError: 'NoneType' object is not callable ([#29414](https://github.com/vllm-project/vllm/pull/29414)) by @mostrowskix
* [BugFix] Fix ValueError in NewRequestData repr methods ([#29392](https://github.com/vllm-project/vllm/pull/29392)) by @maang-h
* [Bugfix] Missing tokens in `return_token_ids` when tool parsers is enabled in streaming mode ([#29074](https://github.com/vllm-project/vllm/pull/29074)) by @Peng-YM
* Fix boolean nested params, add dict format support, and enhance plotting for vllm bench sweep ([#29025](https://github.com/vllm-project/vllm/pull/29025)) by @app/copilot-swe-agent
* [Bugfix][sleepmode][fp8 kv cache]: Fix FP8 KV cache + sleep(level=2) gibberish output ([#28783](https://github.com/vllm-project/vllm/pull/28783)) by @Flink-ddd
* [Bugfix] Respect VLLM_CONFIGURE_LOGGING value ([#28671](https://github.com/vllm-project/vllm/pull/28671)) by @elizabetht
* [Bugfix] Missing cached item in the MultiModalReceiverCache ([#28525](https://github.com/vllm-project/vllm/pull/28525)) by @knlnguyen1802
* [Bugfix] Mistral tool parser streaming update ([#19425](https://github.com/vllm-project/vllm/pull/19425)) by @avigny

### âš¡ï¸ Performance

* [Perf] Enable separate shared_experts stream only for CUDA ([#30085](https://github.com/vllm-project/vllm/pull/30085)) by @alexm-redhat
* [Perf] Avoid pageable HtoD transfer in MinTokensLogitsProcessor ([#29826](https://github.com/vllm-project/vllm/pull/29826)) by @jthomson04
* [Performance][DP/EP] Add silu_mul_per_token_group_quant_fp8_colmajor kernel ([#29470](https://github.com/vllm-project/vllm/pull/29470)) by @varun-sundar-rabindranath
* [Perf] Optimize EAGLE prepare_inputs_padded with triton kernels ([#28597](https://github.com/vllm-project/vllm/pull/28597)) by @benchislett

### ðŸ¤– Model Support

* [Model] Add Holo2 reasoning parser ([#30048](https://github.com/vllm-project/vllm/pull/30048)) by @hdlj-h
* [Model][6/N] Improve all pooling task | Support chunked prefill with ALL pooling ([#27145](https://github.com/vllm-project/vllm/pull/27145)) by @noooop

### ðŸ”Œ Hardware & Backend

* [ROCm][CI][Bugfix] Fixing the `Multi-Modal Models Test (Extended) 1` group ([#30013](https://github.com/vllm-project/vllm/pull/30013)) by @AndreasKaratzas
* [ROCm] add fallback for aiter fp8 decode mla ([#30005](https://github.com/vllm-project/vllm/pull/30005)) by @yeqcharlotte
* [Rocm][CI] Fix test_speculator_eagle3 by skipping the CompressedTensorw4a16 Model ([#30001](https://github.com/vllm-project/vllm/pull/30001)) by @charlifu
* [ROCm] [Bugfix] [AITER] `compute_attn_mask_seqlen` for qwen3 omni ([#29974](https://github.com/vllm-project/vllm/pull/29974)) by @tjtanaa
* [ROCm][CI] Fix v1/logits_processors failure on ROCm ([#29927](https://github.com/vllm-project/vllm/pull/29927)) by @micah-wil
* [ROCm][CI][Bugfix] Disable Flash/MemEfficient SDP on ROCm to avoid HF Transformers accuracy issues ([#29909](https://github.com/vllm-project/vllm/pull/29909)) by @AndreasKaratzas
* [ROCm][CI] Fix test_cudagraph_mode.py Failure For AMD CI ([#29808](https://github.com/vllm-project/vllm/pull/29808)) by @micah-wil
* [XPU] Fix AWQ skipped layer detection in IPEX quantization ([#29774](https://github.com/vllm-project/vllm/pull/29774)) by @faaany
* [ROCm][Bugfix] Patch for the `Multi-Modal Processor Test` group ([#29702](https://github.com/vllm-project/vllm/pull/29702)) by @AndreasKaratzas
* [ROCm] Fallback pytorch GELU with tanh approximation to GELU() ([#29244](https://github.com/vllm-project/vllm/pull/29244)) by @divakar-amd
* [ROCm][Attention] Sliding window support for `AiterFlashAttentionBackend` ([#29234](https://github.com/vllm-project/vllm/pull/29234)) by @ganyi1996ppo
* [Rocm] Set VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS default is disabled ([#28985](https://github.com/vllm-project/vllm/pull/28985)) by @zhyajie

### âš™ï¸ Refactoring & Core

* [Core] Remove forced None assignment for deprecated PassConfig flags ([#29994](https://github.com/vllm-project/vllm/pull/29994)) by @arpitkh101
* [Frontend] Fixes anthropic /v1/messages streaming not containing input_tokens on first chunk ([#29971](https://github.com/vllm-project/vllm/pull/29971)) by @bbartels
* [Core] Fix standalone runs of test_reset_prefix_cache_e2e ([#29899](https://github.com/vllm-project/vllm/pull/29899)) by @markmc
* Remove default values from `InitVar`s so that they're not stored ([#29859](https://github.com/vllm-project/vllm/pull/29859)) by @hmellor
* [Frontend] supports deepseekv32 chat template ([#29837](https://github.com/vllm-project/vllm/pull/29837)) by @chaunceyjiang
* [Frontend] refactor harmony utils output message parsing ([#29820](https://github.com/vllm-project/vllm/pull/29820)) by @daniel-salib
* [Core] Eliminate redundant is_encoder_decoder lookups (>20us/token) ([#29800](https://github.com/vllm-project/vllm/pull/29800)) by @wushidonguc
* [Core] Enable `inputs_embeds_size` separate from `hidden_size` ([#29741](https://github.com/vllm-project/vllm/pull/29741)) by @DarkLight1337
* [Frontend] Perform offline path replacement to `tokenizer` ([#29706](https://github.com/vllm-project/vllm/pull/29706)) by @a4lg
* Remove `all_special_tokens_extended` from tokenizer code ([#29686](https://github.com/vllm-project/vllm/pull/29686)) by @hmellor
* [Core] Rename PassConfig flags as per RFC #27995 ([#29646](https://github.com/vllm-project/vllm/pull/29646)) by @arpitkh101
* [Frontend] Resettle pooling entrypoints  ([#29634](https://github.com/vllm-project/vllm/pull/29634)) by @noooop
* [Frontend] Remap -O to -cc commandline flag ([#29557](https://github.com/vllm-project/vllm/pull/29557)) by @gmagogsfm
* Remove upstream fa checks ([#29471](https://github.com/vllm-project/vllm/pull/29471)) by @Victor49152
* [Frontend] Add tool filtering support to ToolServer ([#29224](https://github.com/vllm-project/vllm/pull/29224)) by @daniel-salib
* [Core] Add xxHash as a high-performance hash option for accelerating prefix caching ([#29163](https://github.com/vllm-project/vllm/pull/29163)) by @LuminolT
* [refactor] CTMoEMethods to use QuantizationArgs ([#28871](https://github.com/vllm-project/vllm/pull/28871)) by @HDCharles
* [Core] Support resetting all running requests' KV while calling `reset_prefix_cache` ([#28827](https://github.com/vllm-project/vllm/pull/28827)) by @zhuohan123
* [Refactor] [1/N] to simplify the vLLM serving architecture ([#28040](https://github.com/vllm-project/vllm/pull/28040)) by @chaunceyjiang
* [Core][Observability] Add KV cache residency metrics ([#27793](https://github.com/vllm-project/vllm/pull/27793)) by @shivampr
* [Kernel][Quantization] add w4a8 support for marlin kernel ([#24722](https://github.com/vllm-project/vllm/pull/24722)) by @jinzhen-lin
* [Frontend] add 'verbose_json' and 'timestamp' feature on Whisper Transcription/Translation ([#24209](https://github.com/vllm-project/vllm/pull/24209)) by @sangbumlikeagod

### ðŸ”§ Build, CI & Testing

* [CI] fix silent error in nightly wheel index generation script, add generation time to HTML index ([#30060](https://github.com/vllm-project/vllm/pull/30060)) by @Harry-Chen
* [CI][AMD] Match Main CI Behavior By Skipping test_eplb_spec_decode In AMD CI ([#30006](https://github.com/vllm-project/vllm/pull/30006)) by @micah-wil
* [CI] Fix re import error ([#29973](https://github.com/vllm-project/vllm/pull/29973)) by @yewentao256
* [CI] fix docker image build by specifying merge-base commit id when downloading pre-compiled wheels ([#29930](https://github.com/vllm-project/vllm/pull/29930)) by @Harry-Chen
* [CI][DCP][Perf] reduce DCP CI execution time ([#29858](https://github.com/vllm-project/vllm/pull/29858)) by @pisceskkk
* [CI] Renovation of nightly wheel build & generation (take 2) ([#29838](https://github.com/vllm-project/vllm/pull/29838)) by @Harry-Chen
* [CI][AMD] spec_decode:eagle skip FLASH_ATTN for deepseek on ROCm ([#29827](https://github.com/vllm-project/vllm/pull/29827)) by @divakar-amd
* [ci] Make distributed 8 gpus test optional ([#29801](https://github.com/vllm-project/vllm/pull/29801)) by @khluu
* [CI] fix url-encoding behavior in nightly metadata generation ([#29787](https://github.com/vllm-project/vllm/pull/29787)) by @Harry-Chen
* [CI] Skip paddleocr_vl for transformer 4.57.3 ([#29758](https://github.com/vllm-project/vllm/pull/29758)) by @hl475
* [CI] Renovation of nightly wheel build & generation ([#29690](https://github.com/vllm-project/vllm/pull/29690)) by @Harry-Chen
* [CI] Add Async Eplb nightly CI tests ([#29385](https://github.com/vllm-project/vllm/pull/29385)) by @david6666666
* [CI][ROCm] Fix test_correctness_sliding_window ([#29243](https://github.com/vllm-project/vllm/pull/29243)) by @divakar-amd
* [CI][ROCm][tests/v1/e2e] Fix multiprocessing launch for the test ([#29123](https://github.com/vllm-project/vllm/pull/29123)) by @divakar-amd
* [CI] Fix Bad_words test for tokenizer encode/decode asymmetry ([#28193](https://github.com/vllm-project/vllm/pull/28193)) by @zhyajie

### ðŸ“š Documentation

* docs: update metrics design doc to use new vllm:kv_cache_usage_perc ([#30041](https://github.com/vllm-project/vllm/pull/30041)) by @haitwang-cloud
* [docs] Remove _total from counter metrics names ([#30028](https://github.com/vllm-project/vllm/pull/30028)) by @googs1025
* [Doc] clarify nightly builds in developer docs ([#30019](https://github.com/vllm-project/vllm/pull/30019)) by @Harry-Chen
* [Docs] Discuss api key limitations in security guide ([#29922](https://github.com/vllm-project/vllm/pull/29922)) by @russellb
* [DOC] Add Arm to list of compute resouces providers ([#29894](https://github.com/vllm-project/vllm/pull/29894)) by @fadara01
* [Doc] Update description disable_any_whitespace ([#29784](https://github.com/vllm-project/vllm/pull/29784)) by @FredericOdermatt
* [Doc] fix heading levels ([#29783](https://github.com/vllm-project/vllm/pull/29783)) by @KKKZOZ
* [Doc] Add allocate_slots parameter docs ([#29777](https://github.com/vllm-project/vllm/pull/29777)) by @maang-h
* [Doc]: Fix typo in fused_moe layer ([#29731](https://github.com/vllm-project/vllm/pull/29731)) by @BowTen
* [Doc]: fix code block rendering ([#29728](https://github.com/vllm-project/vllm/pull/29728)) by @dublc
* [Doc]: fixing typos in various files. ([#29717](https://github.com/vllm-project/vllm/pull/29717)) by @didier-durand
* [Docs] Add CLI reference doc for `vllm bench sweep plot_pareto` ([#29689](https://github.com/vllm-project/vllm/pull/29689)) by @hmellor
* [Doc]: fixing typos in multiple files. ([#29685](https://github.com/vllm-project/vllm/pull/29685)) by @didier-durand
* [Docs] Add SPLADE and Ultravox models to supported models documentation ([#29659](https://github.com/vllm-project/vllm/pull/29659)) by @wilsonwu
* [Doc] Reorganize benchmark docs ([#29658](https://github.com/vllm-project/vllm/pull/29658)) by @DarkLight1337
* [Doc] Improve abnormal information string ([#29655](https://github.com/vllm-project/vllm/pull/29655)) by @maang-h
* [Docs] Update supported models for Olmo 3 in tool calling documentation ([#29411](https://github.com/vllm-project/vllm/pull/29411)) by @wilsonwu

### ðŸ“¦ Miscellaneous

* [CI/Build] Update batch invariant test trigger ([#30080](https://github.com/vllm-project/vllm/pull/30080)) by @zhewenl
* Delete HF version of Phi 4 MM ([#30049](https://github.com/vllm-project/vllm/pull/30049)) by @hmellor
* Use Transformers v5 RoPE standardisation and validation ([#30046](https://github.com/vllm-project/vllm/pull/30046)) by @hmellor
* [Chore] Deprecate `merge_by_field_config` arg ([#30035](https://github.com/vllm-project/vllm/pull/30035)) by @DarkLight1337
* [Model Runner V2] Implement get_num_sampled_and_rejected kernel ([#30029](https://github.com/vllm-project/vllm/pull/30029)) by @WoosukKwon
* [Misc] Move functions into `PoolingMetadata` ([#30027](https://github.com/vllm-project/vllm/pull/30027)) by @DarkLight1337
* [Misc] Add docker build env for Ascend NPU ([#30015](https://github.com/vllm-project/vllm/pull/30015)) by @Potabk
* [release] install regex ([#30008](https://github.com/vllm-project/vllm/pull/30008)) by @khluu
* [CI/Build][AMD] Skip test on test_hybrid_attention_mamba_tensor_shapes on ROCm, requires FLASHINFER ([#29995](https://github.com/vllm-project/vllm/pull/29995)) by @rasmith
* [CI/Build] Add MM code path to Examples Test ([#29986](https://github.com/vllm-project/vllm/pull/29986)) by @zhewenl
* [Misc] Various cleanups for MM input processing ([#29970](https://github.com/vllm-project/vllm/pull/29970)) by @DarkLight1337
* Access `partial_rotary_factor` from `rope_parameters` ([#29966](https://github.com/vllm-project/vllm/pull/29966)) by @hmellor
* [GPU Backend] [Doc]: Remove duplicate statements on missing GPU wheels. ([#29962](https://github.com/vllm-project/vllm/pull/29962)) by @ioghiban
* [PCP&DCP] move CUDAGraph check for PCP&DCP to the check func of platforms ([#29952](https://github.com/vllm-project/vllm/pull/29952)) by @pisceskkk
* [Misc] Allow `fetch_*` utils to access local files by default ([#29932](https://github.com/vllm-project/vllm/pull/29932)) by @DarkLight1337
* [Kernels] Remove BatchedTritonOrDeepGemmExperts and default fallback to Triton ([#29929](https://github.com/vllm-project/vllm/pull/29929)) by @bnellnm
* Reverting re-direction to amd_mi355_X. ([#29914](https://github.com/vllm-project/vllm/pull/29914)) by @Alexei-V-Ivanov-AMD
* Mark DBO test as flaky on b200 for Distributed B200 test ([#29913](https://github.com/vllm-project/vllm/pull/29913)) by @dougbtv
* [CI/Build] Avoid duplicate empty inputs test for common multimodal generation tests ([#29907](https://github.com/vllm-project/vllm/pull/29907)) by @Isotr0py
* SigLIP example add chat_template ([#29902](https://github.com/vllm-project/vllm/pull/29902)) by @piood
* Update AMD-CI testing mirror (as of 2025-12-02) ([#29898](https://github.com/vllm-project/vllm/pull/29898)) by @Alexei-V-Ivanov-AMD
* feat(model): Add BitsAndBytes quantization support for Qwen3-Omni-MoE ([#29896](https://github.com/vllm-project/vllm/pull/29896)) by @Navanit-git
* [Chore]: Reorganize gguf utils funtions under `transformers_utils` ([#29891](https://github.com/vllm-project/vllm/pull/29891)) by @Isotr0py
* [CPU Backend] [Doc]: Update Installation Docs for CPUs ([#29868](https://github.com/vllm-project/vllm/pull/29868)) by @ioghiban
* [Chore] Use `tokenizer.encode` and `tokenizer.decode` directly ([#29851](https://github.com/vllm-project/vllm/pull/29851)) by @DarkLight1337
* [CI/Build][AMD] Skip test_shared_storage_connector_hashes in test_shared_storage_connector.py due to hipErrorLaunchFailure when calling .cpu() ([#29839](https://github.com/vllm-project/vllm/pull/29839)) by @rasmith
* enable multi-node in external launcher mode ([#29833](https://github.com/vllm-project/vllm/pull/29833)) by @xieyangxu
* [CI/Build] Fixes missing runtime dependencies ([#29822](https://github.com/vllm-project/vllm/pull/29822)) by @bbartels
* Revert #29787 and #29690 ([#29815](https://github.com/vllm-project/vllm/pull/29815)) by @khluu
* [Misc] Update conftest for entrypoints/sagemaker test folder ([#29799](https://github.com/vllm-project/vllm/pull/29799)) by @zhaozuy
* Update FAQ on interleaving sliding windows support ([#29796](https://github.com/vllm-project/vllm/pull/29796)) by @finbarrtimbers
* [Chore] Move tokenizer initialization methods ([#29793](https://github.com/vllm-project/vllm/pull/29793)) by @DarkLight1337
* [Hardware][AMD] Remove ROCm skip conditions for transformers backend tests ([#29782](https://github.com/vllm-project/vllm/pull/29782)) by @Abdennacer-Badaoui
* [Misc] Throw error on unintended access to scheduler_config.max_model_len ([#29771](https://github.com/vllm-project/vllm/pull/29771)) by @frank-wei
* Bump actions/setup-python from 6.0.0 to 6.1.0 ([#29768](https://github.com/vllm-project/vllm/pull/29768)) by @app/dependabot
* [Misc] Unify tokenizer registration ([#29767](https://github.com/vllm-project/vllm/pull/29767)) by @DarkLight1337
* [Model Runner V2] Use packed mask for prompt bin counts ([#29756](https://github.com/vllm-project/vllm/pull/29756)) by @WoosukKwon
* [crashfix] Eagle + multimodal can crash on mm cache miss ([#29750](https://github.com/vllm-project/vllm/pull/29750)) by @mickaelseznec
* [Misc]Remove redundant hidden_size property in ModelConfig ([#29749](https://github.com/vllm-project/vllm/pull/29749)) by @charlotte12l
* [Quantization] Enable compressed-tensors AWQ for Turing GPU ([#29732](https://github.com/vllm-project/vllm/pull/29732)) by @Isotr0py
* [Misc] Update `TokenizerLike` interface and move `get_cached_tokenizer` ([#29730](https://github.com/vllm-project/vllm/pull/29730)) by @DarkLight1337
* [Chore] Move `detokenizer_utils` to `vllm/tokenizers` ([#29727](https://github.com/vllm-project/vllm/pull/29727)) by @DarkLight1337
* [Chore] Enable passing `tokenizer=None` into MM processor ([#29724](https://github.com/vllm-project/vllm/pull/29724)) by @DarkLight1337
* [Model Runner V2] Fuse penalties and temperature into single kernel ([#29720](https://github.com/vllm-project/vllm/pull/29720)) by @WoosukKwon
* [Model Runner V2] Add sample/ directory and reorganize files ([#29719](https://github.com/vllm-project/vllm/pull/29719)) by @WoosukKwon
* [Model Runner V2] Don't use UVA buffer for prefill_len  ([#29713](https://github.com/vllm-project/vllm/pull/29713)) by @WoosukKwon
* [Model Runner V2] Refactor prefill token preparation ([#29712](https://github.com/vllm-project/vllm/pull/29712)) by @WoosukKwon
*  SM120 / NVFP4: add device guard and runtime SM dispatch to cutlass_scaled_fp4_mm ([#29711](https://github.com/vllm-project/vllm/pull/29711)) by @hholtmann
* [KVConnector] remove unused code (the model aware kv ops class) ([#29709](https://github.com/vllm-project/vllm/pull/29709)) by @KuntaiDu
* [LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 ([#29708](https://github.com/vllm-project/vllm/pull/29708)) by @xyang16
* [KVConnector] Remove v0-related kv connector components such as kv pipe and kv lookup buffer ([#29705](https://github.com/vllm-project/vllm/pull/29705)) by @KuntaiDu
* [Model Runner V2] Support penalties using bin counts ([#29703](https://github.com/vllm-project/vllm/pull/29703)) by @WoosukKwon
* Revert "[LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 (#28971)" ([#29697](https://github.com/vllm-project/vllm/pull/29697)) by @hl475
* [compile] Include `enable_sleep_mode` into caching factors. ([#29696](https://github.com/vllm-project/vllm/pull/29696)) by @zhxchen17
* [Misc] Refactor tokenizer interface ([#29693](https://github.com/vllm-project/vllm/pull/29693)) by @DarkLight1337
* [CI/Build] Rework CPU multimodal processor test ([#29684](https://github.com/vllm-project/vllm/pull/29684)) by @Isotr0py
* [Chore] Rename `Processor` to `InputProcessor` ([#29682](https://github.com/vllm-project/vllm/pull/29682)) by @DarkLight1337
* [Misc] Remove redundant `ClassRegistry` ([#29681](https://github.com/vllm-project/vllm/pull/29681)) by @DarkLight1337
* [Chore]: Reorganize model repo operating functions in `transformers_utils` ([#29680](https://github.com/vllm-project/vllm/pull/29680)) by @Isotr0py
* [Misc] Remove `yapf` directives ([#29675](https://github.com/vllm-project/vllm/pull/29675)) by @DarkLight1337
* [mypy] Enable type checking for more directories ([#29674](https://github.com/vllm-project/vllm/pull/29674)) by @DarkLight1337
* hfrunner.classify should return list[list[float]] not list[str] ([#29671](https://github.com/vllm-project/vllm/pull/29671)) by @nwaughachukwuma
* [Optimization] Early return for `_apply_matches` and `_iter_placeholders` ([#29668](https://github.com/vllm-project/vllm/pull/29668)) by @DarkLight1337
* [mypy] Pass type checking for `vllm/utils` and `vllm/v1/pool` ([#29666](https://github.com/vllm-project/vllm/pull/29666)) by @DarkLight1337
* [CPU] Update torch 2.9.1 for CPU backend ([#29664](https://github.com/vllm-project/vllm/pull/29664)) by @bigPYJ1151
* [Misc] Remove redundant attention var constants ([#29650](https://github.com/vllm-project/vllm/pull/29650)) by @DarkLight1337
* Revert "[CPU]Update CPU PyTorch to 2.9.0 (#29589)" ([#29647](https://github.com/vllm-project/vllm/pull/29647)) by @DarkLight1337
* [Multimodal][Core] Optimize multimodal preprocessing cache by hashing image bytes instead of pixel values ([#29621](https://github.com/vllm-project/vllm/pull/29621)) by @ImaGoodFella
* [LoRA] Cleanup LoRA unused code ([#29611](https://github.com/vllm-project/vllm/pull/29611)) by @jeejeelee
* [CPU]Parallelize over tokens in int4 moe ([#29600](https://github.com/vllm-project/vllm/pull/29600)) by @xiangze-arm
* [Multimodal][Speculative Decoding]Eagle3 mm support, enablement on qwen3vl ([#29594](https://github.com/vllm-project/vllm/pull/29594)) by @EanWang211123
* [CPU]Update CPU PyTorch to 2.9.0 ([#29589](https://github.com/vllm-project/vllm/pull/29589)) by @scydas
* [Misc][Profiling] Make PyTorch profiler gzip and CUDA time dump configurable ([#29568](https://github.com/vllm-project/vllm/pull/29568)) by @zhangruoxu
* [responsesAPI][4] fix responseOutputItem Kimi K2 thinking bug ([#29555](https://github.com/vllm-project/vllm/pull/29555)) by @qandrew
* [responsesAPI] support input output messages for non harmony models ([#29549](https://github.com/vllm-project/vllm/pull/29549)) by @qandrew
* [multimodal][test] Reduce memory utilization for test_siglip to avoid OOM  ([#29504](https://github.com/vllm-project/vllm/pull/29504)) by @zhxchen17
* [docker] Build CUDA kernels in separate Docker stage for faster rebuilds ([#29452](https://github.com/vllm-project/vllm/pull/29452)) by @amrmahdi
* Updated CI mirror 2025-11-25 ([#29434](https://github.com/vllm-project/vllm/pull/29434)) by @Alexei-V-Ivanov-AMD
* Guard FlashInfer sampler using the same check as FlashInfer attention backend ([#29415](https://github.com/vllm-project/vllm/pull/29415)) by @hmellor
* [responsesAPI][3] ResponsesParser to set up non harmony MCP ([#29413](https://github.com/vllm-project/vllm/pull/29413)) by @qandrew
* [vLLM Benchmark Suite] Add default parameters section and update CPU benchmark cases ([#29381](https://github.com/vllm-project/vllm/pull/29381)) by @louie-tsai
* [examples] Resettle pooling examples. ([#29365](https://github.com/vllm-project/vllm/pull/29365)) by @noooop
* [Attention][CUDAGraph] Remove CG padding from attention backends ([#29352](https://github.com/vllm-project/vllm/pull/29352)) by @MatthewBonanni
* Revert "Supress verbose logs from model_hosting_container_standards (â€¦ ([#29335](https://github.com/vllm-project/vllm/pull/29335)) by @HappyAmazonian
* Validating Runai Model Streamer Integration with S3 Object Storage ([#29320](https://github.com/vllm-project/vllm/pull/29320)) by @noa-neria
* [CI/Build]: make it possible to build with a free-threaded interpreter ([#29241](https://github.com/vllm-project/vllm/pull/29241)) by @rgommers
* [aot_compile]change VLLM backend to read fake args from example_value ([#29104](https://github.com/vllm-project/vllm/pull/29104)) by @laithsakka
* [LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 ([#28971](https://github.com/vllm-project/vllm/pull/28971)) by @xyang16
* [Ascend]: Fixed the issue where OOT Platform vllm-ascend could not enable SP in Eager mode ([#28935](https://github.com/vllm-project/vllm/pull/28935)) by @leo-pony
* bugfix: correct attn output with base 2 or e ([#28840](https://github.com/vllm-project/vllm/pull/28840)) by @staugust
* [CI/Build][AMD] Add Llama4 Maverick FP8 to AMD CI ([#28695](https://github.com/vllm-project/vllm/pull/28695)) by @zhewenl
* [V0 deprecation] Clean up legacy paged attention helper functions ([#28043](https://github.com/vllm-project/vllm/pull/28043)) by @Isotr0py
* [MoE] CuteDSL MoE with Nvfp4 DeepEP dispatch  ([#27141](https://github.com/vllm-project/vllm/pull/27141)) by @wenscarl
* Improve enable chunked_prefill & prefix_caching logic. ([#26623](https://github.com/vllm-project/vllm/pull/26623)) by @noooop
* Abstract eplb algo ([#26471](https://github.com/vllm-project/vllm/pull/26471)) by @Mercykid-bash
* [v1] Add real sliding window calculation to FlexAttention direct BlockMask building ([#26015](https://github.com/vllm-project/vllm/pull/26015)) by @Isotr0py
* [P/D] Introduce Mooncake Transfer Engine as kv_connector ([#24718](https://github.com/vllm-project/vllm/pull/24718)) by @dtcccc
* [Misc] Add ReplicaId to Ray metrics ([#24267](https://github.com/vllm-project/vllm/pull/24267)) by @eicherseiji

## Contributors

@Abdennacer-Badaoui, @Alexei-V-Ivanov-AMD, @AndreasKaratzas, @BowTen, @BoyuanFeng, @DarkLight1337, @EanWang211123, @Flink-ddd, @FredericOdermatt, @HDCharles, @HappyAmazonian, @Harry-Chen, @Iceber, @ImaGoodFella, @Isotr0py, @KKKZOZ, @KuntaiDu, @LucasWilkinson, @LuminolT, @MaoJianwei, @MatthewBonanni, @Mercykid-bash, @Navanit-git, @Peng-YM, @Potabk, @SageMoore, @Victor49152, @WoosukKwon, @Xu-Wenqing, @ZJY0516, @a4lg, @alexm-redhat, @amrmahdi, @angelayi, @app/copilot-swe-agent, @app/dependabot, @arpitkh101, @avigny, @bbartels, @benchislett, @bigPYJ1151, @bnellnm, @charlifu, @charlotte12l, @chaunceyjiang, @daniel-salib, @david6666666, @didier-durand, @divakar-amd, @dougbtv, @dtcccc, @dublc, @eicherseiji, @elizabetht, @elvischenv, @faaany, @fadara01, @finbarrtimbers, @frank-wei, @ganyi1996ppo, @gausah01, @gmagogsfm, @googs1025, @guodongxiaren, @haitwang-cloud, @hdlj-h, @hholtmann, @hl475, @hmellor, @hyongtao-code, @ioghiban, @jcyang43, @jeejeelee, @jinzhen-lin, @jthomson04, @juliendenize, @khluu, @knlnguyen1802, @laithsakka, @lengrongfu, @leo-pony, @louie-tsai, @maang-h, @markmc, @mertunsall, @micah-wil, @mickaelseznec, @mostrowskix, @njhill, @noa-neria, @noooop, @nwaughachukwuma, @omera-nv, @piood, @pisceskkk, @qGentry, @qandrew, @rasmith, @rgommers, @russellb, @sangbumlikeagod, @sarckk, @scydas, @shivampr, @staugust, @taoyun951753, @tjtanaa, @usberkeley, @varun-sundar-rabindranath, @wenscarl, @wilsonwu, @wkcn, @wushidonguc, @xiangze-arm, @xieyangxu, @xyang16, @yeqcharlotte, @yewentao256, @ywang96, @zhangruoxu, @zhaozuy, @zhewenl, @zhuohan123, @zhxchen17, @zhyajie

