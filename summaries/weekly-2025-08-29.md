# Weekly Release Notes for vllm-project/vllm (2025-08-29)

## What's Changed

### ‚ú® Features & Enhancements

* Add scale_config.yml file for Meta autoscalers for GH Actions ([#23840](https://github.com/vllm-project/vllm/pull/23840)) by @jeanschmidt
* Add vLLM Korea Meetup in the README.md and meetups.md ([#23746](https://github.com/vllm-project/vllm/pull/23746)) by @rebel-hongseok
* feat: add triton fused moe config for GLM-4.5-Air-FP8 on B200 ([#23695](https://github.com/vllm-project/vllm/pull/23695)) by @zixuanzhang226
* [Feature] Add Hopper DeepGEMM E8M0 for DeepSeekV3.1 scale_fmt ([#23666](https://github.com/vllm-project/vllm/pull/23666)) by @yewentao256
* Add deprecation warning for lora_extra_vocab_size ([#23635](https://github.com/vllm-project/vllm/pull/23635)) by @ahengljh
* [Feature] Add `VLLM_DISABLE_PAD_FOR_CUDAGRAPH` to Avoid Hang Issue ([#23595](https://github.com/vllm-project/vllm/pull/23595)) by @yewentao256
* feat: add usage to TranscriptionResponse (text and json response_format) ([#23576](https://github.com/vllm-project/vllm/pull/23576)) by @gcalmettes
* [Feature] models: pass layer prefix to replace_linear_class for per-layer quantization routing. Addresses #23239 ([#23556](https://github.com/vllm-project/vllm/pull/23556)) by @Shrey1306
* [Feature][Responses API] Support MCP tool in background mode ([#23494](https://github.com/vllm-project/vllm/pull/23494)) by @wuhang2014
* Support DeepSeek-V3.1 tool call ([#23454](https://github.com/vllm-project/vllm/pull/23454)) by @Xu-Wenqing
* Add glm4.5v tp2,4 fp8 config on H100_80GB ([#23443](https://github.com/vllm-project/vllm/pull/23443)) by @chenxi-yang
* Add unit tests for batched guided and non-guided requests ([#23389](https://github.com/vllm-project/vllm/pull/23389)) by @sarckk
* [Feature] Enable DeepGEMM Linear on B200; 1.5% E2E throughput improvement ([#23351](https://github.com/vllm-project/vllm/pull/23351)) by @yewentao256
* Support FlashAttention Backend for Hybrid SSM Models ([#23299](https://github.com/vllm-project/vllm/pull/23299)) by @heheda12345
* add an env var for path to pre-downloaded flashinfer cubin files ([#22675](https://github.com/vllm-project/vllm/pull/22675)) by @842974287

### üêõ Bug Fixes

* [bugfix] [spec-decoding] fix data race in sample_recovered_tokens_kernel (vLLM v1) ([#23829](https://github.com/vllm-project/vllm/pull/23829)) by @He-Jingkai
* [Bugfix] Fix benchmark_moe.py for blockwise fp8. ([#23823](https://github.com/vllm-project/vllm/pull/23823)) by @crischeng
* [BugFix][Spec Decode] Use float64 for uniform_probs ([#23803](https://github.com/vllm-project/vllm/pull/23803)) by @WoosukKwon
* Fix pre-commit on main ([#23747](https://github.com/vllm-project/vllm/pull/23747)) by @hmellor
* [BugFix][FlashInfer] Fix potential race condition for paged_kv_indptr_cpu ([#23737](https://github.com/vllm-project/vllm/pull/23737)) by @WoosukKwon
* [Bugfix] Fix task field initialization when PYTHONOPTIMIZE is enabled ([#23718](https://github.com/vllm-project/vllm/pull/23718)) by @cndoit18
* [Bugfix] Fix for V1 priority scheduling crashes at preemption ([#23713](https://github.com/vllm-project/vllm/pull/23713)) by @Hanchenli
* [Bugfix] when set offline model running error ([#23711](https://github.com/vllm-project/vllm/pull/23711)) by @lengrongfu
* [Bugfix] Lazy import gpt_oss_triton_kernels_moe for mxfp4 ([#23678](https://github.com/vllm-project/vllm/pull/23678)) by @mgoin
* [Bugfix] Fix incorrect original shape in hashing ([#23672](https://github.com/vllm-project/vllm/pull/23672)) by @DarkLight1337
* [Bugfix] Fix Marlin NVFP4 for modelopt ([#23659](https://github.com/vllm-project/vllm/pull/23659)) by @mgoin
* fix pynccl reduce_scatter ([#23648](https://github.com/vllm-project/vllm/pull/23648)) by @youzhedian
* [Bugfix] Fix cuda event usage with CPU model runner ([#23643](https://github.com/vllm-project/vllm/pull/23643)) by @bigPYJ1151
* [Bugfix] Add missing enable_log_outputs parameter to init_app_state function ([#23634](https://github.com/vllm-project/vllm/pull/23634)) by @lordmathis
* Fix writing benchmark results with tuple keys ([#23633](https://github.com/vllm-project/vllm/pull/23633)) by @huydhn
* Fix CLI parameter documentation inconsistency in pooling_models.md ([#23630](https://github.com/vllm-project/vllm/pull/23630)) by @oneraghavan
* [Bugfix] fix bf16 multimodal model hash ([#23623](https://github.com/vllm-project/vllm/pull/23623)) by @yuekaizhang
* [Bugfix][gpt-oss] passing the cache config in gpt-oss ([#23613](https://github.com/vllm-project/vllm/pull/23613)) by @frank-wei
* [Bugfix] Fix Qwen25VL packed_modules_mapping ([#23604](https://github.com/vllm-project/vllm/pull/23604)) by @jeejeelee
* [Bug] Fix DeepGEMM Env Control ([#23591](https://github.com/vllm-project/vllm/pull/23591)) by @yewentao256
* [fix] fix seed-oss-parser ([#23560](https://github.com/vllm-project/vllm/pull/23560)) by @FoolPlayer
* Fix nits from #20059 ([#23548](https://github.com/vllm-project/vllm/pull/23548)) by @hmellor
* [Bugfix] Fix scheduling when repeated images in one request ([#23544](https://github.com/vllm-project/vllm/pull/23544)) by @ywang96
* [Bugfix] fix when config.yaml config value is list parse error ([#23528](https://github.com/vllm-project/vllm/pull/23528)) by @lengrongfu
* [Bugfix] Allow dynamic number of patches for llava_onevision ([#23525](https://github.com/vllm-project/vllm/pull/23525)) by @DarkLight1337
* [Bugfix] Fix Qwen2.5-VL quantized model weights loading ([#23512](https://github.com/vllm-project/vllm/pull/23512)) by @zifeitong
* [Fix] DeepSeek V3.1 tool parser error message ([#23492](https://github.com/vllm-project/vllm/pull/23492)) by @skyloevil
* [Bugfix] Fix Qwen3 MoE GPTQ inference ([#23490](https://github.com/vllm-project/vllm/pull/23490)) by @Isotr0py
* fix incompatibililty with non cuda platform for nvfp4 ([#23478](https://github.com/vllm-project/vllm/pull/23478)) by @luccafong
* [Bugfix] Add strong reference to CUDA pluggable allocator callbacks ([#23477](https://github.com/vllm-project/vllm/pull/23477)) by @22quinn
* [Bugfix] Fix broken Florence-2 model ([#23426](https://github.com/vllm-project/vllm/pull/23426)) by @Isotr0py
* [Bugfix] Fix Dense module loading for sentence-transformers embedding models (simplified V2) ([#23408](https://github.com/vllm-project/vllm/pull/23408)) by @FFFfff1FFFfff
* [Bugfix][V1][P/D]Fix the issue where repeated requests for the same input produce abnormal outputs for P2pNcclConnector ([#23403](https://github.com/vllm-project/vllm/pull/23403)) by @Abatom
* [BugFix] Fix `MinPLogitsProcessor.update_states()` ([#23401](https://github.com/vllm-project/vllm/pull/23401)) by @njhill
* [BugFix] Fix batch updates for pooling models ([#23398](https://github.com/vllm-project/vllm/pull/23398)) by @njhill
* [Bugfix] Fix pooling models on non-CUDA devices ([#23392](https://github.com/vllm-project/vllm/pull/23392)) by @bigPYJ1151
* [BugFix] Fix the issue where image embeddings were incorrectly split.‚Ä¶ ([#23366](https://github.com/vllm-project/vllm/pull/23366)) by @bppps
* [Bugfix]: Installing dev environment due to pydantic incompatible version ([#23353](https://github.com/vllm-project/vllm/pull/23353)) by @hickeyma
* [Bugfix] Add fake mode around passes ([#23349](https://github.com/vllm-project/vllm/pull/23349)) by @angelayi
* [Bugfix]: Qwen3 Coder Tool Parser ([#23099](https://github.com/vllm-project/vllm/pull/23099)) by @ranpox
* [Bugfix] UnboundLocalError when GptOss reasoning specified ([#23054](https://github.com/vllm-project/vllm/pull/23054)) by @coval3nte
* [BugFix][AMD][Quantization] Fix torch.compile issue where wvSplitKQ not being called when it should when using quantized FP8 model ([#22281](https://github.com/vllm-project/vllm/pull/22281)) by @rasmith
* [Fix] Bump triton version in rocm-build requirements ([#21630](https://github.com/vllm-project/vllm/pull/21630)) by @bringlein
* [BugFix] Fix topk_softmax assert ([#19764](https://github.com/vllm-project/vllm/pull/19764)) by @ProExpertProg

### ‚ö°Ô∏è Performance

* [Perf] Tune configs for triton block fp8 gemm H100/H200 ([#23748](https://github.com/vllm-project/vllm/pull/23748)) by @mgoin
* [Perf] Add Triton config for DeepSeek V3 FP8 EP32 H200 ([#23504](https://github.com/vllm-project/vllm/pull/23504)) by @minosfuture
* [Perf] Remove duplicated NVFP4 blockscales to save memory ([#23379](https://github.com/vllm-project/vllm/pull/23379)) by @mgoin
* Optimize input preparation for FlashInfer [2/N] ([#23174](https://github.com/vllm-project/vllm/pull/23174)) by @WoosukKwon
* [PERF] PyTorch Symmetric Memory All-Reduce ([#20759](https://github.com/vllm-project/vllm/pull/20759)) by @ilmarkov

### ü§ñ Model Support

* [Model][gpt-oss] Support DP+EP for GPT-OSS with FlashInfer trtllm-gen MoE ([#23819](https://github.com/vllm-project/vllm/pull/23819)) by @nvpohanh
* [Model] [gpt-oss] fix gpt-oss pp support ([#23815](https://github.com/vllm-project/vllm/pull/23815)) by @ZJY0516
* [Model] Merge `SupportsMultiModalWithRawInput` with `SupportsMultiModal` ([#23749](https://github.com/vllm-project/vllm/pull/23749)) by @DarkLight1337
* [Model] Enable native HF format InternVL support ([#23742](https://github.com/vllm-project/vllm/pull/23742)) by @Isotr0py
* [Model] Explicit `default_pooling_type` interface ([#23736](https://github.com/vllm-project/vllm/pull/23736)) by @DarkLight1337
* [Model] Interface to enable batch-level DP support ([#23733](https://github.com/vllm-project/vllm/pull/23733)) by @DarkLight1337
* [Model] Add PP support and VLM backbone compatability for GPT-OSS ([#23680](https://github.com/vllm-project/vllm/pull/23680)) by @Isotr0py
* [Model] Enable video support for InternVL3.5 models ([#23658](https://github.com/vllm-project/vllm/pull/23658)) by @Isotr0py
* [Model] fix DeepSeek e_score_correction_bias dtype to fp32 ([#23640](https://github.com/vllm-project/vllm/pull/23640)) by @jeejeelee
* [model] support qwen2audio embedding input ([#23625](https://github.com/vllm-project/vllm/pull/23625)) by @yuekaizhang
* [model] Support MiniCPM-V 4.5 ([#23586](https://github.com/vllm-project/vllm/pull/23586)) by @tc-mb
* [gpt-oss] Enable unit test for response API harmony integration ([#23533](https://github.com/vllm-project/vllm/pull/23533)) by @heheda12345
* [Model] Enable BLOOM on V1 ([#23488](https://github.com/vllm-project/vllm/pull/23488)) by @DarkLight1337
* [gpt-oss] Streaming Output for Python Tool ([#23409](https://github.com/vllm-project/vllm/pull/23409)) by @ZJY0516
* [Model] Add Ovis2.5 PP support ([#23405](https://github.com/vllm-project/vllm/pull/23405)) by @Isotr0py
* [Model] Support DP for ViT on MiniCPM-V-4 ([#23327](https://github.com/vllm-project/vllm/pull/23327)) by @david6666666
* [gpt-oss] use reasoning channel for reasoning text in serving_chat ([#22920](https://github.com/vllm-project/vllm/pull/22920)) by @yuguo68
* [gpt-oss] add input/output usage in responses api when harmony context is leveraged ([#22667](https://github.com/vllm-project/vllm/pull/22667)) by @gcalmettes
* [Model] Add Ernie4.5 VL Model Support ([#22514](https://github.com/vllm-project/vllm/pull/22514)) by @CSWYF3634076

### üîå Hardware & Backend

* [ROCm][Fix] Fix rocm build caused by #23791 ([#23847](https://github.com/vllm-project/vllm/pull/23847)) by @charlifu
* [XPU]fix cuda event used in XPU model runner ([#23708](https://github.com/vllm-project/vllm/pull/23708)) by @jikunshang
* [NVIDIA] Support SiluMul + NVFP4 quant fusion ([#23671](https://github.com/vllm-project/vllm/pull/23671)) by @elvischenv
* [TPU][Bugfix] Fixes prompt_token_ids error in tpu tests. ([#23574](https://github.com/vllm-project/vllm/pull/23574)) by @patemotter
* [ROCm] Starting to add AMD code reviewers for ROCm components ([#23496](https://github.com/vllm-project/vllm/pull/23496)) by @hongxiayang
* [ROCm][Aiter] Add triton fp8 bmm kernel for mla ([#23264](https://github.com/vllm-project/vllm/pull/23264)) by @divakar-amd
* [XPU] Delay BF16 check to worker init for spawn compatibility ([#22979](https://github.com/vllm-project/vllm/pull/22979)) by @chaojun-zhang
* [XPU] support data parallel for MoE models on XPU ([#22887](https://github.com/vllm-project/vllm/pull/22887)) by @chaojun-zhang
* [NVIDIA] Support Flashinfer TRTLLM FP8-q/kv NVFP4-out Attention Kernel ([#22703](https://github.com/vllm-project/vllm/pull/22703)) by @elvischenv
* [XPU] Add xpu torch.compile support ([#22609](https://github.com/vllm-project/vllm/pull/22609)) by @jikunshang
* [XPU] Fix OOM issue for data parallel with Ray backend ([#22500](https://github.com/vllm-project/vllm/pull/22500)) by @faaany

### ‚öôÔ∏è Refactoring & Core

* [Kernel] cuda kernels for upcoming decode context parallel feature ([#23791](https://github.com/vllm-project/vllm/pull/23791)) by @youzhedian
* [Core] Asynchronous h2d in merge_multimodal_embeddings via pinned memory. ([#23686](https://github.com/vllm-project/vllm/pull/23686)) by @huachenheli
* [Frontend] Optimize beam search performance by limiting concurrency ([#23599](https://github.com/vllm-project/vllm/pull/23599)) by @heheda12345
* [Refactor] Pass `tokenizer` explicitly instead of binding to prompt update ([#23542](https://github.com/vllm-project/vllm/pull/23542)) by @DarkLight1337
* [Refactor] Refactor persistent buffers with CpuGpuBuffer  ([#23515](https://github.com/vllm-project/vllm/pull/23515)) by @WoosukKwon
* [Refactor] Dynamic `target` and `content` for prompt updates ([#23411](https://github.com/vllm-project/vllm/pull/23411)) by @DarkLight1337
* Remove graph_pool as member of VllmBackend and argument to CUDAGraphWrapper ([#23385](https://github.com/vllm-project/vllm/pull/23385)) by @app/copilot-swe-agent
* [Core] Support custom executor qualname ([#23314](https://github.com/vllm-project/vllm/pull/23314)) by @22quinn
* [Kernel] Add fused grouped_topk kernel for MoE ([#23274](https://github.com/vllm-project/vllm/pull/23274)) by @xyang16
* [kernel] Support W4A8 on Hopper ([#23198](https://github.com/vllm-project/vllm/pull/23198)) by @czhu-cohere
* [Core] Use key-only cache for `BaseMultiModalProcessor` ([#23018](https://github.com/vllm-project/vllm/pull/23018)) by @DarkLight1337
* [Frontend] Add --log-error-stack to print stack trace for error response ([#22960](https://github.com/vllm-project/vllm/pull/22960)) by @heheda12345
* [Kernel] Added flashinfer fp8 per-tensor gemms ([#22895](https://github.com/vllm-project/vllm/pull/22895)) by @nvjullin
* [Core][Multimodal] Track encode cache entries by mm_hash and enable embedding sharing between requests ([#22711](https://github.com/vllm-project/vllm/pull/22711)) by @fake0fan
* [Kernel] Add FP8 support with FlashMLA backend ([#22668](https://github.com/vllm-project/vllm/pull/22668)) by @MatthewBonanni

### üîß Build, CI & Testing

* [CI] Fix linting error on main ([#23835](https://github.com/vllm-project/vllm/pull/23835)) by @tdoublep
* [CI] make all multi-gpu weight loading tests run nightly ([#23792](https://github.com/vllm-project/vllm/pull/23792)) by @killershrimp
* [CI] enable idefics3 and fuyu-8b test in multimodal test ([#23790](https://github.com/vllm-project/vllm/pull/23790)) by @ZJY0516
* [ci] breaks down V1 Test into 3 groups of approx 30 minutes runtime ([#23757](https://github.com/vllm-project/vllm/pull/23757)) by @jeanschmidt
* [test][RL] Add sleep level 2 test and fix reload with sleep mode ([#23521](https://github.com/vllm-project/vllm/pull/23521)) by @22quinn
* [CI] Add end-to-end V1 min_tokens test coverage ([#22495](https://github.com/vllm-project/vllm/pull/22495)) by @arjunbreddy22
* [CI] [Doc]: Add GH Action for auto labeling issues with `rocm` tag ([#20988](https://github.com/vllm-project/vllm/pull/20988)) by @vllmellm

### üìö Documentation

* [Doc]: fix typos in Python scripts ([#23828](https://github.com/vllm-project/vllm/pull/23828)) by @didier-durand
* [Doc]: fix typos in .md files (including those of #23751) ([#23825](https://github.com/vllm-project/vllm/pull/23825)) by @didier-durand
* [Doc]: upgrade version of crate-ci tool for improved typo detection ([#23755](https://github.com/vllm-project/vllm/pull/23755)) by @didier-durand
* [Docs] Fix warnings in `mkdocs build` (continued) ([#23743](https://github.com/vllm-project/vllm/pull/23743)) by @Zerohertz
* [Docs] Fix a 1-2-3 list and style issues in tpu.md ([#23729](https://github.com/vllm-project/vllm/pull/23729)) by @windsonsea
* [Docs] Fix an admonition important ([#23726](https://github.com/vllm-project/vllm/pull/23726)) by @windsonsea
* [Docs] Fix math rendering in docs ([#23676](https://github.com/vllm-project/vllm/pull/23676)) by @hmellor
* [Docs] [V1] [Hybrid] Update docs to remove FlashInfer constraint for hybrid models ([#23665](https://github.com/vllm-project/vllm/pull/23665)) by @tdoublep
* [Docs] Move quant supported hardware table to README ([#23663](https://github.com/vllm-project/vllm/pull/23663)) by @hmellor
* [Docs] Reduce requirements for docs build ([#23651](https://github.com/vllm-project/vllm/pull/23651)) by @hmellor
* [Docs] Fix warnings in `mkdocs build` ([#23649](https://github.com/vllm-project/vllm/pull/23649)) by @Zerohertz
* [Docs] Fix broken links to `docs/api/summary.md` ([#23637](https://github.com/vllm-project/vllm/pull/23637)) by @hmellor
* [Doc]: fix various spelling issues in multiple files ([#23636](https://github.com/vllm-project/vllm/pull/23636)) by @didier-durand
* [Docs] Remove in-tree Gaudi install instructions ([#23628](https://github.com/vllm-project/vllm/pull/23628)) by @hmellor
* [Docs] Update Documentation of Cohere Command-A Models ([#23584](https://github.com/vllm-project/vllm/pull/23584)) by @Terrencezzj
* [Docs] Fix titles for multi-file examples that are rendered in the docs ([#23573](https://github.com/vllm-project/vllm/pull/23573)) by @hmellor
* [Doc] Add caution for API server scale-out ([#23550](https://github.com/vllm-project/vllm/pull/23550)) by @DarkLight1337
* [Doc] Update the doc for log probs + prefix caching ([#23399](https://github.com/vllm-project/vllm/pull/23399)) by @heheda12345
* [Doc]: fix various typos in multiple files ([#23179](https://github.com/vllm-project/vllm/pull/23179)) by @didier-durand
* [doc] Hybrid KV Cache Manager design doc ([#22688](https://github.com/vllm-project/vllm/pull/22688)) by @heheda12345

### üì¶ Miscellaneous

* [V0 Deprecation] Remove V0 Samplers test ([#23862](https://github.com/vllm-project/vllm/pull/23862)) by @WoosukKwon
* [Log] Use Debug Once for DeepGEMM E8M0 When not Enabled ([#23858](https://github.com/vllm-project/vllm/pull/23858)) by @yewentao256
* chore: build release image by default ([#23852](https://github.com/vllm-project/vllm/pull/23852)) by @simon-mo
* [CI/Build][Bugfix] Fix Qwen VL tests on CPU ([#23818](https://github.com/vllm-project/vllm/pull/23818)) by @bigPYJ1151
* Disable `torch.compile` for dynamic rope models in Transformers backend ([#23738](https://github.com/vllm-project/vllm/pull/23738)) by @hmellor
* [FlashInfer] Cache hyper params in metadata builder ([#23732](https://github.com/vllm-project/vllm/pull/23732)) by @WoosukKwon
* [Misc] Move CpuGpuBuffer to vllm/v1/utils.py ([#23728](https://github.com/vllm-project/vllm/pull/23728)) by @WoosukKwon
* Only run `get_attr_docs` if generating help text ([#23723](https://github.com/vllm-project/vllm/pull/23723)) by @hmellor
* [CI/Build] Reduce LoRA layer test cases ([#23721](https://github.com/vllm-project/vllm/pull/23721)) by @jeejeelee
* [V1] [Hybrid] Disable prefix caching by default for hybrid or mamba-based models  ([#23716](https://github.com/vllm-project/vllm/pull/23716)) by @tdoublep
* [CI/Build] Remove redundant register in model init tests ([#23715](https://github.com/vllm-project/vllm/pull/23715)) by @DarkLight1337
* [CI/Build] Remove redundant LoRA model tests ([#23706](https://github.com/vllm-project/vllm/pull/23706)) by @jeejeelee
* [Multimodal] Generate mm_hash based on request metadata when caching is turned off ([#23690](https://github.com/vllm-project/vllm/pull/23690)) by @ywang96
* [Compile] Fix Cmake Warning ([#23689](https://github.com/vllm-project/vllm/pull/23689)) by @yewentao256
* [Misc] Fix comments in `tests/kernels/quantization` ([#23675](https://github.com/vllm-project/vllm/pull/23675)) by @ZJY0516
* [v1] Add cross-attention KV cache support for encoder-decoder models ([#23664](https://github.com/vllm-project/vllm/pull/23664)) by @russellb
* [V1][Mamba] - Enable V1 by default for Mamba Models ([#23650](https://github.com/vllm-project/vllm/pull/23650)) by @Josephasafg
* [Misc] Add override for allreduce fusion thresholds ([#23639](https://github.com/vllm-project/vllm/pull/23639)) by @nvjullin
* [mypy] Fix incorrect type hint for EAGLE3 support ([#23617](https://github.com/vllm-project/vllm/pull/23617)) by @DarkLight1337
* [CI/Build] Fix typo in #23561 ([#23616](https://github.com/vllm-project/vllm/pull/23616)) by @DarkLight1337
* [V1] Enable V1 for compute capability < 8.0 + FP32 ([#23614](https://github.com/vllm-project/vllm/pull/23614)) by @DarkLight1337
* DP/EP Support for gpt-oss with deepep-ht comm kernel on SM100 ([#23608](https://github.com/vllm-project/vllm/pull/23608)) by @zyongye
* [Misc] Add release note draft to PR template ([#23598](https://github.com/vllm-project/vllm/pull/23598)) by @simon-mo
* [Misc] Simplify FlashInfer attention metadata ([#23585](https://github.com/vllm-project/vllm/pull/23585)) by @WoosukKwon
* [quantization] use channel scales for w4a8 + misc fixes ([#23570](https://github.com/vllm-project/vllm/pull/23570)) by @czhu-cohere
* [CI Fix] Pin deepep and pplx tags in tools/ep_kernels/, gate multigpu tests ([#23568](https://github.com/vllm-project/vllm/pull/23568)) by @mgoin
* [Hardware][Mac] Fix the installation fail for Apple Silicon (CPU)  ([#23565](https://github.com/vllm-project/vllm/pull/23565)) by @OYE93
* [CI/Build] Use vLLM client's user agent to fetch images ([#23561](https://github.com/vllm-project/vllm/pull/23561)) by @DarkLight1337
* Update Flashinfer to  0.2.14.post1 ([#23537](https://github.com/vllm-project/vllm/pull/23537)) by @weireweire
* [V1][P/D]P2pNcclConnector supports flashinfer ([#23536](https://github.com/vllm-project/vllm/pull/23536)) by @Abatom
* [misc] add shanghai meetup ([#23535](https://github.com/vllm-project/vllm/pull/23535)) by @youkaichao
* Enhance the pre-notification policy ([#23532](https://github.com/vllm-project/vllm/pull/23532)) by @sidhpurwala-huzaifa
* [New Model]: Support GteNewModelForSequenceClassification ([#23524](https://github.com/vllm-project/vllm/pull/23524)) by @noooop
* [Misc] Unified linear print info ([#23516](https://github.com/vllm-project/vllm/pull/23516)) by @jeejeelee
* Migrate DonutImagePixelInputs to TensorSchema ([#23509](https://github.com/vllm-project/vllm/pull/23509)) by @bbeckca
* [Misc] Remove unused slot_mapping buffer ([#23502](https://github.com/vllm-project/vllm/pull/23502)) by @WoosukKwon
* Migrate tarsier inputs to TensorSchema ([#23500](https://github.com/vllm-project/vllm/pull/23500)) by @bbeckca
* Migrate skyworkr1v inputs to TensorSchema ([#23499](https://github.com/vllm-project/vllm/pull/23499)) by @bbeckca
* [Doc: ]fix various typos in multiple files ([#23487](https://github.com/vllm-project/vllm/pull/23487)) by @didier-durand
* Migrate Qwen inputs to TensorSchema ([#23473](https://github.com/vllm-project/vllm/pull/23473)) by @bbeckca
* Migrate Pixtral inputs to TensorSchema ([#23472](https://github.com/vllm-project/vllm/pull/23472)) by @bbeckca
* Migrate Paligemma inputs to TensorSchema ([#23470](https://github.com/vllm-project/vllm/pull/23470)) by @bbeckca
* [Misc] Modify CacheConfig import ([#23459](https://github.com/vllm-project/vllm/pull/23459)) by @jeejeelee
* (Misc): add missing test for zero truncation size. ([#23457](https://github.com/vllm-project/vllm/pull/23457)) by @teekenl
* fix(tests): Correct unreachable assertion in truncation test ([#23425](https://github.com/vllm-project/vllm/pull/23425)) by @AzizCode92
* [Misc] Move M-RoPE init logic to _init_mrope_positions ([#23422](https://github.com/vllm-project/vllm/pull/23422)) by @WoosukKwon
* [misc] Remove outdate comment about runai_model_streamer ([#23421](https://github.com/vllm-project/vllm/pull/23421)) by @carlory
* [Misc] local import code clean ([#23420](https://github.com/vllm-project/vllm/pull/23420)) by @andyxning
* [V0 Deprecation] Remove V0 LoRA test ([#23418](https://github.com/vllm-project/vllm/pull/23418)) by @jeejeelee
* fix(tests): Ensure reliable CUDA cache clearing in MoE test ([#23416](https://github.com/vllm-project/vllm/pull/23416)) by @AzizCode92
* Revert "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)" ([#23396](https://github.com/vllm-project/vllm/pull/23396)) by @DarkLight1337
* [Bug fix] Dynamically setting the backend variable for genai_perf_tests in the run-nightly-benchmark script ([#23375](https://github.com/vllm-project/vllm/pull/23375)) by @namanlalitnyu
* [LogitsProcs] Deduplicate built-in LP implementation logic ([#23362](https://github.com/vllm-project/vllm/pull/23362)) by @njhill
* [UX] Move Dockerfile DeepGEMM install to tools/install_deepgemm.sh ([#23360](https://github.com/vllm-project/vllm/pull/23360)) by @mgoin
* [CI/Build] Skip Idefics3 and SmolVLM generation test again ([#23356](https://github.com/vllm-project/vllm/pull/23356)) by @Isotr0py
* [Speculators][Speculative Decoding] Fix Qwen 2 Eagle3 Support ([#23337](https://github.com/vllm-project/vllm/pull/23337)) by @PapaGoose
* [Misc] update dict parse to EPLBConfig from json dumps to dict unpacking ([#23305](https://github.com/vllm-project/vllm/pull/23305)) by @lengrongfu
* [Attention] Allow V1 flash_attn to support cross-attention ([#23297](https://github.com/vllm-project/vllm/pull/23297)) by @russellb
* [New Model] Add Seed-Oss model ([#23241](https://github.com/vllm-project/vllm/pull/23241)) by @FoolPlayer
* [New Model]Donut model ([#23229](https://github.com/vllm-project/vllm/pull/23229)) by @princepride
* ci: Add arm64 docker build to release pipeline ([#23210](https://github.com/vllm-project/vllm/pull/23210)) by @seemethere
* [Quantization] Allow GGUF quantization to skip unquantized layer ([#23188](https://github.com/vllm-project/vllm/pull/23188)) by @Isotr0py
* [Attention] Unify mamba and attention backend selection ([#23171](https://github.com/vllm-project/vllm/pull/23171)) by @ayushsatyam146
* Gracefully handle edge cases in harmony utils ([#23155](https://github.com/vllm-project/vllm/pull/23155)) by @Ithanil
* [Attention] Refactor AttentionMetadata Preparation for Encoder-only Models ([#23154](https://github.com/vllm-project/vllm/pull/23154)) by @heheda12345
* [CPU] add cpu fused moe pytorch native implementation ([#23146](https://github.com/vllm-project/vllm/pull/23146)) by @TianyuLi0
* [Misc] Remove unnecessary `_send_reconfig_message()` in `core_client.py` ([#23127](https://github.com/vllm-project/vllm/pull/23127)) by @njhill
* Feature/benchmark/random mm data/images ([#23119](https://github.com/vllm-project/vllm/pull/23119)) by @h-brenoskuk
* [P/D][Nixl] Make kv cache register compatible with hybrid memory allocator ([#23079](https://github.com/vllm-project/vllm/pull/23079)) by @sfeng33
* [Benchmarks] add benchmark for embedding models ([#23000](https://github.com/vllm-project/vllm/pull/23000)) by @ZJY0516
* [FIXBUG] Add return_success parameter to moe_wna16_weight_loader function ([#22797](https://github.com/vllm-project/vllm/pull/22797)) by @JartX
* [Disagg][Perf] Use CUDA event sync instead of blocking `tolist` to avoid unintentional copy ops blocking across different CUDA streams, improving disagg TTIT/TTFT ([#22760](https://github.com/vllm-project/vllm/pull/22760)) by @liuzijing2014
* [Quantization] Expand compressed-tensors MoE matching logic to support NFP4 + FP8 MoEs ([#22674](https://github.com/vllm-project/vllm/pull/22674)) by @dsikka
* [V1] Enable prefill optimization for Gemma3n ([#22628](https://github.com/vllm-project/vllm/pull/22628)) by @sarckk
* [V1] [Hybrid] Enable Full CUDA graph by default for hybrid models in V1 ([#22594](https://github.com/vllm-project/vllm/pull/22594)) by @tdoublep
* [V1] [Hybrid] Enable compile and piecewise CUDA graph for MiniMax-Text models ([#22589](https://github.com/vllm-project/vllm/pull/22589)) by @tdoublep
* Frontend: Adding LM Format Enforcer support to V1 engine ([#22564](https://github.com/vllm-project/vllm/pull/22564)) by @noamgat
* Quantization: support FP4 quantized models on AMD CDNA2/CDNA3 GPUs ([#22527](https://github.com/vllm-project/vllm/pull/22527)) by @fengli1702
* [Transform] [Quantization] Add transforms to compressed tensors ([#22486](https://github.com/vllm-project/vllm/pull/22486)) by @kylesayrs
* Migrate Llama4ImagePatchInputs to TensorSchema ([#22021](https://github.com/vllm-project/vllm/pull/22021)) by @bbeckca
* Migrate MllamaImagePixelInputs to TensorSchema ([#22020](https://github.com/vllm-project/vllm/pull/22020)) by @bbeckca
* [CI/Build] add EP dependencies to docker ([#21976](https://github.com/vllm-project/vllm/pull/21976)) by @zhewenl
* Migrate MiniCPMOAudioInputs to TensorSchema ([#21847](https://github.com/vllm-project/vllm/pull/21847)) by @bbeckca
* Updates to Flex + VLLm integration ([#21416](https://github.com/vllm-project/vllm/pull/21416)) by @drisspg
* [Models] Improve iteration over layers ([#19497](https://github.com/vllm-project/vllm/pull/19497)) by @lgeiger
* [Deprecation] Remove `prompt_token_ids` arg fallback in `LLM.generate` and `LLM.embed` ([#18800](https://github.com/vllm-project/vllm/pull/18800)) by @DarkLight1337
* [Misc] Add gemma3 chat template with pythonic-style function calling ([#17149](https://github.com/vllm-project/vllm/pull/17149)) by @philipchung

## Contributors

@22quinn, @842974287, @Abatom, @AzizCode92, @CSWYF3634076, @DarkLight1337, @FFFfff1FFFfff, @FoolPlayer, @Hanchenli, @He-Jingkai, @Isotr0py, @Ithanil, @JartX, @Josephasafg, @MatthewBonanni, @OYE93, @PapaGoose, @ProExpertProg, @Shrey1306, @Terrencezzj, @TianyuLi0, @WoosukKwon, @Xu-Wenqing, @ZJY0516, @Zerohertz, @ahengljh, @andyxning, @angelayi, @app/copilot-swe-agent, @arjunbreddy22, @ayushsatyam146, @bbeckca, @bigPYJ1151, @bppps, @bringlein, @carlory, @chaojun-zhang, @charlifu, @chenxi-yang, @cndoit18, @coval3nte, @crischeng, @czhu-cohere, @david6666666, @didier-durand, @divakar-amd, @drisspg, @dsikka, @elvischenv, @faaany, @fake0fan, @fengli1702, @frank-wei, @gcalmettes, @h-brenoskuk, @heheda12345, @hickeyma, @hmellor, @hongxiayang, @huachenheli, @huydhn, @ilmarkov, @jeanschmidt, @jeejeelee, @jikunshang, @killershrimp, @kylesayrs, @lengrongfu, @lgeiger, @liuzijing2014, @lordmathis, @luccafong, @mgoin, @minosfuture, @namanlalitnyu, @njhill, @noamgat, @noooop, @nvjullin, @nvpohanh, @oneraghavan, @patemotter, @philipchung, @princepride, @ranpox, @rasmith, @rebel-hongseok, @russellb, @sarckk, @seemethere, @sfeng33, @sidhpurwala-huzaifa, @simon-mo, @skyloevil, @tc-mb, @tdoublep, @teekenl, @vllmellm, @weireweire, @windsonsea, @wuhang2014, @xyang16, @yewentao256, @youkaichao, @youzhedian, @yuekaizhang, @yuguo68, @ywang96, @zhewenl, @zifeitong, @zixuanzhang226, @zyongye

