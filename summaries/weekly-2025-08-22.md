# Weekly Release Notes for vllm-project/vllm (2025-08-22)

## What's Changed

### ‚ú® Features & Enhancements

* [Feature] Enable DeepGEMM Linear on B200; 1.5% E2E throughput improvement ([#23351](https://github.com/vllm-project/vllm/pull/23351)) by @yewentao256
* [Feature][Responses API] Support logprobs(non-stream) ([#23319](https://github.com/vllm-project/vllm/pull/23319)) by @kebe7jun
* Add docs for PrefixRepetitionDataset + enable usage with `vllm bench throughput` ([#23012](https://github.com/vllm-project/vllm/pull/23012)) by @eicherseiji
* [Feature] Full Cuda Graph Support for Cutlass MLA and 6% E2E Throughput Improvement ([#22763](https://github.com/vllm-project/vllm/pull/22763)) by @yewentao256
* [FEAT] [Performance] Enable DP for ViT in Qwen2.5VL ([#22742](https://github.com/vllm-project/vllm/pull/22742)) by @tjtanaa
* Support multiple attention groups for KV sharing ([#22672](https://github.com/vllm-project/vllm/pull/22672)) by @sarckk
* Add return_token_ids parameter to OpenAI API endpoints ([#22587](https://github.com/vllm-project/vllm/pull/22587)) by @ultmaster
* add tg-mxfp4-moe-test ([#22540](https://github.com/vllm-project/vllm/pull/22540)) by @IwakuraRein
* Support conditional torch.compile per module ([#22269](https://github.com/vllm-project/vllm/pull/22269)) by @sarckk
* Add PrefixRepetitionRandomDataset to `vllm bench serve` datasets ([#20638](https://github.com/vllm-project/vllm/pull/20638)) by @eicherseiji
* [Feature] use --eplb_config to set eplb param ([#20562](https://github.com/vllm-project/vllm/pull/20562)) by @lengrongfu

### üêõ Bug Fixes

* [BugFix][gpt-oss] Fix Chat Completion with Multiple Output Message ([#23318](https://github.com/vllm-project/vllm/pull/23318)) by @heheda12345
* [Bugfix] set system_message in phi4mini chat template ([#23309](https://github.com/vllm-project/vllm/pull/23309)) by @zhuangqh
* [BugFix] Fix Python 3.9 Support ([#23306](https://github.com/vllm-project/vllm/pull/23306)) by @jaredoconnell
* [Bug] Fix R1 Accuracy 0 Bug ([#23294](https://github.com/vllm-project/vllm/pull/23294)) by @yewentao256
* [Fix] remove is_marlin param in benchmark_moe ([#23286](https://github.com/vllm-project/vllm/pull/23286)) by @shixianc
* [Bugfix] Fix extra whitespace in strings caused by newline ([#23272](https://github.com/vllm-project/vllm/pull/23272)) by @DarkLight1337
* [Bugfix] Ensure correctness of HCXVision processing ([#23254](https://github.com/vllm-project/vllm/pull/23254)) by @DarkLight1337
* [Bugfix] Ensure correctness of Cohere2Vision processing ([#23245](https://github.com/vllm-project/vllm/pull/23245)) by @DarkLight1337
* Fix missing quotes ([#23242](https://github.com/vllm-project/vllm/pull/23242)) by @wzshiming
* [BugFix] fix CUTLASS MLA full cudagraph  ([#23200](https://github.com/vllm-project/vllm/pull/23200)) by @LucasWilkinson
* fix: use cache_salt for gpt-oss ([#23186](https://github.com/vllm-project/vllm/pull/23186)) by @dr75
* [Bugfix] Fix benchmark_moe.py  ([#23177](https://github.com/vllm-project/vllm/pull/23177)) by @jeejeelee
* Fix nvfp4 swizzling ([#23140](https://github.com/vllm-project/vllm/pull/23140)) by @yiliu30
* fix: OpenAI SDK compat (ResponseTextConfig) ([#23126](https://github.com/vllm-project/vllm/pull/23126)) by @h-brenoskuk
* [Bugfix] Fix accuracy issue when using flashinfer cutlass moe, TP=1 and modelopt. ([#23125](https://github.com/vllm-project/vllm/pull/23125)) by @bnellnm
* [Bugfix] Support compile for Transformers multimodal ([#23095](https://github.com/vllm-project/vllm/pull/23095)) by @zucchini-nlp
* fix: gptq marlin weight loading failure ([#23066](https://github.com/vllm-project/vllm/pull/23066)) by @simon-mo
* [Bugfix] fix Qwen2.5-Omni processor output mapping ([#23058](https://github.com/vllm-project/vllm/pull/23058)) by @DoubleVII
* [Bugfix][CI] Machete kernels: deterministic ordering for more cache hits ([#23055](https://github.com/vllm-project/vllm/pull/23055)) by @andylolu2
* Fix a performance comparison issue in Benchmark Suite ([#23047](https://github.com/vllm-project/vllm/pull/23047)) by @louie-tsai
* [Bugfix] fix qwen3 moe fp8 accuracy issue ([#23031](https://github.com/vllm-project/vllm/pull/23031)) by @jinzhen-lin
* [Bugfix] fix IntermediateTensors equal method ([#23027](https://github.com/vllm-project/vllm/pull/23027)) by @andyxning
* Fix handling of `max_num_batched_tokens` for pooling tasks ([#23004](https://github.com/vllm-project/vllm/pull/23004)) by @maxdebayser
* [BugFix] Fix regression caused by mamba state dtype PR ([#22998](https://github.com/vllm-project/vllm/pull/22998)) by @tdoublep
* [BugFix] Handle case where async utility call is cancelled ([#22996](https://github.com/vllm-project/vllm/pull/22996)) by @njhill
* [BugFix] Fix stuck stats/metrics after requests are aborted ([#22995](https://github.com/vllm-project/vllm/pull/22995)) by @njhill
* [Fix] enable swap_ab for pplx problem size computation ([#22991](https://github.com/vllm-project/vllm/pull/22991)) by @shixianc
* [BugFix] Make `run_once` thread-safe ([#22978](https://github.com/vllm-project/vllm/pull/22978)) by @oraluben
* [Bugfix] should use stack instead of concat ([#22972](https://github.com/vllm-project/vllm/pull/22972)) by @947132885
* [BugFix] Fix for IMA in FA3 varlen combine ([#22967](https://github.com/vllm-project/vllm/pull/22967)) by @LucasWilkinson
* [BugFix] Add support for loading prompt embeds tensors serialized on unavailable devices and sparse tensors ([#22962](https://github.com/vllm-project/vllm/pull/22962)) by @qthequartermasterman
* [Bugfix] fix cuda 12.6 and 11.8 build ([#22952](https://github.com/vllm-project/vllm/pull/22952)) by @jinzhen-lin
* Fix GLM-4.5V-FP8 numerical issue ([#22949](https://github.com/vllm-project/vllm/pull/22949)) by @zixi-qi
* [Bugfix] Fix DeepSeek MTP ([#22934](https://github.com/vllm-project/vllm/pull/22934)) by @benchislett
* [Bugfix] Unquote file uri before reading image ([#22912](https://github.com/vllm-project/vllm/pull/22912)) by @sayandipdutta
* [BugFix][KVConn] Fix use of `get_required_kvcache_layout` ([#22734](https://github.com/vllm-project/vllm/pull/22734)) by @njhill
* fix cuda graph ([#22721](https://github.com/vllm-project/vllm/pull/22721)) by @fsx950223
* [Fix] fix offline env use local mode path ([#22526](https://github.com/vllm-project/vllm/pull/22526)) by @lengrongfu
* [Bugfix] Added more env vars to hash ([#22449](https://github.com/vllm-project/vllm/pull/22449)) by @nvjullin
* [BugFix] Skip the Q component for QKVParallelLinear in the case of QKVCrossParallelLinear since its width is 0 ([#22369](https://github.com/vllm-project/vllm/pull/22369)) by @sstamenk
* [BugFix] Fix port lookup in internal DP LB tests ([#22252](https://github.com/vllm-project/vllm/pull/22252)) by @njhill
* [Bugfix] Fix broken Minimax-01-VL model ([#22116](https://github.com/vllm-project/vllm/pull/22116)) by @Isotr0py
* [Bugfix] Fix port conflict by obtaining a list of open ports upfront ([#21894](https://github.com/vllm-project/vllm/pull/21894)) by @minosfuture
* [Fix] correct tool_id for kimi-k2 when use tool_choice=required ([#21259](https://github.com/vllm-project/vllm/pull/21259)) by @MoyanZitto

### ‚ö°Ô∏è Performance

* [Perf] Small optimizations for silu_mul_fp8_quant_deep_gemm ([#23265](https://github.com/vllm-project/vllm/pull/23265)) by @mgoin
* [Performance] V1 Pooling Models E2E Performance Optimization ([#23162](https://github.com/vllm-project/vllm/pull/23162)) by @noooop

### ü§ñ Model Support

* [Model][VLM] Support R-4B Model ([#23246](https://github.com/vllm-project/vllm/pull/23246)) by @yannqi
* [Model] Improve olmo and olmo2 ([#23228](https://github.com/vllm-project/vllm/pull/23228)) by @jeejeelee
* [Model] Add transformers problem_type (e.g. multi_label_classification) support ([#23173](https://github.com/vllm-project/vllm/pull/23173)) by @noooop
* [Model] Removes redundant all-reduce operation in Qwen3MoeSparseMoeBlock ([#23169](https://github.com/vllm-project/vllm/pull/23169)) by @yiz-liu
* [Model] Support Pipeline Parallelism for moonshotai/Kimi-VL-A3B-Thinking-2506 ([#23114](https://github.com/vllm-project/vllm/pull/23114)) by @ZJY0516
* [Model] support new model ovis2.5 ([#23084](https://github.com/vllm-project/vllm/pull/23084)) by @myselvess
* [Model] Granite-4 support loading quantized checkpoint ([#22925](https://github.com/vllm-project/vllm/pull/22925)) by @cyang49
* [Model] Add LFM2 architecture ([#22845](https://github.com/vllm-project/vllm/pull/22845)) by @paulpak58
* [Model] use autoWeightsLoader for gptoss ([#22446](https://github.com/vllm-project/vllm/pull/22446)) by @calvin0327
* [Model][V1] Support Ernie MTP ([#22169](https://github.com/vllm-project/vllm/pull/22169)) by @xyxinyang
* [Model] Support deepseek with eagle ([#21086](https://github.com/vllm-project/vllm/pull/21086)) by @xyang16

### üîå Hardware & Backend

* [TPU] make ptxla not imported when using tpu_commons ([#23081](https://github.com/vllm-project/vllm/pull/23081)) by @yaochengji
* [XPU] Fix compile size for xpu ([#23069](https://github.com/vllm-project/vllm/pull/23069)) by @jikunshang
* [XPU] fix xpu to set cudagraph batch sizes ([#23044](https://github.com/vllm-project/vllm/pull/23044)) by @calvin0327
* [XPU]avoid circular import during XPU init ([#23017](https://github.com/vllm-project/vllm/pull/23017)) by @jikunshang
* [XPU][CI]add xpu env vars in CI scripts ([#22946](https://github.com/vllm-project/vllm/pull/22946)) by @jikunshang
* [NVIDIA] Add SM100 Flashinfer Cutlass MoE fp8 backend ([#22357](https://github.com/vllm-project/vllm/pull/22357)) by @amirkl94
* [NVIDIA] Support Flashinfer TRTLLM FP8-q/kv/out Attention Kernel ([#21716](https://github.com/vllm-project/vllm/pull/21716)) by @elvischenv

### ‚öôÔ∏è Refactoring & Core

* [Core] Support custom executor qualname ([#23314](https://github.com/vllm-project/vllm/pull/23314)) by @22quinn
* [Refactor] Simplify code for MM budget ([#23310](https://github.com/vllm-project/vllm/pull/23310)) by @DarkLight1337
* Remove duplicate entry in vllm.attention.__all__ ([#23296](https://github.com/vllm-project/vllm/pull/23296)) by @russellb
* [Core] Always use tensor cores for Flashinfer Decode Wrapper ([#23214](https://github.com/vllm-project/vllm/pull/23214)) by @pavanimajety
* Remove chunked_prefill_enabled flag in V1 MLA ([#23183](https://github.com/vllm-project/vllm/pull/23183)) by @MatthewBonanni
* [Refactor] Get prompt updates earlier ([#23097](https://github.com/vllm-project/vllm/pull/23097)) by @DarkLight1337
* [Frontend] Add `/collective_rpc` API endpoint ([#23075](https://github.com/vllm-project/vllm/pull/23075)) by @22quinn
* [Refactor] Define MultiModalKwargsItems separate from MultiModalKwargs ([#23053](https://github.com/vllm-project/vllm/pull/23053)) by @DarkLight1337
* [Kernel] CUTLASS MoE FP8: Integrate cuda moe permute/unpermute ([#23045](https://github.com/vllm-project/vllm/pull/23045)) by @shixianc
* [Refactor] Defer tensor data construction in MultiModalKwargs ([#23030](https://github.com/vllm-project/vllm/pull/23030)) by @DarkLight1337
* [Refactor] Allow optional MultiModalKwargsItem in IPC ([#23022](https://github.com/vllm-project/vllm/pull/23022)) by @DarkLight1337
* [Core] Make cudagraph check cuda platform only ([#23005](https://github.com/vllm-project/vllm/pull/23005)) by @yaochengji
* [Frontend] improve error logging of chat completion ([#22957](https://github.com/vllm-project/vllm/pull/22957)) by @heheda12345
* [Kernel] Add cuda kernel for gpt_oss activation ([#22951](https://github.com/vllm-project/vllm/pull/22951)) by @jeejeelee
* [Frontend] Avoid list copies in `serving_chat.py` ([#22947](https://github.com/vllm-project/vllm/pull/22947)) by @njhill
* [Core] direct indexing on self.block_table_np in compute_slot_mapping ([#22940](https://github.com/vllm-project/vllm/pull/22940)) by @linzebing
* [Frontend] Expose do_log_stats interval to env ([#22905](https://github.com/vllm-project/vllm/pull/22905)) by @Csrayz
* refactor: Change scaling factors calculation for flashinfer FusedMoE ([#22812](https://github.com/vllm-project/vllm/pull/22812)) by @amirkl94
* [Kernel] Simplify `get_kv_cache_layout` and cache `use_trtllm_attention` env-dependent bit ([#22735](https://github.com/vllm-project/vllm/pull/22735)) by @NickLucche
* [Kernel] Add FP8 support with FlashMLA backend ([#22668](https://github.com/vllm-project/vllm/pull/22668)) by @MatthewBonanni
* [Kernel]  Add cuda kernel for gpt_oss activation ([#22538](https://github.com/vllm-project/vllm/pull/22538)) by @jeejeelee
* [Core] Optimize scheduler request removal for single completions ([#21917](https://github.com/vllm-project/vllm/pull/21917)) by @chi2liu
* [Core] Add torch profiler CPU traces for AsyncLLM. ([#21794](https://github.com/vllm-project/vllm/pull/21794)) by @huachenheli
* [Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer ([#20059](https://github.com/vllm-project/vllm/pull/20059)) by @fhl2000
* [Frontend] Added support for HermesToolParser for models without special tokens ([#16890](https://github.com/vllm-project/vllm/pull/16890)) by @minpeter

### üîß Build, CI & Testing

* [CI] improve pr comments bot ([#23380](https://github.com/vllm-project/vllm/pull/23380)) by @simon-mo
* [CI] Clean up actions: remove helm, publish workflows and improve pr ‚Ä¶ ([#23377](https://github.com/vllm-project/vllm/pull/23377)) by @simon-mo
* [CI] Block the cu126 wheel build while broken ([#23285](https://github.com/vllm-project/vllm/pull/23285)) by @mgoin
* [Build] Env var to disable sccache ([#22968](https://github.com/vllm-project/vllm/pull/22968)) by @LucasWilkinson
* [CI][Bugfix] Skip Ovis2 generation test because of broken remote code ([#22954](https://github.com/vllm-project/vllm/pull/22954)) by @Isotr0py
* [CI] Remove duplicated docs build from buildkite ([#22924](https://github.com/vllm-project/vllm/pull/22924)) by @hmellor
* [CI] Pooling models mteb test uses enforce_eager ([#22878](https://github.com/vllm-project/vllm/pull/22878)) by @noooop
* [CI][V0 Deprecation] Removed V0 Only Chunked Prefill and Prefix Caching Tests ([#22871](https://github.com/vllm-project/vllm/pull/22871)) by @robertgshaw2-redhat
* [CI] Speed up Whisper tests by reusing server ([#22859](https://github.com/vllm-project/vllm/pull/22859)) by @mgoin
* [CI] Add end-to-end V1 min_tokens test coverage ([#22495](https://github.com/vllm-project/vllm/pull/22495)) by @arjunbreddy22

### üìö Documentation

* [Doc] Fix batch-level DP example ([#23325](https://github.com/vllm-project/vllm/pull/23325)) by @DarkLight1337
* [Doc] Update V1 status of various pooling models ([#23189](https://github.com/vllm-project/vllm/pull/23189)) by @DarkLight1337
* [Doc] use power of 2 ([#23172](https://github.com/vllm-project/vllm/pull/23172)) by @Tialo

### üì¶ Miscellaneous

* [Structured Outputs] Refactor bitmask construction into get_grammar_bitmask ([#23361](https://github.com/vllm-project/vllm/pull/23361)) by @WoosukKwon
* [CI/Build] Skip Idefics3 and SmolVLM generation test again ([#23356](https://github.com/vllm-project/vllm/pull/23356)) by @Isotr0py
* [ci/build] Fix abi tag for aarch64 ([#23329](https://github.com/vllm-project/vllm/pull/23329)) by @youkaichao
* [Multimodal] Always enable hashing mm data ([#23308](https://github.com/vllm-project/vllm/pull/23308)) by @ywang96
* [Misc] Misc code cleanup/simplification ([#23304](https://github.com/vllm-project/vllm/pull/23304)) by @njhill
* [V1] Remove unnecessary check for main thread ([#23298](https://github.com/vllm-project/vllm/pull/23298)) by @robertgshaw2-redhat
* Delete images older than 24h. ([#23291](https://github.com/vllm-project/vllm/pull/23291)) by @QiliangCui
* [Compile] Fix Compile Warning SM100 Cutlass MLA ([#23287](https://github.com/vllm-project/vllm/pull/23287)) by @yewentao256
* [CI Bugfix] Fix CI by fully removing --enable-prompt-adapter ([#23284](https://github.com/vllm-project/vllm/pull/23284)) by @mgoin
* Always use cache mounts when installing vllm to avoid populating pip cache in the image. Also remove apt cache. ([#23270](https://github.com/vllm-project/vllm/pull/23270)) by @tvalentyn
* Small fix for Command-A-Vision ([#23268](https://github.com/vllm-project/vllm/pull/23268)) by @dongluw
* Limit HTTP header count and size ([#23267](https://github.com/vllm-project/vllm/pull/23267)) by @russellb
* Do not use eval() to convert unknown types ([#23266](https://github.com/vllm-project/vllm/pull/23266)) by @russellb
* [Optimization] Make new_block_ids None if empty ([#23262](https://github.com/vllm-project/vllm/pull/23262)) by @WoosukKwon
* [CI/Build] Split out mm processor tests ([#23260](https://github.com/vllm-project/vllm/pull/23260)) by @DarkLight1337
* [New Model] Add Seed-Oss model ([#23241](https://github.com/vllm-project/vllm/pull/23241)) by @FoolPlayer
* [Misc] Add max_seq_len to CommonAttentionMetadata  ([#23216](https://github.com/vllm-project/vllm/pull/23216)) by @WoosukKwon
* [Kernel/Quant] Remove the original marlin format and qqq ([#23204](https://github.com/vllm-project/vllm/pull/23204)) by @mgoin
* [Quantization] Bump Compressed Tensors Version ([#23202](https://github.com/vllm-project/vllm/pull/23202)) by @kylesayrs
* [Benchmarks] Add video inputs to ShareGPTDataset.  ([#23199](https://github.com/vllm-project/vllm/pull/23199)) by @huachenheli
* Feature/mla tests ([#23195](https://github.com/vllm-project/vllm/pull/23195)) by @MatthewBonanni
* [Misc] Enable yapf for FlashInfer backend ([#23193](https://github.com/vllm-project/vllm/pull/23193)) by @WoosukKwon
* [Misc] fix VLLM_TORCH_PROFILER_DIR to absolute path ([#23191](https://github.com/vllm-project/vllm/pull/23191)) by @andyxning
* [CLI][Doc] Formalize `--mm-encoder-tp-mode` ([#23190](https://github.com/vllm-project/vllm/pull/23190)) by @DarkLight1337
* [Attention] Optimize make_local_attention_virtual_batches for Flash Attention ([#23185](https://github.com/vllm-project/vllm/pull/23185)) by @linzebing
* Make sure that vectorize_with_alignment produced vectorized global loads ([#23182](https://github.com/vllm-project/vllm/pull/23182)) by @elvircrn
* [CI/Build] Sync multimodal tests ([#23181](https://github.com/vllm-project/vllm/pull/23181)) by @DarkLight1337
* [Misc] Fix seq_lens for graph capture ([#23175](https://github.com/vllm-project/vllm/pull/23175)) by @WoosukKwon
* [Misc] Avoid accessing req_ids inside a loop ([#23159](https://github.com/vllm-project/vllm/pull/23159)) by @WoosukKwon
* [Attention] Refactor AttentionMetadata Preparation for Encoder-only Models ([#23154](https://github.com/vllm-project/vllm/pull/23154)) by @heheda12345
* [Misc] Minor refactoring for FlashInfer backend ([#23147](https://github.com/vllm-project/vllm/pull/23147)) by @WoosukKwon
* [Log] Warning Once for Cutlass MLA  ([#23137](https://github.com/vllm-project/vllm/pull/23137)) by @yewentao256
* Install tpu_info==0.4.0 to fix core dump for TPU ([#23135](https://github.com/vllm-project/vllm/pull/23135)) by @xiangxu-google
* [CI Perf] Only test bfloat16 for tests/compile/test_fusion_all_reduce.py ([#23132](https://github.com/vllm-project/vllm/pull/23132)) by @mgoin
* Update to flashinfer-python==0.2.12 and disable AOT compile for non-release image ([#23129](https://github.com/vllm-project/vllm/pull/23129)) by @mgoin
* [Misc] Add @tdoublep as a maintainer of hybrid model and Triton-attention related code ([#23122](https://github.com/vllm-project/vllm/pull/23122)) by @tdoublep
* [CI Bugfix] Pin `openai<1.100` to unblock CI ([#23118](https://github.com/vllm-project/vllm/pull/23118)) by @mgoin
* [misc] split engine_model into json file for nsys profile tool ([#23117](https://github.com/vllm-project/vllm/pull/23117)) by @gracehonv
* [Misc] Minor refactoring for prepare_inputs ([#23116](https://github.com/vllm-project/vllm/pull/23116)) by @WoosukKwon
* [misc] fix multiple arch wheels for the nightly index ([#23110](https://github.com/vllm-project/vllm/pull/23110)) by @youkaichao
* [CI/Build] Update transformers to v4.55.2 ([#23093](https://github.com/vllm-project/vllm/pull/23093)) by @Isotr0py
* chore: remove unnecessary patch_padding_side for the chatglm model ([#23090](https://github.com/vllm-project/vllm/pull/23090)) by @carlory
* [P/D][Nixl] Make kv cache register compatible with hybrid memory allocator ([#23079](https://github.com/vllm-project/vllm/pull/23079)) by @sfeng33
* [CPU] Refactor CPU W8A8 scaled_mm ([#23071](https://github.com/vllm-project/vllm/pull/23071)) by @bigPYJ1151
* [Misc] Fix backward compatibility from #23030 ([#23070](https://github.com/vllm-project/vllm/pull/23070)) by @ywang96
* [Misc] Add request_id into benchmark_serve.py ([#23065](https://github.com/vllm-project/vllm/pull/23065)) by @hustxiayang
* [Misc] Minor code cleanup for _get_prompt_logprobs_dict ([#23064](https://github.com/vllm-project/vllm/pull/23064)) by @WoosukKwon
* [Misc] Remove dead return ([#23061](https://github.com/vllm-project/vllm/pull/23061)) by @WoosukKwon
* [Misc] Convert use_structured_output property into constant ([#23060](https://github.com/vllm-project/vllm/pull/23060)) by @WoosukKwon
* [Misc] enhance static type hint ([#23059](https://github.com/vllm-project/vllm/pull/23059)) by @andyxning
* [Misc] fix typo in the multimodal doc ([#23051](https://github.com/vllm-project/vllm/pull/23051)) by @KevinZeng08
* chore: disable enable_cpp_symbolic_shape_guards ([#23048](https://github.com/vllm-project/vllm/pull/23048)) by @xiszishu
* [Misc] method name typo fix ([#23042](https://github.com/vllm-project/vllm/pull/23042)) by @andyxning
* [Spec Decode] Make `propose_draft_token_ids` non-blocking for lower TTFT ([#23041](https://github.com/vllm-project/vllm/pull/23041)) by @WoosukKwon
* [CI/Build] Also check DP in benchmarks throughput script ([#23038](https://github.com/vllm-project/vllm/pull/23038)) by @zhewenl
* [V1][Mamba1] - Full CUDA and Piecewise CUDA Graphs Support ([#23035](https://github.com/vllm-project/vllm/pull/23035)) by @Josephasafg
* [Misc] refactor function name ([#23029](https://github.com/vllm-project/vllm/pull/23029)) by @andyxning
* [Flaky CI] Increase timeout tolerance for test_mp_crash_detection+test_default_mm_lora_chat_completions ([#23028](https://github.com/vllm-project/vllm/pull/23028)) by @mgoin
* [Misc] Add --save-dir option to benchmark_moe ([#23020](https://github.com/vllm-project/vllm/pull/23020)) by @jeejeelee
* [Bugfix gpt-oss] Fix float32 convert for flashinfer sink support ([#23016](https://github.com/vllm-project/vllm/pull/23016)) by @mgoin
* Use Blackwell FlashInfer MXFP4 MoE by default if available  ([#23008](https://github.com/vllm-project/vllm/pull/23008)) by @mgoin
* [UX] Separate marlin moe config logic from triton moe ([#23006](https://github.com/vllm-project/vllm/pull/23006)) by @mgoin
* [CI/Build] Replace lm-eval gsm8k tests with faster implementation ([#23002](https://github.com/vllm-project/vllm/pull/23002)) by @mgoin
* Use regex in convert-results-json-to-markdown.py ([#22989](https://github.com/vllm-project/vllm/pull/22989)) by @mgoin
* [misc] nsys profile output kernel classifier and visualizer ([#22971](https://github.com/vllm-project/vllm/pull/22971)) by @gracehonv
* [V0 Deprecation] Remove advance_step ([#22969](https://github.com/vllm-project/vllm/pull/22969)) by @WoosukKwon
* [Structured Outputs] [Bug] Fix misalignment in apply_grammar_bitmask causing unintended masking and NaN logits ([#22963](https://github.com/vllm-project/vllm/pull/22963)) by @rishitdholakia13
* Revert "[ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module." ([#22956](https://github.com/vllm-project/vllm/pull/22956)) by @tjtanaa
* [Benchmarks] Include image data when ShareGPT4V dataset is used. ([#22955](https://github.com/vllm-project/vllm/pull/22955)) by @huachenheli
* [MM] Allow skipping memory profiling for multimodal models. ([#22950](https://github.com/vllm-project/vllm/pull/22950)) by @ywang96
* Revert "[Kernel]  Add cuda kernel for gpt_oss activation" ([#22948](https://github.com/vllm-project/vllm/pull/22948)) by @simon-mo
* [Misc] Support passing multiple request ids at once to `AsyncLLM.abort()` ([#22944](https://github.com/vllm-project/vllm/pull/22944)) by @njhill
* [Kernel/Quant] Remove AQLM ([#22943](https://github.com/vllm-project/vllm/pull/22943)) by @mgoin
* [CI Perf] Prune tests in `tests/kernels/quantization/` ([#22942](https://github.com/vllm-project/vllm/pull/22942)) by @mgoin
* [CI Perf] Prune tests in `tests/kernels/moe/` ([#22939](https://github.com/vllm-project/vllm/pull/22939)) by @mgoin
* [CI Perf] Prune tests in `tests/kernels/attention/` ([#22936](https://github.com/vllm-project/vllm/pull/22936)) by @mgoin
* [V1] [Hybrid] Support using float32 for state in Hybrid Models (Mamba2, Mamba1, Minimax) ([#22928](https://github.com/vllm-project/vllm/pull/22928)) by @tdoublep
* [FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches ([#22896](https://github.com/vllm-project/vllm/pull/22896)) by @JartX
* [Benchmark] Add flag --served-model-name to benchmark_serving_multi_turn ([#22889](https://github.com/vllm-project/vllm/pull/22889)) by @pliops-daniels
* [New Model]mBART model ([#22883](https://github.com/vllm-project/vllm/pull/22883)) by @princepride
* [Multimodal] Update Tensor schema test to cover arbitrary shape mm inputs ([#22867](https://github.com/vllm-project/vllm/pull/22867)) by @Isotr0py
* [Log] Debug Once for Randomizing dummy data for DP Rank ([#22860](https://github.com/vllm-project/vllm/pull/22860)) by @yewentao256
* Improve multimodal hasher performance for re-used Image prompts ([#22825](https://github.com/vllm-project/vllm/pull/22825)) by @p88h
* [Mamba] - refactor: Renamed mamba_attn to mamba2_attn ([#22818](https://github.com/vllm-project/vllm/pull/22818)) by @Josephasafg
* [Misc] Ignore ep_kernels_workspace ([#22807](https://github.com/vllm-project/vllm/pull/22807)) by @jeejeelee
* [FIXBUG ] Allow disabling rocm_aiter_fa backend for ROCm GPUs not compatible with AITER ([#22795](https://github.com/vllm-project/vllm/pull/22795)) by @JartX
* chore: support pytorch format in lora  ([#22790](https://github.com/vllm-project/vllm/pull/22790)) by @KilJaeeun
* [V0 Deprecation] Remove V0 FlashInfer attention backend ([#22776](https://github.com/vllm-project/vllm/pull/22776)) by @WoosukKwon
* [Hardware][IBM Z]Enable v1 for s390x and s390x dockerfile fixes ([#22725](https://github.com/vllm-project/vllm/pull/22725)) by @nikheal2
* [bug fix] Fix llama4 spec decoding ([#22691](https://github.com/vllm-project/vllm/pull/22691)) by @zixi-qi
* [EP] Add logging for experts map ([#22685](https://github.com/vllm-project/vllm/pull/22685)) by @22quinn
* [Misc] Fix the benchmark's README and improve the error messages for the benchmark's argument checks ([#22654](https://github.com/vllm-project/vllm/pull/22654)) by @tanruixiang
* [V1] - Split Prefill and Decode for Mamba1 models ([#22653](https://github.com/vllm-project/vllm/pull/22653)) by @amirai21
* [P/D]Provide bucket algorithm rate limiter  for proxy_server ([#22643](https://github.com/vllm-project/vllm/pull/22643)) by @frankie-ys
* minor: zero workspace buffer init for flashinfer trtllm-gen attn ([#22603](https://github.com/vllm-project/vllm/pull/22603)) by @yyihuang
* [Structured Output] Make the output of structured output example more complete ([#22481](https://github.com/vllm-project/vllm/pull/22481)) by @shen-shanshan
* [Attention] FA3 Attention Sinks Perf Boost ([#22478](https://github.com/vllm-project/vllm/pull/22478)) by @LucasWilkinson
* [Sampler] Support returning final logprobs ([#22387](https://github.com/vllm-project/vllm/pull/22387)) by @22quinn
* [Kernels] Clean up FusedMoeMethodBase and modular kernel setup.  Remove extra arguments from modular kernel methods. ([#22035](https://github.com/vllm-project/vllm/pull/22035)) by @bnellnm
* Migrate InternVLImagePixelInputs (in nemotron_vl.py) to TensorSchema ([#22023](https://github.com/vllm-project/vllm/pull/22023)) by @bbeckca
* Migrate MolmoImageInputs to TensorSchema ([#22022](https://github.com/vllm-project/vllm/pull/22022)) by @bbeckca
* Migrate MllamaImagePixelInputs to TensorSchema ([#22020](https://github.com/vllm-project/vllm/pull/22020)) by @bbeckca
* [V1] support min_tokens for detokener ([#22014](https://github.com/vllm-project/vllm/pull/22014)) by @calvin0327
* Migrate Mistral3ImagePixelInputs to TensorSchema ([#21945](https://github.com/vllm-project/vllm/pull/21945)) by @bbeckca
* Migrate LlavaOnevisionMultiInputs to TensorSchema ([#21844](https://github.com/vllm-project/vllm/pull/21844)) by @bbeckca
* ci: Add CUDA + arm64 release builds ([#21201](https://github.com/vllm-project/vllm/pull/21201)) by @seemethere
* [Optimization] Speed up function `_convert_tokens_to_string_with_added_encoders` by 13.7x ([#20413](https://github.com/vllm-project/vllm/pull/20413)) by @misrasaurabh1
* [V1] Logits processors extensibility ([#19912](https://github.com/vllm-project/vllm/pull/19912)) by @afeldman-nm
* [v1] Move block_hashes from KVCacheManager to Request.block_hashes (#19728) ([#19728](https://github.com/vllm-project/vllm/pull/19728)) by @orozery
* [Deprecation] Remove `prompt_token_ids` arg fallback in `LLM.generate` and `LLM.embed` ([#18800](https://github.com/vllm-project/vllm/pull/18800)) by @DarkLight1337
* [Misc] Add gemma3 chat template with pythonic-style function calling ([#17149](https://github.com/vllm-project/vllm/pull/17149)) by @philipchung
* [CI/Build] Add support for Python 3.13 ([#13164](https://github.com/vllm-project/vllm/pull/13164)) by @mgoin

## Contributors

@22quinn, @947132885, @Csrayz, @DarkLight1337, @DoubleVII, @FoolPlayer, @Isotr0py, @IwakuraRein, @JartX, @Josephasafg, @KevinZeng08, @KilJaeeun, @LucasWilkinson, @MatthewBonanni, @MoyanZitto, @NickLucche, @QiliangCui, @Tialo, @WoosukKwon, @ZJY0516, @afeldman-nm, @amirai21, @amirkl94, @andylolu2, @andyxning, @arjunbreddy22, @bbeckca, @benchislett, @bigPYJ1151, @bnellnm, @calvin0327, @carlory, @chi2liu, @cyang49, @dongluw, @dr75, @eicherseiji, @elvircrn, @elvischenv, @fhl2000, @frankie-ys, @fsx950223, @gracehonv, @h-brenoskuk, @heheda12345, @hmellor, @huachenheli, @hustxiayang, @jaredoconnell, @jeejeelee, @jikunshang, @jinzhen-lin, @kebe7jun, @kylesayrs, @lengrongfu, @linzebing, @louie-tsai, @maxdebayser, @mgoin, @minosfuture, @minpeter, @misrasaurabh1, @myselvess, @nikheal2, @njhill, @noooop, @nvjullin, @oraluben, @orozery, @p88h, @paulpak58, @pavanimajety, @philipchung, @pliops-daniels, @princepride, @qthequartermasterman, @rishitdholakia13, @robertgshaw2-redhat, @russellb, @sarckk, @sayandipdutta, @seemethere, @sfeng33, @shen-shanshan, @shixianc, @simon-mo, @sstamenk, @tanruixiang, @tdoublep, @tjtanaa, @tvalentyn, @ultmaster, @wzshiming, @xiangxu-google, @xiszishu, @xyang16, @xyxinyang, @yannqi, @yaochengji, @yewentao256, @yiliu30, @yiz-liu, @youkaichao, @ywang96, @yyihuang, @zhewenl, @zhuangqh, @zixi-qi, @zucchini-nlp

