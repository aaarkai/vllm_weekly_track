# Weekly Release Report for vllm-project/vllm (2025-12-26)

This week merged 135 PRs from 84 contributors. Key areas: features 5, fixes 6, performance 8.

## Executive Summary

本周的版本更新聚焦于功能增强、性能优化和模型支持扩展。主要新增了 Feature/isaac 0.1 集成、针对 NVFP4 量化内核在小批量场景下的性能调优，以及自动根据可用内存调整上下文长度的 `--max-model-len auto` 功能。在模型支持方面，加强了对 Ernie4.5-VL、GPT-OSS 系列及 Qwen3-Omni 等多模态和工具调用模型的处理能力，并修复了相关解析器问题。性能工作包括优化 NUMA 内存分配和修复 FP4 量化内核的初始化缺陷。

本次更新包含若干重要变更。移除了针对 v0.14 版本的已弃用接口，并清理了未使用的代码。值得注意的是，为前端提供的请求 ID 添加了随机后缀以增强唯一性。在升级时需注意：对 PLaMo 2/3 模型的 LoRA 和 GPTQModel 支持可能引入行为变化；此外，在 ROCm 平台上，由于 deepep 依赖暂未支持，可能会遇到相关错误。前缀缓存块的释放逻辑已调整，使其在内存需要时可被回收。

## Highlights

* Feature/isaac 0.1 ([#28367](https://github.com/vllm-project/vllm/pull/28367)) by @oscardev256
* [Chore][1/2] Drop `v0.14` deprecations ([#31285](https://github.com/vllm-project/vllm/pull/31285)) by @DarkLight1337
* [NVFP4][Perf] Tune NVFP4 input quant kernel for small batch size ([#30897](https://github.com/vllm-project/vllm/pull/30897)) by @mgoin
* [Core] Add a random suffix to frontend-provided request IDs ([#27987](https://github.com/vllm-project/vllm/pull/27987)) by @markmc
* [Chore] Remove unused `noqa`s ([#31263](https://github.com/vllm-project/vllm/pull/31263)) by @DarkLight1337

## Features & Enhancements

* Add util function for checking nesting of rope parameters ([#31146](https://github.com/vllm-project/vllm/pull/31146)) by @hmellor
* add aarnphm and chaunceyjiang to the new tool_parser directory ([#31088](https://github.com/vllm-project/vllm/pull/31088)) by @chaunceyjiang
* Add hidden dimension validation for multimodal embedding inputs ([#30968](https://github.com/vllm-project/vllm/pull/30968)) by @wenqiglantz
* Add `--max-model-len auto` to auto-fit context to available memory ([#29431](https://github.com/vllm-project/vllm/pull/29431)) by @mgoin
* Feature/isaac 0.1 ([#28367](https://github.com/vllm-project/vllm/pull/28367)) by @oscardev256

## Bug Fixes

* [BugFix] Fix async scheduling + reasoning with struct output ([#31332](https://github.com/vllm-project/vllm/pull/31332)) by @njhill
* [Quantization] support logical_widths for fp8 marlin ([#30962](https://github.com/vllm-project/vllm/pull/30962)) by @jinzhen-lin
* [Quantization] fix marlin w8a8 check ([#30961](https://github.com/vllm-project/vllm/pull/30961)) by @jinzhen-lin
* [BugFix] Handle errors when preprocessing added requests ([#30895](https://github.com/vllm-project/vllm/pull/30895)) by @njhill
* [BugFix] Fix logprobs with spec decode and modified logits ([#30846](https://github.com/vllm-project/vllm/pull/30846)) by @njhill
* Fix edge case Mistral tool parser ([#30724](https://github.com/vllm-project/vllm/pull/30724)) by @joa-stdn

## Performance

* [benchmark] use model card root instead of id ([#31329](https://github.com/vllm-project/vllm/pull/31329)) by @andyxning
* Revert "[bench] Support common prefix len config (for decode-only bench)" ([#31240](https://github.com/vllm-project/vllm/pull/31240)) by @minosfuture
* [Chore] Update more locations to use `attention_config.backend` ([#31153](https://github.com/vllm-project/vllm/pull/31153)) by @DarkLight1337
* [FIX] FP4 quantization kernel padding initialization bug ([#31097](https://github.com/vllm-project/vllm/pull/31097)) by @danielafrimi
* [NVFP4][Perf] Tune NVFP4 input quant kernel for small batch size ([#30897](https://github.com/vllm-project/vllm/pull/30897)) by @mgoin
* [PERF] Add interleaved memory allocation to NUMA module ([#30800](https://github.com/vllm-project/vllm/pull/30800)) by @skaraban3807
* use the same stream for cuda graph catpure and replay for NCCL ([#29207](https://github.com/vllm-project/vllm/pull/29207)) by @Amir-19
* Enable aarch64 CPU performance benchmarks ([#26494](https://github.com/vllm-project/vllm/pull/26494)) by @None

## Model Support

* [Model][Ernie4.5-VL] Support video metadata for timestamp rendering ([#31274](https://github.com/vllm-project/vllm/pull/31274)) by @Tiiiktak
* [Doc] Add tool call parser documentation for GPT-OSS models ([#31212](https://github.com/vllm-project/vllm/pull/31212)) by @amithkk
* [ROCm][CI/Build] Fix triton version to one that has triton_kernels required for gpt-oss to run ([#31159](https://github.com/vllm-project/vllm/pull/31159)) by @gshtras
* [Model] Fix bagel failed to run ([#31132](https://github.com/vllm-project/vllm/pull/31132)) by @Potabk
* [Model] Introduce verify_and_update_model_config for VerifyAndUpdateConfig. ([#31131](https://github.com/vllm-project/vllm/pull/31131)) by @noooop
* [Misc] Fix spelling typos in model comments ([#31117](https://github.com/vllm-project/vllm/pull/31117)) by @c0de128
* [CI] Add Qwen3-Next-FP8 to Blackwell model tests ([#31049](https://github.com/vllm-project/vllm/pull/31049)) by @vadiklyutiy
* [Qwen3-Omni] fixed _get_feat_extract_output_lengths function ([#31007](https://github.com/vllm-project/vllm/pull/31007)) by @wangxiongts
* [Feature]: Support NVIDIA ModelOpt HF FP8 variants FP8_PER_CHANNEL_PER_TOKEN and FP8_PB_WO  in vLLM ([#30957](https://github.com/vllm-project/vllm/pull/30957)) by @CedricHwong
* GLM-4.7 Tool Parser and Doc Update ([#30876](https://github.com/vllm-project/vllm/pull/30876)) by @zRzRzRzRzRzRzR
* [Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony models ([#30867](https://github.com/vllm-project/vllm/pull/30867)) by @HaloWorld
* [Model] Add MiMo-V2-Flash support ([#30836](https://github.com/vllm-project/vllm/pull/30836)) by @Abatom
* [BugFix]fix gpt-oss v1/completions response bug ([#30608](https://github.com/vllm-project/vllm/pull/30608)) by @princepride
* [Frontend] Support using chat template as custom score template for reranking models ([#30550](https://github.com/vllm-project/vllm/pull/30550)) by @jzakrzew
* [ROCm][CI][Bugfix] Multi-Modal Model Support Fixes and Attention Backend Improvements ([#30270](https://github.com/vllm-project/vllm/pull/30270)) by @AndreasKaratzas
* [BugFix] skip language model in Encoder ([#30242](https://github.com/vllm-project/vllm/pull/30242)) by @Bounty-hunter
* [gpt-oss] Fix harmony parser in streaming responses ([#30205](https://github.com/vllm-project/vllm/pull/30205)) by @AlonKejzman
* [Mamba] - Consolidate Mambas Attention Logic ([#28133](https://github.com/vllm-project/vllm/pull/28133)) by @Josephasafg
* [Hybrid] Mamba2 prefix cache blocks freeing for running requests ([#28047](https://github.com/vllm-project/vllm/pull/28047)) by @s3woz

## Hardware & Backend

* [Chore] Bump `lm-eval` version ([#31264](https://github.com/vllm-project/vllm/pull/31264)) by @DarkLight1337
* [Bugfix][ROCm] Fix load issue on deepseek quark quantization when shared expert enabled ([#31261](https://github.com/vllm-project/vllm/pull/31261)) by @ganyi1996ppo
* [ROCm][CI] Set TORCH_NCCL_BLOCKING_WAIT Distributed Tests On ROCm ([#31259](https://github.com/vllm-project/vllm/pull/31259)) by @micah-wil
* [ROCm][CI] Set VLLM_FLOAT32_MATMUL_PRECISION="tf32" For terratorch Tests In AMD CI ([#31242](https://github.com/vllm-project/vllm/pull/31242)) by @micah-wil
* [ROCm][CI][Bugfix] Fix Siglip2 rotary embedding dispatch and InternVL video test tolerance ([#31235](https://github.com/vllm-project/vllm/pull/31235)) by @AndreasKaratzas
* [Misc] Introduce `encode_*_url` utility function ([#31208](https://github.com/vllm-project/vllm/pull/31208)) by @DarkLight1337
* [ROCm][Bugfix] Fix RuntimeError in MMEncoderAttention by replacing .view() with .reshape() ([#31203](https://github.com/vllm-project/vllm/pull/31203)) by @AndreasKaratzas
* Revert "[SM100] Enable fp8 compute for prefill MLA (#30746)" ([#31197](https://github.com/vllm-project/vllm/pull/31197)) by @pavanimajety
* [AMD][CI] fix v1/engine test_preprocess_error_handling ([#31192](https://github.com/vllm-project/vllm/pull/31192)) by @divakar-amd
* [Bugfix][Hardware][AMD] Fix FP8 dtype in silu_mul quantization ([#31179](https://github.com/vllm-project/vllm/pull/31179)) by @c0de128
* [Doc] Add vllm-metal to hardware plugin documentation ([#31174](https://github.com/vllm-project/vllm/pull/31174)) by @mgoin
* [Bug] Fix `'CutlassMLAImpl' object has no attribute '_workspace_buffer'` ([#31173](https://github.com/vllm-project/vllm/pull/31173)) by @yewentao256
* [Bugfix][ROCm][Dynamo][DS 3.1][FP8] fix unsupported hasattr call when Dynamo tracing for ROCm device ([#31149](https://github.com/vllm-project/vllm/pull/31149)) by @zejunchen-zejun
* [Bugfix][ROCm] Fix typo: is_linear_fp8_enaled -> is_linear_fp8_enabled ([#31109](https://github.com/vllm-project/vllm/pull/31109)) by @c0de128
* [MoE Refactor][7/N] AITER MK ([#31102](https://github.com/vllm-project/vllm/pull/31102)) by @robertgshaw2-redhat
* ci: add nvidia-smi warmup before Prime-RL integration test ([#31093](https://github.com/vllm-project/vllm/pull/31093)) by @AmeenP
* [CI] FIx `fixture 'siglip_attention_config' not found` ([#31053](https://github.com/vllm-project/vllm/pull/31053)) by @LucasWilkinson
* [MoE Refactor][9/N] Use modular kernel for unquantized Triton MoE ([#31052](https://github.com/vllm-project/vllm/pull/31052)) by @zyongye
* [AMD][CI] Add "V1 Test e2e + engine" to mi325_8 Agent Pool ([#31040](https://github.com/vllm-project/vllm/pull/31040)) by @micah-wil
* [CustomOp][Refactor] Extract common methods for ApplyRotaryEmb CustomOp ([#31021](https://github.com/vllm-project/vllm/pull/31021)) by @shen-shanshan
* [Doc][CPU] Fix index link for CPU regular release wheels ([#31015](https://github.com/vllm-project/vllm/pull/31015)) by @bigPYJ1151
* [ROCm][CI/Build] Update ROCm dockerfiles ([#30991](https://github.com/vllm-project/vllm/pull/30991)) by @gshtras
* [XPU] enable fp8 online streaming quantization  ([#30944](https://github.com/vllm-project/vllm/pull/30944)) by @yma11
* [Bugfix] [Kernel] Triton attention kernels: mask out V blocks that fall outside sliding window ([#30887](https://github.com/vllm-project/vllm/pull/30887)) by @tdoublep
* [CPU][Bugfix] Fix ppc64le CPU build ([#30871](https://github.com/vllm-project/vllm/pull/30871)) by @npanpaliya
* [Bugfix] fix the alias bug of AttentionBackendEnum when register CUSTOM attention backend to vllm ([#30869](https://github.com/vllm-project/vllm/pull/30869)) by @zejunchen-zejun
* [MoE Refactor][2/N] Use Modular Kernels for Fp8 ([#30825](https://github.com/vllm-project/vllm/pull/30825)) by @robertgshaw2-redhat
* [Kernel] Enable fused_qknorm_rope_kernel supports partial rope ([#30821](https://github.com/vllm-project/vllm/pull/30821)) by @jeejeelee
* [SM100] Enable fp8 compute for prefill MLA ([#30746](https://github.com/vllm-project/vllm/pull/30746)) by @pavanimajety
* [XPU] decrease IGC_ForceOCLSIMDWidth for speculative decoding triton-xpu kernel compilation ([#30538](https://github.com/vllm-project/vllm/pull/30538)) by @yma11
* [SpecDecode] Simplified alternative padded-speculation acceptance rate fix ([#29845](https://github.com/vllm-project/vllm/pull/29845)) by @LucasWilkinson
* [ROCm][CI] Fix entrypoints tests and Python-only installation test on ROCm ([#28979](https://github.com/vllm-project/vllm/pull/28979)) by @AndreasKaratzas
* [MoE Refactor][5/N] Isolate zero expert to LongCatFlash ([#28891](https://github.com/vllm-project/vllm/pull/28891)) by @baonudesifeizhai
* [ROCm][FEAT] Support AITER RMSNorm quantization fusion pass  ([#26575](https://github.com/vllm-project/vllm/pull/26575)) by @vllmellm

## Refactoring & Core

* [Chore] Simplify logic of `_execute_mm_encoder` ([#31222](https://github.com/vllm-project/vllm/pull/31222)) by @DarkLight1337
* [MoE Refactor][4/N] Marlin Fp8 Mk ([#31036](https://github.com/vllm-project/vllm/pull/31036)) by @robertgshaw2-redhat
* [Refactor] Refactor for `DeepGemmQuantScaleFMT` using cache ([#30898](https://github.com/vllm-project/vllm/pull/30898)) by @yewentao256

## Build, CI & Testing

* [CI] Reorganization pooling_mteb_test ([#31265](https://github.com/vllm-project/vllm/pull/31265)) by @noooop
* [Bugfix] Fix eagle dp tests on A100 ([#31241](https://github.com/vllm-project/vllm/pull/31241)) by @zou3519
* [ci] Fix Pytorch compilation test oom in 2.10 ([#31194](https://github.com/vllm-project/vllm/pull/31194)) by @angelayi
* [CI Failure] Disable mosaicml/mpt-7b and databricks/dbrx-instruct tests ([#31182](https://github.com/vllm-project/vllm/pull/31182)) by @mgoin
* [CI][Bugfix] Fix `entrypoints/openai/test_audio.py` ([#31151](https://github.com/vllm-project/vllm/pull/31151)) by @NickLucche
* [CI] Fix "2 Node Tests (4 GPUs in total)" ([#31090](https://github.com/vllm-project/vllm/pull/31090)) by @LucasWilkinson
* [Doc] Clarify FP8 KV cache computation workflow ([#31071](https://github.com/vllm-project/vllm/pull/31071)) by @westers
* [CI] Fix H200 Distributed test ([#31054](https://github.com/vllm-project/vllm/pull/31054)) by @LucasWilkinson
* [CI] add polling for precompiled wheel in python_only_compile.sh, fix index generation for releases ([#30781](https://github.com/vllm-project/vllm/pull/30781)) by @Harry-Chen
* [CI/Build] Ignore max transformers version skipping for initialization tests ([#30619](https://github.com/vllm-project/vllm/pull/30619)) by @Isotr0py
* [CI/Build] Ignore data_parallel_size_local ([#30281](https://github.com/vllm-project/vllm/pull/30281)) by @rjrock

## Documentation

* [Doc] Add troubleshooting for Triton PTX error about undefined gpu-name ([#31338](https://github.com/vllm-project/vllm/pull/31338)) by @Isotr0py
* docs: Add llm-d integration to the website ([#31234](https://github.com/vllm-project/vllm/pull/31234)) by @terrytangyuan
* [Frontend] add FunctionGemma tool parser support ([#31218](https://github.com/vllm-project/vllm/pull/31218)) by @gateremark
* Correct position of docstring of class attributes ([#31209](https://github.com/vllm-project/vllm/pull/31209)) by @wdhongtw
* Update MiniMax-M2 ToolCall and add MiniMax-M2.1 in Docs ([#31083](https://github.com/vllm-project/vllm/pull/31083)) by @rogeryoungh
* Update Pytorch version update docs ([#30982](https://github.com/vllm-project/vllm/pull/30982)) by @atalman
* [Bugfix] Add validation for tool requests when tool_parser is unavailable ([#30613](https://github.com/vllm-project/vllm/pull/30613)) by @majiayu000
* [docker] Fix downloading sccache on aarch64 platform ([#30070](https://github.com/vllm-project/vllm/pull/30070)) by @NickCao

## Miscellaneous

* [cli] complete vllm cli help message ([#31226](https://github.com/vllm-project/vllm/pull/31226)) by @andyxning
* [Bugfix] Enable `dynamic_dims` for different embeds shape ([#31223](https://github.com/vllm-project/vllm/pull/31223)) by @DarkLight1337
* Only patch `original_max_position_embeddings` for Transformers v4 ([#31214](https://github.com/vllm-project/vllm/pull/31214)) by @hmellor
* [Bugfix] Fix Jais2ForCausalLM ([#31198](https://github.com/vllm-project/vllm/pull/31198)) by @jeejeelee
* [Bugfix] Fix MoE LoRA bin/pt loading ([#31161](https://github.com/vllm-project/vllm/pull/31161)) by @jeejeelee
* [Bug] Fix `Number of dimensions of tensors must match.` for Deepseek V3.2 ([#31160](https://github.com/vllm-project/vllm/pull/31160)) by @yewentao256
* [UX] improve profiler error message ([#31125](https://github.com/vllm-project/vllm/pull/31125)) by @BoyuanFeng
* [Misc] Fix typo: 'occured' -> 'occurred' ([#31120](https://github.com/vllm-project/vllm/pull/31120)) by @c0de128
* [Misc] Fix quantization-related typos ([#31116](https://github.com/vllm-project/vllm/pull/31116)) by @c0de128
* [Misc] Fix grammar errors in comments and messages ([#31115](https://github.com/vllm-project/vllm/pull/31115)) by @c0de128
* [Misc] Fix spelling typos in comments ([#31114](https://github.com/vllm-project/vllm/pull/31114)) by @c0de128
* adapt voxtral ([#31095](https://github.com/vllm-project/vllm/pull/31095)) by @patrickvonplaten
* [Quantization] add marlin w4a8/w8a8 check ([#31061](https://github.com/vllm-project/vllm/pull/31061)) by @jinzhen-lin
* [Bug] Fix `error 'Dynamo failed to run FX node with fake tensors` for Deepseek V3.2 ([#31046](https://github.com/vllm-project/vllm/pull/31046)) by @yewentao256
* [Bugfix] Read truncate_prompt_tokens from pooling_params in AsyncLLM.encode() ([#31013](https://github.com/vllm-project/vllm/pull/31013)) by @jeffreywang-anyscale
* [Quantization] enable compressed-tensors marlin support for turing (2) ([#31008](https://github.com/vllm-project/vllm/pull/31008)) by @jinzhen-lin
* [Quantization] enable compressed-tensors marlin support for turing ([#31000](https://github.com/vllm-project/vllm/pull/31000)) by @jinzhen-lin
* [Bugfix] Fix incorrect tiles creation for mm prefix triton attention ([#30974](https://github.com/vllm-project/vllm/pull/30974)) by @Isotr0py
* [BugFix] Fix TypeError: unhashable type: 'dict' when serving deepseek32 ([#30924](https://github.com/vllm-project/vllm/pull/30924)) by @LucasWilkinson
* [KVEvent] User request.block_hash for parent block_hash ([#30544](https://github.com/vllm-project/vllm/pull/30544)) by @heheda12345
* [OpenAI] Add parameter metadata to validation errors ([#30134](https://github.com/vllm-project/vllm/pull/30134)) by @R3hankhan123
* [P/D] Mooncake connector support more protocols ([#30133](https://github.com/vllm-project/vllm/pull/30133)) by @LCAIZJ
* [Feature] Batch invariant: Lora ([#30097](https://github.com/vllm-project/vllm/pull/30097)) by @quanliu1991
* Use helper function instead of looping through attribute names ([#29788](https://github.com/vllm-project/vllm/pull/29788)) by @hmellor

## Breaking Changes

* Support LoRA and GPTQModel for PLaMo 2/3  ([#31322](https://github.com/vllm-project/vllm/pull/31322)) by @Alnusjaponica
* [Bugfix] Remove dead `block_quant_to_tensor_quant` function ([#31294](https://github.com/vllm-project/vllm/pull/31294)) by @yurekami
* [Chore][1/2] Drop `v0.14` deprecations ([#31285](https://github.com/vllm-project/vllm/pull/31285)) by @DarkLight1337
* [Chore] Remove unused `noqa`s ([#31263](https://github.com/vllm-project/vllm/pull/31263)) by @DarkLight1337
* [Bugfix] Fix `max_model_len="auto"` handling ([#31260](https://github.com/vllm-project/vllm/pull/31260)) by @DarkLight1337
* [ROCm][CI] Fix "Distributed Tests (H200)" Test ([#31227](https://github.com/vllm-project/vllm/pull/31227)) by @kliuae
* [Perf] Remove blocking copy in GDN Attention ([#31167](https://github.com/vllm-project/vllm/pull/31167)) by @benchislett
* [ROCm] [Critical]: Remove unused variable ([#31156](https://github.com/vllm-project/vllm/pull/31156)) by @tjtanaa
* [DeepSeek v3.2] Remove unnecessary syncwarps ([#31047](https://github.com/vllm-project/vllm/pull/31047)) by @MatthewBonanni
* [Perf] Add skip_clone to SamplingParams for internal request handling ([#31041](https://github.com/vllm-project/vllm/pull/31041)) by @mgoin
* [Benchmark Suite] improve cpu Benchmark Suite tests and comparison report for 0.12.0 ([#30994](https://github.com/vllm-project/vllm/pull/30994)) by @louie-tsai
* [MoE Refactor][3/N] Deprecate cutlass block quant fp8 (b200) ([#30990](https://github.com/vllm-project/vllm/pull/30990)) by @robertgshaw2-redhat
* [Misc] Remove unused custom ops `copy_blocks` and `copy_blocks_mla` ([#30967](https://github.com/vllm-project/vllm/pull/30967)) by @lengrongfu
* [XPU] Remove distributed_executor_backend check  ([#30760](https://github.com/vllm-project/vllm/pull/30760)) by @1643661061leo
* [Frontend][Bug] allow tool calls in analysis channel ([#28139](https://github.com/vllm-project/vllm/pull/28139)) by @dr75
* [Core] Add a random suffix to frontend-provided request IDs ([#27987](https://github.com/vllm-project/vllm/pull/27987)) by @markmc
* Make engine core client handshake timeout configurable  ([#27444](https://github.com/vllm-project/vllm/pull/27444)) by @eicherseiji

## Upgrade Notes

- However, proceeding with the latest vLLM would result in missing deepep dependency error, since deepep is currently not supported on ROCm. Additionally, collectives like all reduce is currently cudagraph breaking on ROCm. This PR makes changes to the DP test by replacing the deepep all2all backend w
- This PR marks the prefix cache blocks that aren't strictly needed by running requests as available for eviction. Note: This doesn't mean that they are evicted immediately, but rather that they are added to the list of memory blocks potentially available for eviction if more memory is needed. Thus, u

## Contributors

@1643661061leo, @Abatom, @Alnusjaponica, @AlonKejzman, @AmeenP, @Amir-19, @AndreasKaratzas, @Bounty-hunter, @BoyuanFeng, @CedricHwong, @DarkLight1337, @HaloWorld, @Harry-Chen, @Isotr0py, @Josephasafg, @LCAIZJ, @LucasWilkinson, @MatthewBonanni, @NickCao, @NickLucche, @Potabk, @R3hankhan123, @Tiiiktak, @amithkk, @andyxning, @angelayi, @atalman, @baonudesifeizhai, @benchislett, @bigPYJ1151, @c0de128, @chaunceyjiang, @danielafrimi, @divakar-amd, @dr75, @eicherseiji, @ganyi1996ppo, @gateremark, @gshtras, @heheda12345, @hmellor, @jeejeelee, @jeffreywang-anyscale, @jinzhen-lin, @joa-stdn, @jzakrzew, @kliuae, @lengrongfu, @louie-tsai, @majiayu000, @markmc, @mgoin, @micah-wil, @minosfuture, @njhill, @noooop, @npanpaliya, @oscardev256, @patrickvonplaten, @pavanimajety, @princepride, @quanliu1991, @rjrock, @robertgshaw2-redhat, @rogeryoungh, @s3woz, @shen-shanshan, @skaraban3807, @tdoublep, @terrytangyuan, @tjtanaa, @vadiklyutiy, @vllmellm, @wangxiongts, @wdhongtw, @wenqiglantz, @westers, @yewentao256, @yma11, @yurekami, @zRzRzRzRzRzRzR, @zejunchen-zejun, @zou3519, @zyongye